There has been an increasing acceptance by investigators of the importance of registering trials at inception and providing access to their trials results. Despite perceptions and even assertions to the contrary, however, there is no global, universal legal requirement to register clinical trials at inception or at any other stage in the process, although some countries are beginning to introduce such legislation (Viergever and Li 2015).

Efforts have been made by a number of organizations, including organizations representing the pharmaceutical industry and individual pharmaceutical companies, to begin to provide central access to ongoing trials and in some cases trial results on completion, either on a national or international basis. A recent audit of pharmaceutical companies' policies on access to trial data, results and methods, however, showed that the commitments made by companies to transparency of trials were highly variable (Goldacre et al 2017). Increasingly, as already noted, trials registers such as ClinicalTrials.gov also contain the results of completed trials, not just simply listing the details of the trial.

#### 4.3.4 Regulatory agency sources and clinical study reports

Potentially relevant regulatory agency sources include the EU Clinical Trials Register, Drugs@FDA and OpenTrialSFDA. Details of these are provided in the online Technical Supplement. Clinical study reports (CSRs) are the reports of clinical trials providing detailed information on the methods and results of clinical trials submitted in support of marketing authorization applications. In late 2010, the European Medicines Agency (EMA) began releasing CSRs (on request) under their Policy 0043. In October 2016, they began to release CSRs under their Policy 0070. The policy applies only to documents received since 1 January 2015. The terms of use for access are based on the purposes to which the clinical data will be put.

A recent study by Jefferson and colleagues (Jefferson et al 2018) that looked at use of regulatory documents in Cochrane Reviews, found that understanding within the Cochrane community was limited and guidance and support would be required if review authors were to engage with regulatory documents as a source of evidence. Specifically, guidance on how to use data from regulatory sources is needed. For more information about using CSRs, see the online Technical Supplement. Further guidance on collecting data from CSRs is provided in Chapter 5, Section 5.5.6.

#### 4.3.5 Other sources

The online Technical Supplement describes several other important sources of reports of studies. The term 'grey literature' is often used to refer to reports published outside of traditional commercial publishing. Review authors should generally search sources such as dissertations and conference abstracts (see MECIR Box 4.3.e).

Review authors may also consider searching the internet, handsearching of journals and searching full texts of journals electronically where available (see online Technical Supplement for details). They should examine previous reviews on the same topic and check reference lists of included studies and relevant systematic reviews (see MECIR Box 4.3.e).

### 4.4 Designing search strategies

#### 4.4.1 Introduction to search strategies

This section highlights some of the issues to consider when designing search strategies. Designing search strategies can be complex and the section does not fully address the many complexities in this area. Review teams will benefit from the skills and expertise of a medical/healthcare librarian or information specialist. Many of the issues highlighted relate to both the subject aspects of the search (e.g. the PICO elements) and to the study method (e.g. randomized trials). For a search to be robust, both aspects require attention to be sure that relevant records are not missed.

Issues to consider in planning a search include:

* the nature or type of the intervention(s) being assessed;
* the complexity of the review question and the need to consider additional conceptual frameworks (see Chapters 3 and 17);
* the time period when any evaluations of the interventions may have taken place (as specified in the review protocol) (see Section 4.4.5);
* any geographic considerations, such as the need to search the African Index Medicus for studies relating to African populations or the Chinese literature for studies in Chinese herbal medicine (see online Technical Supplement);
* whether the review is limited to randomized trials or other study designs are eligible (see Chapter 24);
* whether a validated methodological search filter (for specific study designs) is available (see Section 4.4.7);
* whether unpublished data are to be sought specifically (see Sections 4.3.2, 4.3.3 and 4.3.4); and* whether the review has specific eligibility criteria around study design to address adverse effects (see Chapter 19), economic issues (see Chapter 20) or qualitative research questions (see Chapter 21), in which case searches to address these criteria should be undertaken (see MECIR Box 4.4.a).

Further evidence-based information about designing search strategies can be found on the SuRe Info portal, which is updated twice per year.

#### 4.4.2 Structure of a search strategy

The starting point for developing a search strategy is to consider the main concepts being examined in a review. This is often referred to as PICO - that is Patient (or Participant or Population or Problem), Intervention, Comparison and Outcomes (Richardson et al 1995): see also Chapters 2 and 3 for guidance on developing and refining PICO definitions that will be operationalized in the search strategy. Examples are provided in the appendices to the Cochrane Information Specialists' Handbook (Littlewood et al 2017). For a Cochrane Review, the review objective should provide the PICO concepts, and the eligibility criteria for studies to be included will further assist in the selection of appropriate subject headings and text words for the search strategy.

The structure of search strategies in bibliographic databases should be informed by the main concepts of the review (see Chapter 3), using appropriate elements from PICO and study design (see MECIR Box 4.4.b). It is usually unnecessary, however, and may even be undesirable, to search on every aspect of the review's clinical question. Although a research question may specify particular comparators or outcomes, these concepts may not be well described in the title or abstract of an article and are often not well indexed with controlled vocabulary terms. Therefore, in general databases, such as MEDLINE, a search strategy will typically have three sets of terms: (i) terms to search for the health condition of interest, i.e. the population; (ii) terms to search for the intervention(s) evaluated; and (iii) terms to search for the types of study design to be included. Typically, a broad set of search terms will be gathered for each concept, and combined with the OR Boolean operator to achieve sensitivity within concepts. The results for each concept are then combined using the AND Boolean operator, to ensure each concept is represented in the final search results.

It is important to consider the structure of the search strategy on a question-by-question basis. In some cases it is possible and reasonable to search for the 

#### 4.4.4 Designing search strategies

##### 4.4.5.1 _C32:_ Structuring search strategies for bibliographic databases (**Mandatory**)

_Inappropriate or inadequate search bibliographic databases around the main concepts of the review, using appropriate elements from PICO and study design. In structuring the search, maximize sensitivity whilst striving for reasonable precision. Ensure correct use of the 'AND' and 'OR' operators._

##### 4.4.5.2 _The search strategy_

The search strategy is based on the main concepts being examined in a review. In general databases, such as MEDLINE, a search strategy to identify studies for a Cochrane Review will typically have three sets of terms: (i) terms to search for the health condition of interest, i.e. the population; (ii) terms to search for the intervention(s) evaluated; and (iii) terms to search for the types of study design to be included (typically a 'filter' for randomized trials). There are exceptions, however. For instance, for reviews of complex interventions, it may be necessary to search only for the population or the intervention. Within each concept, terms are joined together with the Boolean 'OR' operator, and the concepts are combined with the Boolean 'AND' operator. The 'NOT' operator should be avoided where possible to avoid the danger of inadvertently removing records that are relevant from the search set.

##### 4.4.5.3 _The search strategy_

The search strategy is based on the main concepts being examined in a review. In general, the search strategy is based on the main concepts being examined in a review.

public health interventions, and Chapter 21 on qualitative research). Some options to explore for such situations include:

* use a single concept such as searching for the intervention alone (European Food Safety Authority 2010);
* break a concept into two or more subconcepts;
* use a multi-stranded or multi-faceted approach that uses a series of searches, with different combinations of concepts, to capture a complex research question (Lefebvre et al 2013);
* use a variety of different search approaches to compensate for when a specific concept is difficult to define (Shemilt et al 2014); or
* use citation searching on key papers in addition to a database search (Haddaway et al 2015, Hinde and Spackman 2015) (see online Technical Supplement).

#### Sensitivity versus precision

Searches for systematic reviews aim to be as extensive as possible in order to ensure that as many of the relevant studies as possible are included in the review. It is, however, necessary to strike a balance between striving for comprehensiveness and maintaining relevance when developing a search strategy.

The properties of searches are often quantified using'sensitivity' (also called'recall') and 'precision' (see Table 4.4.a). Sensitivity is defined as the number of relevant reports identified divided by the total number of relevant reports in the resource. Precision is defined as the number of relevant reports identified divided by the total number of reports identified. Increasing the comprehensiveness (or sensitivity) of a search will reduce its precision and will usually retrieve more non-relevant reports.

Searches for Cochrane Reviews should seek to maximize sensitivity whilst striving for reasonable precision (see MECIR Box 4.4.b). Article abstracts identified through a database search can usually be screened very quickly to ascertain potential relevance. At a conservatively estimated reading rate of one or two abstracts per minute, the results of a database search can be screened at the rate of 60-120 per hour (or approximately 500-1000 over an 8-hour period), so the high yield and low precision associated with systematic review searching may not be as daunting as it might at first appear in comparison with the total time to be invested in the review.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Reports retrieved & Reports not retrieved \\ \hline
**Relevant reports** & Relevant reports retrieved (a) & Relevant reports not retrieved (b) \\
**Irrelevant reports** & Irrelevant reports retrieved (c) & Irrelevant reports not retrieved (d) \\ \hline \hline \end{tabular}

* Sensitivity: fraction of relevant reports retrieved from all relevant reports (a/(a+b)) Precision: fraction of relevant reports retrieved from all reports retrieved (a/(a+c))

\end{table}
Table 4.4.a: Sensitivity and precision of a search 

#### 4.4.4 Controlled vocabulary and text words

MEDLINE and Embase (and many other databases) can be searched using a combination of two retrieval approaches. One is based on text words, that is terms occurring in the title, abstract or other relevant fields available in the database. The other is based on standardized subject terms assigned to the references by indexers (specialists who appraise the articles and describe their topics by assigning terms from a specific thesaurus or controlled vocabulary). Searches for Cochrane Reviews should use an appropriate combination of these two approaches (see MECIR Box 4.4.c). Approaches for identifying text words and controlled vocabulary to combine appropriately within a search strategy, including text mining approaches, are presented in the online Technical Supplement.

#### 4.4.5 Language, date and document format restrictions

Searches should capture as many studies as possible that meet the eligibility criteria, ensuring that relevant time periods and sources are covered and not restricted by language or publication status (see MECIR Box 4.3.a). Review authors should justify the use of any restrictions in the search strategy on publication date and publication format

#### 4.4.6 Relevant expectations for conduct of intervention reviews

4.7 Inappropriate or inadequate search strategies may fail to identify records that are included in bibliographic databases. Search strategies need to be customized for each database. It is important that MeSH terms are 'exploded' wherever appropriate, in order not to miss relevant articles. The same principle applies to Emtree when searching Embase and also to a number of other databases. The controlled vocabulary search terms for MEDLINE and Embase are not identical, and neither is the approach to indexing. In order to be as comprehensive as possible, it is necessary to include a wide range of free-text terms for each of the concepts selected. This might include the use of truncation and wildcards. Developing a search strategy is an iterative process in which the terms that are used are modified, based on what has already been retrieved.

(see MECIR Box 4.4.d). For example, excluding letters is not recommended because letters may contain important additional information relating to an earlier trial report or new information about a trial not reported elsewhere (lansavichene et al 2008). In addition, articles indexed as 'Comments' should not be routinely excluded without further examination as these may contain early warnings of suspected fraud (see Section 4.4.6).

Evidence indicates that excluding non-English studies does not change the conclusions of most systematic reviews (Morrison et al 2012, Jiao et al 2013, Hartling et al 2017), although exceptions have been observed for complementary and alternative medicine (Moher et al 2003, Pham et al 2005, Wu et al 2013). There is, however, also research related to language bias that supports the inclusion of non-English studies in systematic reviews (Egger et al 1997). For further discussion of these issues see Chapter 13.

Inclusion of non-English studies may also increase the precision of the result and the generalizability and applicability of the findings. There may be differences in therapeutic response to pharmaceutical agents according to ethnicity, either because of phenotype and pathogenesis of disease due to environmental factors or because of population pharmacogenomics and pharmacogenetics (Brusselle and Blasi 2015). The inclusion of non-English studies also makes it possible to perform sensitivity analyses to find out if there is any geographical bias in reporting the positive findings (Vickers et al 1998, Kaptchuk 1999). It also could be an indicator of quality of systematic reviews (Wang et al 2015).

Limiting searching to databases containing predominantly English-language records, even if no language restrictions are applied, may result in missed relevant studies(Pilkington et al 2005). Review authors should, therefore, attempt to identify and assess for eligibility all possibly relevant reports of trials irrespective of language of publication. If a Cochrane Review team requires help with translation of and/or data extraction from non-English language reports of studies, they should seek assistance to do so (this is a common task for which volunteer assistance can be sought via Cochrane's TaskExchange platform, accessible to both Cochrane and non-Cochrane review teams). Where it is not possible to extract the relevant information and data from non-English language reports, the review team should file the study in 'Studies Awaiting Classification' rather than 'Excluded Studies', to inform readers of the review of the availability of other possibly relevant reports and reflect this information in the PRISMA flow diagram (or, if there is no flow diagram, then in the text of the review) as 'Studies Awaiting Classification'.

#### 4.4.6 Identifying fraudulent studies, other retracted publications, errata and comments

When considering the eligibility of studies for inclusion in a Cochrane Review, it is important to be aware that some studies may have been found to contain errors or to be fraudulent or may, for other reasons, have been corrected or retracted since publication. Review authors should examine any relevant retraction statements and errata for information (MECIR Box 4.4.e). This applies both to 'new' studies identified for inclusion in a review and to studies that are already included in a review when the review is updated. For review updates, it is important to search MEDLINE and Embase for the latest version of the citations to the records for the (previously) included studies, in case they have since been corrected or retracted.

Errata are published to correct unintended errors (accepted as errors by the author(s)). Retraction notices are published (usually by the journal editor) where data have been found to be fraudulent, for example in the case of plagiarism. Comments are published under a range of circumstances including when errors are suggested by others and also for early concerns regarding fraud.

[backgroundcolor=gray!20,linewidth=0.5]MECIR Box 4.4.e Relevant expectations for conduct of intervention reviews

[backgroundcolor=gray!20,linewidth=0.5]C48: Examining errata (**Mandatory**)

[backgroundcolor=gray!20,linewidth=0.5]Examine any relevant retraction statements and errata for information.

[backgroundcolor=gray!20,linewidth=0.5]Some studies may have been found to be fraudulent or may have been retracted since publication for other reasons. Errata can reveal important limitations, or even fatal flaws, in included studies. All of these may lead to the potential exclusion of a study from a review or meta-analysis. Care should be taken to ensure that this information is retrieved in all database searches by downloading the appropriate fields, together with the citation data.

Including data from studies that are fraudulent or studies that include errors can have an impact on the overall estimates in systematic reviews. Details of how to identify fraudulent studies, other retracted publications, errata and comments are described in the online Technical Supplement.

#### 4.4.7 Search filters

Search filters are search strategies that are designed to retrieve specific types of records, such as those of a particular methodological design. When searching for randomized trials in humans, a validated filter should be used to identify studies with the appropriate design (see MECIR Box 4.4.f). Filters to identify randomized trials have been developed specifically for MEDLINE and Embase: see the online Technical Supplement for details. CENTRAL, however, aims to contain only reports with study designs possibly relevant for inclusion in Cochrane Reviews, so searches of CENTRAL should not use a trials 'filter' or be limited to human studies.

The InterTASC Information Specialists' Subgroup Search Filter Resource offers a collection of search filters, focusing predominantly on methodological search filters and providing critical appraisals of some of these filters. The site includes, amongst others, filters for identifying systematic reviews, randomized and non-randomized studies and qualitative research in a range of databases and across a range of service providers [16]. For further discussion around the design and use of search filters, see the online Technical Supplement.

#### 4.4.8 Peer review of search strategies

It is strongly recommended that search strategies should be peer reviewed. Peer review of search strategies is increasingly recognized as a necessary step in designing and executing high-quality search strategies to identify studies for possible inclusion in systematic reviews. Studies have shown that errors occur in the search strategiesunderpinning systematic reviews (Sampson and McGowan 2006) and that search strategies are not always conducted or reported to a high standard (Mullins et al 2014, Layton 2017). An evidence-based checklist such as the PRESS Evidence-Based Checklist should be used to assess which elements are important in peer review of electronic search strategies (McGowan et al 2016a, McGowan et al 2016b). The checklist covers not only the technical accuracy of the strategy (line numbers, spellings, etc), but also that the search strategy covers all relevant aspects of the protocol and has interpreted the research question appropriately. Research has shown that peer review using a specially designed checklist can improve the quality of searches (Relevo and Paynter 2012, Spry et al 2013). The names, credentials and institutions of the peer reviewers of the search strategies should be noted in the review (with their permission) in the Acknowledgements section.

#### Alerts

Alerts, also called literature surveillance services, 'push' services or SDIs (selective dissemination of information), are an excellent method of staying up to date with the medical literature currently being published, as a supplement to designing and running specific searches for specific reviews. In practice, alerts are based on a previously developed search strategy, which is saved in a personal account on the database platform (e.g. 'My EBSCOhost - search alerts' on EBSCO, 'My searches & alerts' on Ovid and 'MyNCBI - saved searches' on PubMed). These saved strategies filter the content as the database is being updated with new information. The account owner is notified (usually via email) when new publications meeting their specified search parameters are added to the database. In the case of PubMed, the alert can be set up to be delivered weekly or monthly, or in real-time and can comprise email or RSS feeds.

For review authors, alerts are a useful tool to help monitor what is being published in their review topic after the original search has been conducted. By following the alert, authors can become aware of a new study that meets the review's eligibility criteria, and decide either to include it in the review immediately or mention it as a'study awaiting assessment' for inclusion during the next review update (see online Chapter IV). Authors should consider setting up alerts so that the review can be as current as possible at the time of publication.

Another way of attempting to stay current with the literature as it emerges is by using alerts based on journal tables of contents (TOCs). These usually cannot be specifically tailored to the information needs in the same way as search strategies developed to cover a specific topic. They can, however, be a good way of trying to keep up to date on a more general level by monitoring what is currently being published in journals of interest. Many journals, even those that are available by subscription only, offer TOC alert services free of charge. In addition, a number of publishers and organizations offer TOC services (see online Technical Supplement). Use of TOCs is not proposed as a single alternative to the various other methods of study identification necessary for undertaking systematic reviews, rather as a supplementary method. (See also Chapter 22, Section 22.2 for a discussion of new technologies to support evidence surveillance in the context of 'living' systematic reviews.)

#### Timing of searches

The published review should be as up to date as possible. Searches for all the relevant databases should be rerun prior to publication, if the initial search date is more than 12 months (preferably six months) from the intended publication date (see MECIR Box 4.4.g). This is also good practice for searches of non-database sources. The results should also be screened to identify potentially eligible studies. Ideally, the studies should be incorporated fully in the review. If not, then the potentially eligible studies will need to be reported as references under 'Studies awaiting classification' (or under 'Ongoing studies' if they are not yet completed).

#### When to stop searching

Developing a search is often an iterative and exploratory process. It involves exploring trade-offs between search terms and assessing their overall impact on the sensitivity and precision of the search. It is often difficult to decide in a scientific or objective way when a search is complete and search strategy development can stop. The ability to decide when to stop typically develops through experience of developing many strategies. Suggestions for stopping rules have been made around the retrieval of new records, for example to stop if adding in a series of new terms to a database search strategy yields no new relevant records, or if precision falls below a particularcut-off [113]. Stopping might also be appropriate when the removal of terms or concepts results in missing relevant records. Another consideration is the amount of evidence that has already accrued: in topics where evidence is scarce, authors might need to be more cautious about deciding when to stop searching. Although many methods have been described to assist with deciding when to stop developing the search, there has been little formal evaluation of the approaches [10, 10].

At a basic level, investigation is needed as to whether a strategy is performing adequately. One simple test is to check whether the search is finding the publications that have been recommended as key publications or that have been included in other similar reviews (EUnetHTA 2017). It is not enough, however, for the strategy to find only those records, otherwise this might be a sign that the strategy is biased towards known studies and other relevant records might be being missed. In addition, citation searches and reference checking are useful checks of strategy performance. If those additional methods are finding documents that the searches have already retrieved, but that the team did not necessarily know about in advance, then this is one sign that the strategy might be performing adequately. Also, an evidence-based checklist such as the PRESS Evidence-Based Checklist [113] should be used to assess whether the search strategy is adequate (see Section 4.4.8). If some of the PRESS dimensions seem to be missing without adequate explanation or arouse concerns, then the search may not yet be complete.

Statistical techniques can be used to assess performance, such as capture-recapture [114] (also known as capture-mark-recapture; [114]), or the relative recall technique [114, 115]. Kastner suggests the capture-mark-recapture technique merits further investigation since it could be used to estimate the number of studies in a literature prospectively and to determine where to stop searches once suitable cut-off levels have been identified. Kastner's approach involves searching databases, conducting record selection, calculating capture-mark-recapture and then making decisions about whether further searches are necessary. This would entail potentially an iterative search and selection process. Capture-recapture needs results from at least two searches to estimate the number of missed studies. Further investigation of published prospective techniques seems warranted to learn more about the potential benefits.

Relative recall [114, 115] requires a range of searches to have been conducted so that the relevant studies have been built up by a set of sensitive searches. The performance of the individual searches can then be assessed in each individual database by determining how many of the studies that were deemed eligible for the evidence synthesis and were indexed within a database, can be found by the database search used to populate the synthesis. If a search in a database did not perform well and missed many studies, then that search strategy is likely to have been suboptimal. If the search strategy found most of the studies that were available to be found in the database then it was likely to have been a sensitive strategy. Assessments of precision could also be made, but these mostly inform future search approaches since they cannot affect the searches and record assessment already undertaken. Relative recall may be most useful at the end of the search process since it relies on the achievement of several searches to make judgements about the overall performance of strategies.

In evidence synthesis involving qualitative data, searching is often more organic and intertwined with the analysis such that the searching stops when new information ceases to be identified [6]. The reasons for stopping need to be documented and it is suggested that explanations or justifications for stopping may centre around saturation [6]. Further information on searches for qualitative evidence can be found in Chapter 21.

### Documenting and reporting the search process

Review authors should document the search process in enough detail to ensure that it can be reported correctly in the review (see MECIR Box 4.5.a). The searches of all the databases should be reproducible to the extent that this is possible. By documenting the search process, we refer to internal record-keeping, which is distinct from reporting the search process in the review (discussed in online Chapter III).

Medical/healthcare librarians and information specialists involved with the review should draft, or at least comment on, the search strategy sections of the review prior to publication.

There is currently no clear consensus regarding optimum reporting of systematic review search methods, although suboptimal reporting of commonly recommended items has been observed [20, 21, 22]. Research has also shown a lack of compliance with guidance in the _Handbook_ with respect to search strategy description in published Cochrane Reviews [20, 21, 22]. The PRISMA-Search (PRISMA-S) Extension, an extension to the PRISMA Statement, addressing the reporting of search strategies in systematic reviews, should go some way to addressing this, as should the major revision of PRISMA itself, which is due to report in 2019.

It is recommended that review authors seek guidance from their medical/healthcare librarian or information specialist at the earliest opportunity with respect to documenting the search process. For Cochrane Reviews, the bibliographic database search strategies should be copied and pasted into an appendix exactly as run and in full, together with the search set numbers and the total number of records retrieved by each search strategy. The search strategies should not be re-typed, because this can introduce errors. The same process is also good practice for searches of trials registers and other sources, where the interface used, such as introductory or advanced, should also be specified. Creating a report of the search process can be accomplished through methodical documentation of the steps taken by the searcher. This need not be onerous if suitable record keeping is performed during the process of the search, but it can be nearly impossible to recreate post hoc. Many database interfaces have facilities for search strategies to be saved online or to be emailed; an offline copy in text format should also be saved. For some databases, taking and saving a screenshot of the search may be the most practical approach [24].

Documenting the searching of sources other than databases, including the search terms used, is also required if searches are to be reproducible [15, 14, 16]. Details about contacting experts or manufacturers, searching reference lists, scanning websites, and decisions about search iterations can be kept internally for future updates or external requests and can be reproduced as an appendix in the final document. Since the purpose of search documentation is to support transparency, internal assessment, and reference for any future update, it is important to plan how to record searching of sources other than databases since some activities (contacting experts, reference list searching, and forward citation searching) will occur later on in the review process after the database results have been screened [24]. The searcher should record any correspondence on key decisions and report a summary of this correspondence alongside the search strategy. The narrative describes the major decisions that shaped the strategy and can give a peer reviewer an insight into the rationale for the search approach [11].

It is particularly important to save locally or file print copies of any information found on the internet, such as information about ongoing and/or unpublished trials, as this information may no longer be accessible at the time the review is written. Local copies should be stored in a structured way to allow retrieval when needed. There are also web-based tools which archive webpage content for future reference, such as WebCite [17]. The results of web searches will not be reproducible to the same extent as bibliographic database searches because web content and search engine algorithms frequently change, and search results can differ between users due to a general move towards localization and personalization. It is still important, however, to document the search process to ensure that the methods used can be transparently reported [1]. In cases where a search engine retrieves more results than it is practical to screen in full (it is rarely practical to search thousands of web results, as the precision of web searches is likely to be relatively low), the number of results that are documented and reported should be the number that were screened rather than the total number [15, 16].

Decisions should be documented for all records identified by the search. Details of the flow of studies from the number(s) of references identified in the search to the number of studies included in the review will need to be reported in the final review, ideally using a flow diagram such as that proposed by PRISMA (see online Chapter III); these can be generated using software including Covidence, DistillerSR, EPPI-Reviewer, the METAGEAR package for R, the PRISMA Flow Diagram Generator, and RevMan. A table of 'Characteristics of excluded studies' will also need to be presented (see Section 4.6.5). Numbers of records are sufficient for exclusions based on initial screening of titles and abstracts. Broad categorizations are sufficient for records classed as potentially eligible during an initial screen of the full text. Authors will need to decide for each review when to map records to studies (if multiple records refer to one study). The flow diagram records initially the total number of records retrieved from various sources, then the total number of studies to which these records relate. Review authors need to match the various records to the various studies in order to complete the flow diagram correctly. Lists of included and excluded studies must be based on studies rather than records (see also Section 4.6.1).

### Selecting studies

#### 4.6.1 Studies (not reports) as the unit of interest

A Cochrane Review is a review of studies that meet pre-specified eligibility criteria. Since each study may have been reported in several articles, abstracts or other reports, an extensive search for studies for the review may identify many reports for each potentially relevant study. Two distinct processes are therefore required to determine which studies can be included in the review. One is to link together multiple reports of the same study; and the other is to use the information available in the various reports to determine which studies are eligible for inclusion. Although sometimes there is a single report for each study, it should never be assumed that this is the case.

As well as the studies that inform the systematic review, other studies will also be identified and these should be recorded or tagged as they are encountered, so that they can be listed in the relevant tables in the review:

* records of ongoing trials for which results (either published or unpublished) are not (yet) available; and
* records of studies which seem to be eligible but for which data are incomplete or the publication related to the record could not be obtained.

#### 4.6.2 Identifying multiple reports from the same study

Duplicate publication can introduce substantial biases if studies are inadvertently included more than once in a meta-analysis (Tramer et al 1997). Duplicate publication can take various forms, ranging from identical manuscripts to reports describing different outcomes of the study or results at different time points (von Elm et al 2004). The number of participants may differ in the different publications. It can be difficult to detect duplicate publication and some 'detective work' by the review authors may be required.

Some of the most useful criteria for comparing reports are:

* trial identification numbers (e.g. ClinicalTrials.gov Identifier (NCT number); ISRCTN; Universal Trial Number (UTN) (assigned by the ICTRP); other identifiers such as those from the sponsor);
* author names (most duplicate reports have one or more authors in common, although this is not always the case);* location and setting (particularly if institutions, such as hospitals, are named);
* specific details of the interventions (e.g. dose, frequency);
* numbers of participants and baseline data; and
* date and duration of the study (which can also clarify whether different sample sizes are due to different periods of recruitment).

Where uncertainties remain after considering these and other factors, it may be necessary to correspond with the authors of the reports.

Multiple reports of the same study should be collated, so that each study, rather than each report, is the unit of interest in the review (see MECIR Box 4.6.a). Review authors will need to choose and justify which report (the primary report) to use as a source for study results, particularly if two reports include conflicting results. They should not discard other (secondary) reports, since they may contain additional outcome measures and valuable information about the design and conduct of the study.

#### 4.6.3 A typical process for selecting studies

A typical process for selecting studies for inclusion in a review is as follows (the process should be detailed in the protocol for the review):

1. Merge search results from different sources using reference management software, and remove duplicate records of the same report (i.e. records reporting the same journal title, volume and pages).
2. **Examine titles and abstracts** to remove obviously irrelevant reports (authors should generally be over-inclusive at this stage).
3. Retrieve the full text of the potentially relevant reports.
4. Link together multiple reports of the same study (see Section 4.6.2).
5. **Examine full-text reports** for compliance of studies with eligibility criteria.
6. Correspond with investigators, where appropriate, to clarify study eligibility (it may be appropriate to request further information, such as missing methods information or results, at the same time). If studies remain incomplete/unobtainable they should be tagged/recorded as incomplete, and should be listed in the table of 'Studies awaiting assessment' in the review.
7. Make final decisions on study inclusion and proceed to data collection.

* Tag or record any ongoing trials which have not yet been reported so that they can be added to the ongoing studies table.

Note that studies should not be omitted from a review solely on the basis of measured outcome data not being reported (see MECIR Box 4.6.b and Chapter 13).

#### 4.6.4 Implementation of the selection process

Decisions about which studies to include in a review are among the most influential decisions that are made in the review process and they involve judgement.

Use (at least) two people working independently to determine whether each study meets the eligibility criteria.

Ideally, screening of titles and abstracts to remove irrelevant reports should be done in duplicate by two people working independently (although it is acceptable that this initial screening of titles and abstracts is undertaken by only one person). It is essential, however, that two people working independently are used to make a final determination as to whether each study considered possibly eligible after title/abstract screening meets the eligibility criteria based on the full text of the study report(s) (see MECIR Box 4.6.c).

It has been shown that using at least two authors may reduce the possibility that relevant reports will be discarded (Edwards et al 2002) although other case reportshave suggested single screening approaches may be adequate (Doust et al 2005, Shemilt et al 2016). Opportunities for screening efficiencies seem likely to become available through promising developments in single human screening in combination with machine learning approaches (O'Mara-Eves et al 2015).

Experts in a particular area frequently have pre-formed opinions that can bias their assessment of both the relevance and validity of articles (Cooper and Rible 1989, Oxman and Guyatt 1993). Thus, while it is important that at least one author is knowledgeable in the area under review, it may be an advantage to have a second author who is not a content expert.

Disagreements about whether a study should be included can generally be resolved by discussion. Often the cause of disagreement is a simple oversight on the part of one of the review authors. When the disagreement is due to a difference in interpretation, this may require arbitration by another person. Occasionally, it will not be possible to resolve disagreements about whether to include a study without additional information. In these cases, authors may choose to categorize the study in their review as one that is awaiting assessment until the additional information is obtained from the study authors.

A single failed eligibility criterion is sufficient for a study to be excluded from a review. In practice, therefore, eligibility criteria for each study should be assessed in order of importance, so that the first 'no' response can be used as the primary reason for exclusion of the study, and the remaining criteria need not be assessed. The eligibility criteria order may be different in different reviews and they do not always need to be the same.

For most reviews it will be worthwhile to pilot test the eligibility criteria on a sample of reports (say six to eight articles, including ones that are thought to be definitely eligible, definitely not eligible and doubtful). The pilot test can be used to refine and clarify the eligibility criteria, train the people who will be applying them and ensure that the criteria can be applied consistently by more than one person.

For Cochrane Reviews the selection process must be documented in sufficient detail to be able to complete a flow diagram and a table of 'Characteristics of excluded studies' (see MECIR Box 4.6.d). During the selection process it is crucial to keep track of the number of references and subsequently the number of studies so that a flow diagram can be constructed. The decision and reasons for exclusion can be tracked using reference software, a simple document or spreadsheet, or using specialist systematic review software (see Section 4.6.6.1).

#### Selecting 'excluded studies'

A Cochrane Review includes a list of excluded studies called 'Characteristics of excluded studies', detailing the specific reason for exclusion for any studies that a reader might plausibly expect to see among the included studies. This covers all studies that may, on the surface, appear to meet the eligibility criteria but which, on further inspection, do not. It also covers those that do not meet all of the criteria but are well known and likely to be thought relevant by some readers. By listing such studies as excluded and giving the primary reason for exclusion, the review authors can show that consideration has been given to these studies. The list of excluded studies should be as brief as possible. It should not list all of the reports that were identified by an extensive search. It should not list studies that obviously do not fulfil the eligibility criteria for the review, such as 'Types of studies', 'Types of participants', and 'Types of interventions'. In particular, it should not list studies that are obviously not randomized if the review includes only randomized trials. Based on a (recent) sample of approximately 60% of the intervention reviews in The Cochrane Library which included randomized trials (only), the average number of studies listed in the 'excluded studies' table is 30.

#### 4.6.6 Software support for selecting studies

An extensive search for eligible studies in a systematic review can often identify thousands of records that need to be manually screened. Selecting studies from within these records can be a particularly time-consuming, laborious and logistically challenging aspect of conducting a systematic review. These and other challenges have led to the development of various software tools and packages that offer support for the selection process.

Broadly, software to support selecting studies can be classified as:

* systems that support the study selection process, typically involving multiple reviewers (see Section 4.6.6.1); and
* tools and techniques based on text mining and/or machine learning, which aim to semi- or fully-automate the selection process (see Section 4.6.6.2).

Software to support the selection process, along with other stages of a systematic review, including text mining tools, can be identified using the Systematic Review Toolbox. The SR Toolbox is a community driven, web-based catalogue of tools that provide support for systematic reviews [15].

##### 4.6.6.1 Software for managing the selection process

Managing the selection process can be challenging, particularly in a large-scale systematic review that involves multiple reviewers. Basic productivity tools can help (such as word processors, spreadsheets and reference management software), and several purpose-built systems are also available that offer support for the study selection process.

Examples of tools that support selecting studies include:

* a free web-based screening tool that can prioritize the screening of records using machine learning techniques.
* a web-based software platform for conducting systematic reviews, which includes support for collaborative title and abstract screening, full-text review, risk-of-bias assessment and data extraction. Full access to this system normally requires a paid subscription but is free for authors of Cochrane Reviews. A free trial for non-Cochrane review authors is also available.
* a web-based software application for undertaking bibliographic record screening and data extraction. It has a number of management features to track progress, assess interrater reliability and export data for further analysis. Reduced pricing for Cochrane and Campbell reviews is available.
* web-based software designed to support all stages of the systematic review process, including reference management, screening, risk of bias assessment, data extraction and synthesis. The system is free to use for Cochrane and Campbell reviews, otherwise it requires a paid subscription. A free trial is available.
* a web-based application for collaborative citation screening and full-text selection. The system is currently available free of charge [16].

Compatibility with other software tools used in the review process (such as RevMan) may be a consideration when selecting a tool to support study selection.

Coyidence and EPPI-Reviewer are Cochrane-preferred tools, and are likely to have the strongest integration with RevMan.

##### 4.6.6.2 Automating the selection process

Research into automating the study selection process through machine learning and text mining has received considerable attention over recent years, resulting in the development of various tools and techniques for reviewers to consider. The use of automated tools has the potential to reduce the workload involved with selecting studies significantly [17]. For example, research suggests that adopting automation can reduce the need for manual screening by at least 30% and possibly more than 90%, although sometimes at the cost of up to a 5% reduction in sensitivity [14].

Machine learning models (or 'classifiers') can be built where sufficient data are available. Of particular practical use to Cochrane Review authors is a classifier (the 'RCT Classifier') that can identify reports of randomized trials based on titles and abstracts. The classifier is highly accurate because it is built on a large dataset of hundreds of thousands of records screened by Cochrane Crowd, Cochrane's citizen science platform, where contributors help to identify and describe health research [15]. Guidance on using the RCT Classifier in Cochrane Reviews, for example to exclude studies already flagged as not being randomized trials, or to access Cochrane Crowd to assist with screening, is available from the Cochrane Information Specialists' handbook [16].

In addition to learning from large datasets such as those generated by Cochrane Crowd, it is also possible for machine learning models to learn how to apply eligibility criteria for individual reviews. This approach uses a process called 'active learning' and it is able to semi-automate study selection by continuously promoting records most likely to be relevant to the top of the results list [14]. It is difficult for authors to determine in advance when it is safe to stop screening and allow some records to be eliminated automatically without manual assessment. The automatic elimination of records using this approach has not been recommended for use in Cochrane Reviews at the time of writing. This active learning process can still be useful, however, since by prioritizing records for screening in order of relevance, it enables authors to identify the studies that are most likely to be included much earlier in the screening process than would otherwise be possible. A number of software tools support 'active learning' including:

* Abstrackr ([http://abstrackr.cebm.brown.edu/](http://abstrackr.cebm.brown.edu/));
* Colandr ([https://www.colandrapp.com/](https://www.colandrapp.com/));
* EPPI-Reviewer ([http://eppi.ioe.ac.uk/](http://eppi.ioe.ac.uk/));
* Rayyan ([http://rayyan.qcri.org/](http://rayyan.qcri.org/));
* RobotAnalyst ([http://nactem.ac.uk/robotanalyst/](http://nactem.ac.uk/robotanalyst/)); and
* Swift-review ([http://swift.sciome.com/swift-review/](http://swift.sciome.com/swift-review/)).

Finally, tools are available that use natural language processing to highlight sentences and key phrases automatically (e.g. PICO elements, trial characteristics, details of randomization) to support the reviewer whilst screening [15].

### Chapter information

**Authors:** Carol Lefebvre, Julie Glanville, Simon Briscoe, Anne Littlewood, Chris Marshall, Maria-Inti Metzendorf, Anna Noel-Storr, Tamara Rader, Farhad Shokraneh, James Thomas, L. Susan Wieland; on behalf of the Cochrane Information Retrieval Methods Group

**Acknowledgements:** This chapter has been developed from sections of previous editions of the Cochrane Handbook co-authored since 1995 by Kay Dickersin, Julie Glanville, Kristen Larson, Carol Lefebvre and Eric Manheimer. Many of the sources listed in this chapter and the accompanying online Technical Supplement have been brought to our attention by a variety of people over the years and we should like to acknowledge this. We should like to acknowledge: Ruth Foxlee, (formerly) Information Specialist, Cochrane Editorial Unit; Miranda Cumpston, (formerly) Head of Learning & Support, Cochrane Central Executive; Colleen Finley, Product Manager, John Wiley and Sons, for checking sections relating to searching the Cochrane Library; the (UK) National Institute for Health and Care Excellence and the German Institute for Quality and Efficiency in Health Care (IQWIG) for support in identifying some of the references; the (US) Agency for Healthcare Research and Quality (AHRQ) Effective Healthcare Program Scientific Resource Center Article Alert service; Tianjing Li, Co-Convenor, Comparing Multiple Interventions Methods Group, for text and references that formed the basis of the re-drafting of parts of Section 4.6 Selecting studies; Lesley Gillespie, Cochrane author and former Editor and Trials Search Co-ordinator of the Cochrane Bone, Joint and Muscle Trauma Group, for copy-editing an early draft; the Cochrane Information Specialist Executive, the Cochrane Information Specialists' Support Team, Cochrane Information Specialists and members of the Cochrane Information Retrieval Methods Group for comments on drafts; Su Golder, Co-Convenor, Adverse Effects Methods Group and Steve McDonald, Co-Director, Cochrane Australia for peer review.

### References

Agency for Healthcare Research and Quality. Methods guide for effectiveness and comparative effectiveness reviews: AHRQ publication no. 10(14)-EHC063-EF. 2014. [https://effectivehealthcare.ahrq.gov/topics/cer-methods-guide/overview](https://effectivehealthcare.ahrq.gov/topics/cer-methods-guide/overview).

Arber M, Cikalo M, Glanville J, Lefebvre C, Varley D, Wood H. Annotated bibliography of published studies addressing searching for unpublished studies and obtaining access to unpublished data. 2013. [https://methods.cochrane.org/sites/methods.cochrane.org](https://methods.cochrane.org/sites/methods.cochrane.org).

irmg/files/public/uploads/Annotatedbibliographtifyingunpublishedstudies.pdf.

Atkinson KM, Koenka AC, Sanchez CE, Moshontz H, Cooper H. Reporting standards for literature searches and report inclusion criteria: making research syntheses more transparent and easy to replicate. _Research Synthesis Methods_ 2015; **6**: 87-95.

Bai Y, Gao J, Zou D, Li Z. Is MEDLINE alone enough for a meta-analysis? _Allimentary Pharmacology and Therapeutics_ 2007; **26**: 125-126; author reply 126.

Baudard M, Yavchitz A, Ravaud P, Perrodeau E, Boutron I. Impact of searching clinical trial registries in systematic reviews of pharmaceutical treatments: methodological systematic review and reanalysis of meta-analyses. _BMJ_ 2017; **356**: j448.
* Bennett _et al._ (2003) Bennett DA, Jull A. FDA: untapped source of unpublished trials. _Lancet_ 2003; **361**: 1402-1403.
* Bero _et al._ (2013) Bero L. Searching for unpublished trials using trials registers and trials web sites and obtaining unpublished trial data and corresponding trial protocols from regulatory agencies. 2013. [http://web.archive.org/web/20150108071243/http://methods.ochrane.org:80/projects-developments/searching-unpublished-trials-using-trials-registers-and-trials-web-sites-and-o](http://web.archive.org/web/20150108071243/http://methods.ochrane.org:80/projects-developments/searching-unpublished-trials-using-trials-registers-and-trials-web-sites-and-o).
* Booth _et al._ (2010) Booth A. How much searching is enough? Comprehensive versus optimal retrieval for technology assessments. _International Journal of Technology Assessment in Health Care_ 2010; **26**: 431-435.
* Booth _et al._ (2016) Booth A. Searching for qualitative research for inclusion in systematic reviews: a structured methodological review. _Systematic Reviews_ 2016; **5**: 74.
* Bramer (2016) Bramer WM. Variation in number of hits for complex searches in Google Scholar. _Journal of the Medical Library Association_ 2016; **104**: 143-145.
* Briscoe (2018) Briscoe S. A review of the reporting of web searching to identify studies for Cochrane systematic reviews. _Research Synthesis Methods_ 2018; **9**: 89-99.
* Brusselle _et al._ (2015) Brusselle GG, Blasi F. Risk of a biased assessment of the evidence when limiting literature searches to the English language: macrolides in asthma as an illustrative example. _Pulmonary Pharmacology and Therapeutics_ 2015; **31**: 109-110.
* Chan (2012) Chan AW. Out of sight but not out of mind: how to search for unpublished clinical trial evidence. _BMJ_ 2012; **344**: d8013.
* Chapman _et al._ (2014) Chapman SJ, Shelton B, Mahmood H, Fitzgerald JE, Harrison EM, Bhangu A. Discontinuation and non-publication of surgical randomised controlled trials: observational study. _BMJ_ 2014; **349**: g6870.
* Chilcott _et al._ (2003) Chilcott J, Brennan A, Booth A, Karnon J, Tappenden P. The role of modelling in prioritising and planning clinical trials. _Health Technology Assessment_ 2003; **7**: iii, 1-125.
* Chow (2015) Chow TK. Electronic search strategies should be repeatable. _European Journal of Pain_ 2015; **19**: 1562-1563.
* Cook _et al._ (1993) Cook DJ, Guyatt GH, Ryan G, Clifton J, Buckingham L, Willan A, McIlroy W, Oxman AD. Should unpublished data be included in meta-analyses? Current convictions and controversies. _JAMA_ 1993; **269**: 2749-2753.
* Cooper and Ribble (1989) Cooper H, Ribble RG. Influences on the outcome of literature searches for integrative research reviews. _Science Communication_ 1989; **10**: 179-201.
* what is the value of adding a narrative to peer-review checklists? A case study of NICE interventional procedures guidance. _Evidence Based Library and Information Practice_ 2011; **6**: 72-87.
* de Vet _et al._ (2008) de Vet H, Eisinga A, Riphagen I, Aertegeerts B, Pewsner D. Chapter 7: Searching for studies. In: Deeks J, Bossuyt P, Gatsonis C, editors. _Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy Version 04 (updated September 2008)_: The Cochrane Collaboration; 2008. [https://methods.ochrane.org/sites/methods.ochrane.org.sdt/files/public/uploads/Chapter07-Searching-9%28September-2008%29.pdf](https://methods.ochrane.org/sites/methods.ochrane.org.sdt/files/public/uploads/Chapter07-Searching-9%28September-2008%29.pdf).
* Dellavalle _et al._ (2003) Dellavalle RP, Hester EJ, Heilig LF, Drake AL, Kuntzman JW, Graber M, Schilling LM. Information science. Going, going, gone: lost Internet references. _Science_ 2003; **302**: 787-788.
* D'Auria _et al._ (2008)* [19] Doshi P, Dickersin K, Healy D, Vedula SS, Jefferson T. Restoring invisible and abandoned trials: a call for people to publish the findings. _BMJ_ 2013; **346**: f2865.
* [20] Doutst JA, Pietrzak E, Sanders S, Glasziou PP. Identifying studies for systematic reviews of diagnostic tests was difficult due to the poor sensitivity and precision of methodologic filters and the lack of information in the abstract. _Journal of Clinical Epidemiology_ 2005; **58**: 444-449.
* [21] Easterbrook PJ, Berlin JA, Gopalan R, Matthews DR. Publication bias in clinical research. _Lancet_ 1991; **337**: 867-872.
* [22] Edwards P, Clarke M, DiGuiseppi C, Pratap S, Roberts I, Wentz R. Identification of randomized controlled trials in systematic reviews: accuracy and reliability of screening records. _Statistics in Medicine_ 2002; **21**: 1635-1640.
* [23] Egger M, Zellweger-Zahner T, Schneider M, Junker C, Lengeler C, Antes G. Language bias in randomised controlled trials published in English and German. _Lancet_ 1997; **350**: 326-329.
* [24] Egger M, Smith GD. Bias in location and selection of studies. _BMJ_ 1998; **316**: 61-66.
* [25] Elsevier. Embase content 2016a. [https://www.elsevier.com/solutions/embase-biomedical-research/embase-coverage-and-content](https://www.elsevier.com/solutions/embase-biomedical-research/embase-coverage-and-content).
* [26] Elsevier. Embase classic fact sheet 2016b. [https://www.elsevier.com/__data/assets/pdf_file/0005/58982/R_D-Solutions_Embase_Fact-Sheet_Classic-DIGITAL.pdf](https://www.elsevier.com/__data/assets/pdf_file/0005/58982/R_D-Solutions_Embase_Fact-Sheet_Classic-DIGITAL.pdf).
* [27] EluentHTA. Process of information retrieval for systematic reviews and health technology assessments on clinical effectiveness (Version 1.2). Germany: European network for Health Technology Assessment; 2017. [https://www.eunethta.eu/wp-content/uploads/2018/01/Guideline_Information_Retrieval_V1-2_2017.pdf](https://www.eunethta.eu/wp-content/uploads/2018/01/Guideline_Information_Retrieval_V1-2_2017.pdf).
* [28] European Food Safety Authority. Application of systematic review methodology to food and feed safety assessments to support decision making. _EFSA Journal_ 2010; **8**: 1637. doi:10.2903/j.efsa.2010.1637. www.efsa.europa.eu.
* [29] Eysenbach G, Trudel M. Going, going, still there: using the WebCite service to permanently archive cited web pages. _Journal of Medical Internet Research_ 2005; **7**: e60.
* [30] Franco JVA, Garrote VL, Escobar Liquidity CM, Vietto V. Identification of problems in search strategies in Cochrane Reviews. _Research Synthesis Methods_ 2018; **9**: 408-416.
* [31] Glanville J, Lefebvre C, Wright Ke, editors. ISSG search filter resource York (UK): The InterTASC Information Specialists' Sub-Group 2019. [https://sites.google.com/a/york.ac.uk/issg-search-filters-resource/home](https://sites.google.com/a/york.ac.uk/issg-search-filters-resource/home).
* [32] Glanville JM, Duffy S, McCool R, Varley D. Searching ClinicalTrials.gov and the International Clinical Trials Registry Platform to inform systematic reviews: what are the optimal search approaches? _Journal of the Medical Library Association_ 2014; **102**: 177-183.
* [33] Goldacre B, Lane S, Mahtani KR, Heneghan C, Onakpoya I, Bushfield I, Smeeth L. Pharmaceutical companies' policies on access to trial data, results, and methods: audit study. _BMJ_ 2017; **358**: j3334.
* [34] Greenhalgh T, Peacock R. Effectiveness and efficiency of search methods in systematic reviews of complex evidence: audit of primary sources. _BMJ_ 2005; **331**: 1064-1065.
* [35] Haddaway NR, Collins AM, Coughlin D, Kirk S. The role of Google Scholar in evidence reviews and its applicability to grey literature searching. _PloS One_ 2015; **10**: e0138237.
* [36] Halfpenny NJ, Quigley JM, Thompson JC, Scott DA. Value and usability of unpublished data sources for systematic reviews and network meta-analyses. _Evidence-Based Medicine_ 2016; **21**: 208-213.
* [37]Halladay CW, Trikalinos TA, Schmid IT, Schmid CH, Dahabreh IJ. Using data sources beyond PubMed has a modest impact on the results of systematic reviews of therapeutic interventions. _Journal of Clinical Epidemiology_ 2015; **68**: 1076-1084.
* Hartling et al. (2016) Hartling L, Featherstone R, Nuspl M, Shave K, Dryden DM, Vandermeer B. The contribution of databases to the results of systematic reviews: a cross-sectional study. _BMC Medical Research Methodology_ 2016; **16**: 127.
* Hartling et al. (2017) Hartling L, Featherstone R, Nuspl M, Shave K, Dryden DM, Vandermeer B. Grey literature in systematic reviews: a cross-sectional study of the contribution of non-English reports, unpublished studies and dissertations to the results of meta-analyses in child-relevant reviews. _BMC Medical Research Methodology_ 2017; **17**: 64.
* Hetherington et al. (1989) Hetherington J, Dickersin K, Chalmers I, Meinert CL. Retrospective and prospective identification of unpublished controlled trials: lessons from a survey of obstetricians and pediatricians. _Pediatrics_ 1989; **84**: 374-380.
* Hinde and Spackman (2015) Hinde S, Spackman E. Bidirectional citation searching to completion: an exploration of literature searching methods. _Pharmacoeconomics_ 2015; **33**: 5-11.
* Horton (1997) Horton R. Medical editors trial amnesty. _Lancet_ 1997; **350**: 756.
* Huang et al. (2006) Huang X, Lin J, Demner-Fushman D. Evaluation of PICO as a knowledge representation for clinical questions. _AMIA Annual Symposium Proceedings/AMIA Symposium_ 2006: 359-363.
* Hwang et al. (2016) Hwang TJ, Carpenter D, Lauffenburger JC, Wang B, Franklin JM, Kesselheim AS. Failure of investigational drugs in late-stage clinical development and publication of trial results. _JAMA Internal Medicine_ 2016; **176**: 1826-1833.
* Iansavichene et al. (2008) Iansavichene AE, Sampson M, McGowan J, Ajiferuke IS. Should systematic reviewers search for randomized, controlled trials published as letters? _Annals of Internal Medicine_ 2008; **148**: 714-715.
* Institute of Medicine (2011) Institute of Medicine. Finding what works in health care: standards for systematic reviews. Washington, DC: The National Academies Press; 2011. [http://books.nap.edu/openbook.php?record_id=13059](http://books.nap.edu/openbook.php?record_id=13059)
* Irvin and Hayden (2006) Irvin E, Hayden J. Developing and testing an optimal search strategy for identifying studies of prognosis [Poster]. 14th Cochrane Colloquium; 2006 October 23-26; Dublin, Ireland; 2006. [https://abstracts.cochrane.org/2006-dublin/developing-and-testing-optimal-search-strategy-identifying-studies-prognosis](https://abstracts.cochrane.org/2006-dublin/developing-and-testing-optimal-search-strategy-identifying-studies-prognosis).
* Isojarvi et al. (2018) Isojarvi J, Wood H, Lefebvre C, Glanville J. Challenges of identifying unpublished data from clinical trials: getting the best out of clinical trials registers and other novel sources. _Research Synthesis Methods_ 2018; **9**: 561-578.
* Jefferson et al. (2018) Jefferson T, Doshi P, Boutron I, Golder S, Heneghan C, Hodkinson A, Jones M, Lefebvre C, Stewart LA. When to include clinical study reports and regulatory documents in systematic reviews. _BMJ Evidence-Based Medicine_ 2018; **23**: 210-217.
* Jiao et al. (2013) Jiao S, Tsutani K, Haga N. Review of Cochrane reviews on acupuncture: how Chinese resources contribute to Cochrane reviews. _Journal of Alternative and Complementary Medicine_ 2013; **19**: 613-621.
* Kaptchuk (1999) Kaptchuk T. Certain countries produce only positive trial results. _Focus on Alternative and Complementary Therapies_ 1999; **4**: 86-87.
* Kastner et al. (2009) Kastner M, Straus SE, McKibbon KA, Goldsmith CH. The capture-mark-recapture technique can be used as a stopping rule when searching in systematic reviews. _Journal of Clinical Epidemiology_ 2009; **62**: 149-157.
* Kastner et al. (2018)