* Knelangen et al. (2018) Knelangen M, Hausner E, Metzendorf M-I, Sturtz S, Waffenschmidt S. Trial registry searches for randomized controlled trials of new drugs required registry-specific adaptation to achieve adequate sensitivity. _Journal of Clinical Epidemiology_ 2018; **94**: 69-75.
* Kohler et al. (2015) Kohler M, Haag S, Biester K, Brockhaus AC, McGauran N, Grouven U, Kolsch H, Seay U, Horn H, Moritz G, Staeck K, Wieseler B. Information on new drugs at market entry: retrospective analysis of health technology assessment reports versus regulatory reports, journal publications, and registry reports. _BMJ_ 2015; **350**: h796.
* Kreis et al. (2014) Kreis J, Panteli D, Busse R. How health technology assessment agencies address the issue of unpublished data. _International Journal of Technology Assessment in Health Care_ 2014; **30**: 34-43.
* Lampert et al. (2016) Lampert A, Hoffmann GF, Ries M. Ten years after the International Committee of Medical Journal Editors' clinical trial registration initiative, one quarter of phase 3 pediatric epilepsy clinical trials still remain unpublished: a cross sectional analysis. _PloS One_ 2016; **11**: e0144973.
* Layton (2017) Layton D. A critical review of search strategies used in recent systematic reviews published in selected prosthodontic and implant-related journals: are systematic reviews actually systematic? _International Journal of Prosthodontics_ 2017; **30**: 13-21.
* Lee et al. (2008) Lee K, Bacchetti P, Sim I. Publication of clinical trials supporting successful new drug applications: a literature analysis. _PLoS Medicine_ 2008; **5**: e191.
* the contribution of EMBASE records to the Cochrane Central Register of Controlled Trials (CENTRAL) in The Cochrane Library. _Emerging Themes in Epidemiology_ 2008; **5**: 13.
* Lefebvre et al. (2013) Lefebvre C, Glanville J, Wieland LS, Coles B, Weightman AL. Methodological developments in searching for studies for systematic reviews: past, present and future? _Systematic Reviews_ 2013; **2**: 78.
* Littlewood and Bridges (2017) Littlewood A, Bridges C, for the Cochrane Information Specialist Support Team. Cochrane Information Specialists' Handbook Oslo: The Cochrane Collaboration; 2017. [http://training.cochrane.org/resource/cochrane-information-specialists-handbook](http://training.cochrane.org/resource/cochrane-information-specialists-handbook).
* Lorenc et al. (2014) Lorenc T, Tyner EF, Petticrew M, Duffy S, Martineau FP, Phillips G, Lock K. Cultures of evidence across policy sectors: systematic review of qualitative evidence. _European Journal of Public Health_ 2014; **24**: 1041-1047.
* Lorenzetti et al. (2014) Lorenzetti DL, Topfer LA, Dennett L, Clement F. Value of databases other than MEDLINE for rapid health technology assessments. _International Journal of Technology Assessment in Health Care_ 2014; **30**: 173-178.
* MacLean et al. (2003) MacLean CH, Morton SC, Ofman JJ, Roth EA, Shekelle PG; Southern California Evidence-Based Practice Center. How useful are unpublished data from the Food and Drug Administration in meta-analysis? _Journal of Clinical Epidemiology_ 2003; **56**: 44-51.
* Manheimer and Anderson (2002) Manheimer E, Anderson D. Survey of public information about ongoing clinical trials funded by industry: evaluation of completeness and accessibility. _BMJ_ 2002; **325**: 528-531.
* Marshall and Brereton (2015) Marshall C, Brereton P. Systematic review toolbox: a catalogue of tools to support systematic reviews. _Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering (EASE)_ 2015: Article no. 23.
* Marshall et al. (2016)Marshall I, Noel-Storr A, Kuiper J, Thomas J, Wallace BC. Machine learning for identifying randomized controlled trials: an evaluation and practitioner's guide. _Research Synthesis Methods_ 2018; **9**: 602-614.
* McGowan et al. (2016) McGowan J, Sampson M, Salzwedel D, Cogo E, Foerster V, Lefebvre C. PRESS Peer Review of Electronic Search Strategies: 2015 Guideline Explanation and Elaboration (PRESS E&E). Ottawa: CADTH; 2016a. [https://www.cadth.ca/sites/default/files/pdf/CP0015_PRESS_Update_Report_2016.pdf](https://www.cadth.ca/sites/default/files/pdf/CP0015_PRESS_Update_Report_2016.pdf).
* McGowan et al. (2016) McGowan J, Sampson M, Salzwedel DM, Cogo E, Foerster V, Lefebvre C. PRESS Peer Review of Electronic Search Strategies: 2015 Guideline Statement. _Journal of Clinical Epidemiology_ 2016b; **75**: 40-46.
* Meert et al. (2016) Meert D, Torabi N, Costella J. Impact of librarians on reporting of the literature searching component of pediatric systematic reviews. _Journal of the Medical Library Association_ 2016; **104**: 267-277.
* a Cochrane perspective. _Journal of the European Association for Health Information and Libraries_ 2016; **12**(4): 6-9.
* Moher et al. (2003) Moher D, Pham B, Lawson ML, Klassen TP. The inclusion of reports of randomised trials published in languages other than English in systematic reviews. _Health Technology Assessment_ 2003; **7**: 1-90.
* Morrison et al. (2012) Morrison A, Polisena J, Husereau D, Moulton K, Clark M, Fiander M, Mierzwinski-Urban M, Clifford T, Hutton B, Rabb D. The effect of English-language restriction on systematic review-based meta-analyses: a systematic review of empirical studies. _International Journal of Technology Assessment in Health Care_ 2012; **28**: 138-144.
* Mullins et al. (2010) Mullins MM, DeLuca JB, Crepaz N, Lyles CM. Reporting quality of search methods in systematic reviews of HIV behavioral interventions (2000-2010): are the searches clearly explained, systematic and reproducible? _Research Synthesis Methods_ 2014; **5**: 116-130.
* Niederstadt and Droste (2010) Niederstadt C, Droste S. Reporting and presenting information retrieval processes: the need for optimizing common practice in health technology assessment. _International Journal of Technology Assessment in Health Care_ 2010; **26**: 450-457.
* O'Mara-Eves et al. (2015) O'Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text mining for study identification in systematic reviews: a systematic review of current approaches. _Systematic Reviews_ 2015; **4**: 5. (Erratum in: _Systematic Reviews_ 2015; **4**: 59).
* Oxman and Guyatt (1993) Oxman AD, Guyatt GH. The science of reviewing research. _Annals of the New York Academy of Sciences_ 1993; **703**: 125-133; discussion 133-134.
* Pansieri et al. (2017) Pansieri C, Pandolfini C, Bonati M. Clinical trial registries: more international, converging efforts are needed. _Trials_ 2017; **18**: 86.
* Petticrew and Roberts (2006) Petticrew M, Roberts H. _Systematic Reviews in the Social Sciences: a Practical Guide_. Oxford (UK): Blackwell; 2006.
* Pham et al. (2005) Pham B, Klassen TP, Lawson ML, Moher D. Language of publication restrictions in systematic reviews gave different results depending on whether the intervention was conventional or complementary. _Journal of Clinical Epidemiology_ 2005; **58**: 769-776.
* Pilkington et al. (2005) Pilkington K, Boshnakova A, Clarke M, Richardson J. 'No language restrictions' in database searches: what does this really mean? _Journal of Alternative and Complementary Medicine_ 2005; **11**: 205-207.
* Ponsieri et al. (2015)* Rader et al. (2014) Rader T, Mann M, Stansfield C, Cooper C, Sampson M. Methods for documenting systematic review searches: a discussion of common issues. _Research Synthesis Methods_ 2014; **5**: 98-115.
* Relevo and Paynter (2012) Relevo R, Paynter R. Peer Review of Search Strategies. Rockville (MD): Agency for Healthcare Research and Quality (US); 2012. [https://www.ncbi.nlm.nih.gov/books/NBK98353/](https://www.ncbi.nlm.nih.gov/books/NBK98353/).
* Rethlefsen et al. (2015) Rethlefsen ML, Farrell AM, Osterhaus Trzasko LC, Brigham TJ. Librarian co-authors correlated with higher quality reported search strategies in general internal medicine systematic reviews. _Journal of Clinical Epidemiology_ 2015; **68**: 617-626.
* Reveiz et al. (2006) Reveiz L, Cardona AF, Ospina EG, de Agular S. An e-mail survey identified unpublished studies for systematic reviews. _Journal of Clinical Epidemiology_ 2006; **59**: 755-758.
* Rice et al. (2016) Rice DB, Kloda LA, Levis B, Qi B, Kingsland E, Thombs BD. Are MEDLINE searches sufficient for systematic reviews and meta-analyses of the diagnostic accuracy of depression screening tools? A review of meta-analyses. _Journal of Psychosomatic Research_ 2016; **87**: 7-13.
* Richardson et al. (1995) Richardson WS, Wilson MC, Nishikawa J, Hayward RS. The well-built clinical question: a key to evidence-based decisions. _ACP Journal Club_ 1995; **123**: A12-13.
* Roundtree et al. (2009) Roundtree AK, Kallen MA, Lopez-Olivo MA, Kimmel B, Skidmore B, Ortiz Z, Cox V, Suarez-Almazor ME. Poor reporting of search strategy and conflict of interest in over 250 narrative and systematic reviews of two biologic agents in arthritis: a systematic review. _Journal of Clinical Epidemiology_ 2009; **62**: 128-137.
* Royle et al. (2003) Royle P, Milne R. Literature searching for randomized controlled trials used in Cochrane reviews: rapid versus exhaustive searches. _International Journal of Technology Assessment in Health Care_ 2003; **19**: 591-603.
* Sampson et al. (2003) Sampson M, Barrowman NJ, Moher D, Klassen TP, Pham B, Platt R, St John PD, Viola R, Raina P. Should meta-analysts search Embase in addition to Medline? _Journal of Clinical Epidemiology_ 2003; **56**: 943-955.
* Sampson et al. (2006) Sampson M, Zhang L, Morrison A, Barrowman NJ, Clifford TJ, Platt RW, Klassen TP, Moher D. An alternative to the hand searching gold standard: validating methodological search filters using relative recall. _BMC Medical Research Methodology_ 2006; **6**: 33.
* Sampson and McGowan (2006) Sampson M, McGowan J. Errors in search strategies were identified by type and frequency. _Journal of Clinical Epidemiology_ 2006; **59**: 1057-1063.
* Sampson et al. (2008) Sampson M, McGowan J, Tetzlaff J, Cogo E, Moher D. No consensus exists on search reporting methods for systematic reviews. _Journal of Clinical Epidemiology_ 2008; **61**: 748-754.
* Sampson and McGowan (2011) Sampson M, McGowan J. Inquisitio validus Index Medicus: a simple method of validating MEDLINE systematic review searches. _Research Synthesis Methods_ 2011; **2**: 103-109.
* Sampson et al. (2016) Sampson M, de Bruijn B, Urquhart C, Shojania K. Complementary approaches to searching MEDLINE may be sufficient for updating systematic reviews. _Journal of Clinical Epidemiology_ 2016; **78**: 108-115.
* Scherer et al. (2015) Scherer RW, Ugarte-Gil C, Schmucker C, Meerpohl JJ. Authors report lack of time as main reason for unpublished research presented at biomedical conferences: a systematic review. _Journal of Clinical Epidemiology_ 2015; **68**: 803-810.
* Schroll et al. (2013) Schroll JB, Bero L, Getzsche PC. Searching for unpublished data for Cochrane reviews: cross sectional study. _BMJ_ 2013; **346**: f2231.
* Schroll et al. (2014)Shemilt I, Simon A, Hollands GJ, Marteau TM, Ogilvie D, O'Mara-Eves A, Kelly MP, Thomas J. Pinpointing needles in giant haystacks: use of text mining to reduce impractical screening workload in extremely large scoping reviews. _Research Synthesis Methods_ 2014; **5**: 31-49.
* [Shemilt et al.2016] Shemilt I, Khan N, Park S, Thomas J. Use of cost-effectiveness analysis to compare the efficiency of study identification methods in systematic reviews. _Systematic Reviews_ 2016; **5**: 140.
* [Spoor et al.1996] Spoor P, Airey M, Bennett C, Greensill J, Williams R. Use of the capture-recapture technique to evaluate the completeness of systematic literature searches. _BMJ_ 1996; **313**: 342-343.
* [Spy et al.2013] Spry C, Mierzwinski-Urban M, Rabb D. Peer review of literature search strategies: does it make a difference? 21st Cochrane Colloquium; 2013; Quebec City, Canada. [https://abstracts.cochrane.org/2013-quebec-city/peer-review-literature-search-strategies-does-it-make-difference](https://abstracts.cochrane.org/2013-quebec-city/peer-review-literature-search-strategies-does-it-make-difference).
* [Stevinson et al.2004] Stevinson C, Lawlor DA. Searching multiple databases for systematic reviews: added value or diminishing returns? _Complementary Therapies in Medicine_ 2004; **12**: 228-232.
* [Suarez-Almazor et al.2000] Suarez-Almazor ME, Belseck E, Homik J, Dorgan M, Ramos-Remus C. Identifying clinical trials in the medical literature with electronic databases: MEDLINE alone is not enough. _Controlled Clinical Trials_ 2000; **21**: 476-487.
* [Thomas et al.2017] Thomas J, Noel-Storr A, Marshall I, Wallace B, McDonald S, Mavergames C, Glasziou P, Shemilt I, Synnot A, Turner T, Elliott J; Living Systematic Review Network. Living systematic reviews: 2. Combining human and machine effort. _Journal of Clinical Epidemiology_ 2017; **91**: 31-37.
* [Tramer et al.1997] Tramer MR, Reynolds DJ, Moore RA, McQuay HJ. Impact of covert duplicate publication on meta-analysis: a case study. _BMJ_ 1997; **315**: 635-640.
* [Tsafnat et al.2014] Tsafnat G, Glasziou P, Choong MK, Dunn A, Galgani F, Coiera E. Systematic review automation technologies. _Systematic Reviews_ 2014; **3**: 74.
* [US National Library of Medicine. PubMed.2018] US National Library of Medicine. PubMed. 2018. [https://www.nlm.nih.gov/bsd/pubmed.html](https://www.nlm.nih.gov/bsd/pubmed.html).
* [US National Library of Medicine. MEDLINE(r): Description of the Database.2019. [https://www.nlm.nih.gov/bsd/medline.html](https://www.nlm.nih.gov/bsd/medline.html).
* [US National Library of Medicine. Fact Sheet: MEDLINE, PubMed, and PMC (PubMed Central): How are they different? no date. [https://www.nlm.nih.gov/bsd/difference.html](https://www.nlm.nih.gov/bsd/difference.html).
* [Vickers et al.1998] Vickers A, Goyal N, Harland R, Rees R. Do certain countries produce only positive results? A systematic review of controlled trials. _Controlled Clinical Trials_ 1998; **19**: 159-166.
* [Viergever et al.2013] Viergever RF, Li K. Trends in global clinical trial registration: an analysis of numbers of registered clinical trials in different parts of the world from 2004 to 2013. _BMJ Open_ 2015; **5**: e008932.
* [von Elm et al.2004] von Elm E, Poglia G, Walder B, Tramer MR. Different patterns of duplicate publication: an analysis of articles used in systematic reviews. _JAMA_ 2004; **291**: 974-980.
* [Wallace et al.2013] Wallace S, Daly C, Campbell M, Cody J, Grant A, Vale L, Donaldson C, Khan I, Lawrence P, MacLeod A. After MEDLINE? Dividend from other potential sources of randomised controlled trials. Second International Conference Scientific Basis of Health Services & Fifth Annual Cochrane Colloquium; 1997; Amsterdam, The Netherlands.
* [Wang et al.2015] Wang Z, Brito JP, Tsapas A, Griebeler ML, Alahdab F, Murad MH. Systematic reviews with language restrictions and no author contact have lower overall credibility: a methodology study. _Clinical Epidemiology_ 2015; **7**: 243-247.
* [Weber et al.1998] Weber EJ, Callaham ML, Wears RL, Barton C, Young G. Unpublished research from a medical specialty meeting: why investigators fail to publish. _JAMA_ 1998; **280**: 257-259.
* [Werner et al.2014]* Wieseler et al. (2012) Wieseler B, Kerekes MF, Vervoelgyi V, McGauran N, Kaiser T. Impact of document type on reporting quality of clinical drug trials: a comparison of registry reports, clinical study reports, and journal publications. _BMJ_ 2012; **344**: d8141.
* Witkowski et al. (2015) Witkowski MA, Aldhouse N. Transparency and reproducibility of supplementary search methods in NICE single technology appraisal manufacturer submissions. _Value in Health_ 2015; **18**: A721-722.
* Wood and Averb (2009) Wood H, Arber M. Search strategy development [webpage]. In: Summarized Research in Information Retrieval for HTA (SuRe Info) 2009. Last updated 2019. [http://www.htai.org/vortal/?q=node/790](http://www.htai.org/vortal/?q=node/790).
* Woods et al. (1998) Woods D, Trewheellar K. Medline and Embase complement each other in literature searches. _BMJ_ 1998; **316**: 1166.
* Wu et al. (2013) Wu XY, Tang JL, Mao C, Yuan JQ, Qin Y, Chung VC. Systematic reviews and meta-analyses of traditional Chinese medicine must search Chinese databases to reduce language bias. _Evidence-Based Complementary and Alternative Medicine_ 2013: 812179.
* Yoshii et al. (2009) Yoshii A, Plaut DA, McGraw KA, Anderson MJ, Wellik KE. Analysis of the reporting of search strategies in Cochrane systematic reviews. _Journal of the Medical Library Association_ 2009; **97**: 21-29.
* Young and Hopewell (2011) Young T, Hopewell S. Methods for obtaining unpublished data. _Cochrane Database of Systematic Reviews_ 2011; **11**: MR000027.

## 5 Collecting data

Tianjing Li, Julian PT Higgins, Jonathan J Deeks

This chapter should be cited as: Li T, Higgins JPT, Deeks JJ (editors). Chapter 5: Collecting data. In: Higgins JPT, Thomas J, Chandler J, Cumpston M, Li T, Page MJ, Welch VA (editors). _Cochrane Handbook for Systematic Reviews of Interventions_. 2nd Edition. Chichester (UK): John Wiley & Sons, 2019: 109-142.

The Cochrane Collaboration. Published 2019 by John Wiley & Sons Ltd.

approaches that should be used in systematic reviews for collecting data, including extraction of data directly from journal articles and other reports of studies.

### Sources of data

Studies are reported in a range of sources which are detailed later. As discussed in Section 5.2.1, it is important to link together multiple reports of the same study. The relative strengths and weaknesses of each type of source are discussed in Section 5.2.2. For guidance on searching for and selecting reports of studies, refer to Chapter 4.

**Journal articles** are the source of the majority of data included in systematic reviews. Note that a study can be reported in multiple journal articles, each focusing on some aspect of the study (e.g. design, main results, and other results).

**Conference abstracts** are commonly available. However, the information presented in conference abstracts is highly variable in reliability, accuracy, and level of detail [11].

**Errata** and **letters** can be important sources of information about studies, including critical weaknesses and retractions, and review authors should examine these if they are identified (see MECIR Box 5.2.a).

**Trials registers** (e.g. ClinicalTrials.gov) catalogue trials that have been planned or started, and have become an important data source for identifying trials, for comparing published outcomes and results with those planned, and for obtaining efficacy and safety data that are not available elsewhere [14, 15, 16].

**Clinical study reports (CSRs)** contain unabridged and comprehensive descriptions of the clinical problem, design, conduct and results of clinical trials, following a structure and content guidance prescribed by the International Conference on Harmonisation (ICH 1995). To obtain marketing approval of drugs and biologics for a specific indication, pharmaceutical companies submit CSRs and other required materials to regulatory authorities. Because CSRs also incorporate tables and figures, with appendices containing the protocol, statistical analysis plan, sample case report forms, and patient data listings (including narratives of all serious adverse events), they can be thousands of pages in length. CSRs often contain more data about trial methods and results than any other single data source (Mayo-Wilson et al 2018). CSRs are often difficult to access, and are usually not publicly available. Review authors could request CSRs from the European Medicines Agency (Davis and Miller 2017). The US Food and Drug and Administration had historically avoided releasing CSRs but launched a pilot programme in 2018 whereby selected portions of CSRs for new drug applications were posted on the agency's website. Many CSRs are obtained through unsealed litigation documents, repositories (e.g. clinicalstudydatasetrequest.com), and other open data and data-sharing channels (e.g. The Yale University Open Data Access Project) (Doshi et al 2013, Wieland et al 2014, Mayo-Wilson et al 2018)).

**Regulatory reviews** such as those available from the US Food and Drug Administration or European Medicines Agency provide useful information about trials of drugs, biologics, and medical devices submitted by manufacturers for marketing approval (Turner 2013). These documents are summaries of CSRs and related documents, prepared by agency staff as part of the process of approving the products for marketing, after reanalysing the original trial data. Regulatory reviews often are available only for the first approved use of an intervention and not for later applications (although review authors may request those documents, which are usually brief ). Using regulatory reviews from the US Food and Drug Administration as an example, drug approval packages are available on the agency's website for drugs approved since 1997 (Turner 2013); for drugs approved before 1997, information must be requested through a freedom of information request. The drug approval packages contain various documents: approval letter(s), medical review(s), chemistry review(s), clinical pharmacology review(s), and statistical reviews(s).

**Individual participant data (IPD)** are usually sought directly from the researchers responsible for the study, or may be identified from open data repositories (e.g. www.clinicalstudydatasetrequest.com). These data typically include variables that represent the characteristics of each participant, intervention (or exposure) group, prognostic factors, and measurements of outcomes (Stewart et al 2015). Access to IPD has the advantage of allowing review authors to reanalyse the data flexibly, in accordance with the preferred analysis methods outlined in the protocol, and can reduce the variation in analysis methods across studies included in the review. IPD reviews are addressed in detail in Chapter 26.

#### 5.2.1 Studies (not reports) as the unit of interest

In a systematic review, _studies_ rather than _reports_ of studies are the principal unit of interest. Since a study may have been reported in several sources, a comprehensive search for studies for the review may identify many reports from a potentially relevant study (Mayo-Wilson et al 2017a, Mayo-Wilson et al 2018). Conversely, a report may describe more than one study.

Multiple reports of the same study should be linked together (see MECIR Box 5.2.b). Some authors prefer to link reports before they collect data, and collect data from across the reports onto a single form. Other authors prefer to collect data from each report and then link together the collected data across reports. Either strategy may be appropriate, depending on the nature of the reports at hand. It may not be clear that two reports relate to the same study until data collection has commenced. Although sometimes there is a single report for each study, it should never be assumed that this is the case.

It can be difficult to link multiple reports from the same study, and review authors may need to do some 'detective work'. Multiple sources about the same trial may not reference each other, do not share common authors [14, 15], or report discrepant information about the study design, characteristics, outcomes, and results [21, 22].

Some of the most useful criteria for linking reports are:

* trial registration numbers;
* authors' names;
* sponsor for the study and sponsor identifiers (e.g. grant or contract numbers);
* location and setting (particularly if institutions, such as hospitals, are named);
* specific details of the interventions (e.g. dose, frequency);
* numbers of participants and baseline data; and
* date and duration of the study (which also can clarify whether different sample sizes are due to different periods of recruitment), length of follow-up, or subgroups selected to address secondary goals.

Review authors should use as many trial characteristics as possible to link multiple reports. When uncertainties remain after considering these and other factors, it may be necessary to correspond with the study authors or sponsors for confirmation.

#### 5.2.2 Determining which sources might be most useful

A comprehensive search to identify all eligible studies from all possible sources is resource-intensive but necessary for a high-quality systematic review (see Chapter 4). Because some data sources are more useful than others [21], review authors should consider which data sources may be available and which may contain the most useful information for the review. These considerations should be described in the protocol. Table 5.2.a summarizes the strengths and 

### 5.2 Sources of data

\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt} p{142.3pt}} \hline \hline Source & Strengths & Limitations \\ \hline
**Public sources** & & \\
**Journal** & Found easily & Available for some, but not all studies (with a risk of reporting biases: see Chapters 7 and 13) \\
**Conference abstracts** & Identify unpublished studies & Can omit outcomes, especially harms include little information about study design \\  & & Include limited and unclear information for meta-analysis \\  & & May result in double-counting studies in meta-analysis if not correctly linked to other reports of the same study \\
**Trial registrations** & Identify otherwise unpublished trials & Limited to more recent studies that comply with registration requirements \\  & May contain information about design, risk of bias, and results not included in other public sources & Often contain limited information about trial design and quantitative results \\  & Link multiple sources about the same trial using unique registration number & May report only harms (adverse events) occurring above a threshold (e.g. 5%) \\  & & May be inaccurate or incomplete for trials whose methods have changed during the conduct of the study, or results not kept up to date \\
**Regulatory information** & Identify studies not reported in other public sources & Available only for studies submitted to regulators \\  & Describe details of methods and results not found in other sources & Available for approved indications, but not ‘off-label’ uses Not always in a standard format Not often available for old products \\
**Non-public sources** & & Do not exist or difficult to obtain for most studies \\
**Clinical study** & & \\
**reports** & Can be particularly useful for identifying detailed information about harms & Require more time to obtain and analyse than public sources \\  & Describe aggregate results, which are easy to analyse and sufficient for most reviews \\
**Individual participant** & Allow review authors to use contemporary statistical methods and to standardize analyses across studies & Require considerable expertise and time to obtain and analyse \\
**data** & Permit additional analyses that the review authors desire (e.g. subgroup analyses) & May lead to the same results that can be found in aggregate report \\  & & May not be necessary if one has a CSR \\ \hline \hline \end{tabular}
\end{table}
Table 5.2.a: Strengths and limitations of different data sources for systematic reviewslimitations of different data sources (Mayo-Wilson et al 2018). Gaining access to CSRs and IPD often takes a long time. Review authors should begin searching repositories and contact trial investigators and sponsors as early as possible to negotiate data usage agreements (Mayo-Wilson et al 2015, Mayo-Wilson et al 2018).

#### 5.2.3 Correspondence with investigators

Review authors often find that they are unable to obtain all the information they seek from available reports about the details of the study design, the full range of outcomes measured and the numerical results. In such circumstances, authors are strongly encouraged to contact the original investigators (see MECIR Box 5.2.c). Contact details of study authors, when not available from the study reports, often can be obtained from more recent publications, from university or institutional staff listings, from membership directories of professional societies, or by a general search of the web. If the contact author named in the study report cannot be contacted or does not respond, it is worthwhile attempting to contact other authors.

Review authors should consider the nature of the information they require and make their request accordingly. For descriptive information about the conduct of the trial, it may be most appropriate to ask open-ended questions (e.g. how was the allocation process conducted, or how were missing data handled?). If specific numerical data are required, it may be more helpful to request them specifically, possibly providing a short data collection form (either uncompleted or partially completed). If IPD are required, they should be specifically requested (see also Chapter 26). In some cases, study investigators may find it more convenient to provide IPD rather than conduct additional analyses to obtain the specific statistics requested.

### 5.3 What data to collect

#### 5.3.1 What are data?

For the purposes of this chapter, we define 'data' to be any information about (or derived from) a study, including details of methods, participants, setting, context, interventions, outcomes, results, publications, and investigators. Review authors should plan in advance what data will be required for their systematic review, and develop a strategy for obtaining them (see MECIR Box 5.3.a). The involvement of consumers and other stakeholders can be helpful in ensuring that the categories of data collected are sufficiently aligned with the needs of review users (Chapter 1, Section 1.3). The data to be sought should be described in the protocol, with consideration wherever possible of the issues raised in the rest of this chapter.

The data collected for a review should adequately describe the included studies, support the construction of tables and figures, facilitate the risk of bias assessment, and enable syntheses and meta-analyses. Review authors should familiarize themselves with reporting guidelines for systematic reviews (see online Chapter III and the PRISMA statement; Liberati et al 2009) to ensure that relevant elements and sections are incorporated. The following sections review the types of information that should be sought, and these are summarized in Table 5.3.a (Li et al 2015).

#### 5.3.2 Study methods and potential sources of bias

Different research methods can influence study outcomes by introducing different biases into results. Important study design characteristics should be collected to allow the selection of appropriate methods for assessment and analysis, and to enable description of the design of each included study in a table of 'Characteristics of included studies', including whether the study is randomized, whether the study has a cluster or crossover design, and the duration of the study. If the review includes non-randomized studies, appropriate features of the studies should be described (see Chapter 24).

Detailed information should be collected to facilitate assessment of the risk of bias in each included study. Risk-of-bias assessment should be conducted using the tool most appropriate for the design of each study, and the information required to complete the assessment will depend on the tool. Randomized studies should be assessed using the tool described in Chapter 8. The tool covers bias arising from the randomization process, due to deviations from intended interventions, due to missing outcome data, in measurement of the outcome, and in selection of the reported result. For each item in the tool, a description of what happened in the study is required, which may include verbatim quotes from study reports. Information for assessment of bias due to missing outcome data and selection of the reported result may be most conveniently collected alongside information on outcomes and results. Chapter 7 (Section 7.3.1) discusses some issues in the collection of information for 

#### 5.3.3 Checklist of items to consider in data collection

##### Information about data extraction from reports

Name of data extractors, date of data extraction, and identification features of each report from which data are being extracted

##### Eligibility criteria

Confirm eligibility of the study for the review

Reason for exclusion

##### Study methods

Study design:

Parallel, factorial, crossover, cluster aspects of design for randomized trials, and/or study design features for non-randomized studies

Single or multicentre study; if multicentre, number of recruiting centres

Recruitment and sampling procedures used (including at the level of individual participants and clusters/sites if relevant)

Enrolment start and end dates; length of participant follow-up

Details of random sequence generation, allocation sequence concealment, and masking for randomized trials, and methods used to prevent and control for confounding, selection biases, and information biases for non-randomized studies*

Methods used to prevent and address missing data*

Statistical analysis:

Unit of analysis (e.g. individual participant, clinic, village, body part)

Statistical methods used if computed effect estimates are extracted from reports, including any covariates included in the statistical model

Likelihood of reporting and other biases*

Source(s) of funding or other material support for the study

Authors' financial relationship and other potential conflicts of interest

##### Participants

Setting

Region(s) and country/countries from which study participants were recruited

Study eligibility criteria, including diagnostic criteria

Characteristics of participants at the beginning (or baseline) of the study (e.g. age, sex, comorbidity, socio-economic status)

##### Intervention

Description of the intervention(s) and comparison intervention(s), ideally with sufficient detail for replication:

* Components, routes of delivery, doses, timing, frequency, intervention protocols, length of intervention
* Factors relevant to implementation (e.g. staff qualifications, equipment requirements)
* Integrity of interventions (i.e. the degree to which specified procedures or components of the intervention were implemented as planned)
* Description of co-interventions
* Definition of 'control' groups (e.g. no intervention, placebo, minimally active comparator, or components of usual care)
* Components, dose, timing, frequency
* For observational studies: description of how intervention status was assessed; length of exposure, cumulative exposure

\begin{table}
\begin{tabular}{l} \hline \hline
**Information about data extraction from reports** \\ Name of data extractors, date of data extraction, and identification features of each report from which data are being extracted \\
Eligibility criteria** \\ Confirm eligibility of the study for the review \\ Reason for exclusion \\
##### Study methods \\ Study design: \\
* Parallel, factorial, crossover, cluster aspects of design for randomized trials, and/or study design features for non-randomized studies \\
* Single or multicentre study; if multicentre, number of recruiting centres \\ Recruitment and sampling procedures used (including at the level of individual participants and clusters/sites if relevant) \\ Enrolment start and end dates; length of participant follow-up \\ Details of random sequence generation, allocation sequence concealment, and masking for randomized trials, and methods used to prevent and control for confounding, selection biases, and information biases for non-randomized studies* \\ Methods used to prevent and address missing data* \\ Statistical analysis: \\ Unit of analysis (e.g. individual participant, clinic, village, body part) \\ Statistical methods used if computed effect estimates are extracted from reports, including any covariates included in the statistical model \\ Likelihood of reporting and other biases* \\ Source(s) of funding or other material support for the study \\ Authors’ financial relationship and other potential conflicts of interest \\
Participants \\ Setting \\ Region(s) and country/countries from which study participants were recruited \\ Study eligibility criteria, including diagnostic criteria \\ Characteristics of participants at the beginning (or baseline) of the study (e.g. age, sex, comorbidity, socio-economic status) \\
Intervention** \\ Description of the intervention(s) and comparison intervention(s), ideally with sufficient detail for replication: \\ \end{tabular}
\end{table}
Table 5.3.a: Checklist of items to consider in data collection

### 5.3 What data to collect

**Outcomes**

For each pre-specified outcome domain (e.g. anxiety) in the systematic review:

* Whether there is evidence that the outcome domain was assessed (especially important if the outcome was assessed but the results not presented; see Chapter 13)
* Measurement tool or instrument (including definition of clinical outcomes or endpoints); for a scale, name of the scale (e.g. the Hamilton Anxiety Rating Scale), upper and lower limits, and whether a high or low score is favourable, definitions of any thresholds if appropriate
* Specific metric (e.g. post-intervention anxiety, or change in anxiety from baseline to a post-intervention time point, or post-intervention presence of anxiety (yes/no))
* Method of aggregation (e.g. mean and standard deviation of anxiety scores in each group, or proportion of people with anxiety)
* Timing of outcome measurements (e.g. assessments at end of eight-week intervention period, events occurring during the eight-week intervention period)
* Adverse outcomes need special attention depending on whether they are collected systematically or non-systematically (e.g. by voluntary report)

**Results**

For each group, and for each outcome at each time point: number of participants randomly assigned and included in the analysis; and number of participants who withdrew, were lost to follow-up or were excluded (with reasons for each)

Summary data for each group (e.g. 2x2 table for dichotomous data; means and standard deviations for continuous data)

Between-group estimates that quantify the effect of the intervention, and their precision (e.g. risk ratio, odds ratio, mean difference)

If subgroup analysis is planned, the same information would need to be extracted for each participant subgroup

**Miscellaneous**

Key conclusions of the study authors

Reference to other relevant studies

Correspondence required

Miscellaneous comments from the study authors or by the review authors

\({}^{*}\) Full description required for assessments of risk of bias (see Chapters 8, 23 and 25).

assessments of risk of bias. For non-randomized studies, the most appropriate tool is described in Chapter 25. A separate tool also covers bias due to missing results in meta-analysis (see Chapter 13).

A particularly important piece of information is the funding source of the study and potential conflicts of interest of the study authors.

Some review authors will wish to collect additional information on study characteristics that bear on the quality of the study's conduct but that may not lead directly to risk of bias, such as whether ethical approval was obtained and whether a sample size calculation was performed a priori.

#### 5.3.3 Participants and setting

Details of participants are collected to enable an understanding of the comparability of, and differences between, the participants within and between included studies, and toallow assessment of how directly or completely the participants in the included studies reflect the original review question.

Typically, aspects that should be collected are those that could (or are believed to) affect presence or magnitude of an intervention effect and those that could help review users assess applicability to populations beyond the review. For example, if the review authors suspect important differences in intervention effect between different socio-economic groups, this information should be collected. If intervention effects are thought constant over such groups, and if such information would not be useful to help apply results, it should not be collected. Participant characteristics that are often useful for assessing applicability include age and sex. Summary information about these should always be collected unless they are not obvious from the context. These characteristics are likely to be presented in different formats (e.g. ages as means or medians, with standard deviations or ranges; sex as percentages or counts for the whole study or for each intervention group separately). Review authors should seek consistent quantities where possible, and decide whether it is more relevant to summarize characteristics for the study as a whole or by intervention group. It may not be possible to select the most consistent statistics until data collection is complete across all or most included studies. Other characteristics that are sometimes important include ethnicity, socio-demographic details (e.g. education level) and the presence of comorbid conditions. Clinical characteristics relevant to the review question (e.g. glucose level for reviews on diabetes) also are important for understanding the severity or stage of the disease.

Diagnostic criteria that were used to define the condition of interest can be a particularly important source of diversity across studies and should be collected. For example, in a review of drug therapy for congestive heart failure, it is important to know how the definition and severity of heart failure was determined in each study (e.g. systolic or diastolic dysfunction, severe systolic dysfunction with ejection fractions below 20%). Similarly, in a review of antihypertensive therapy, it is important to describe baseline levels of blood pressure of participants.

If the settings of studies may influence intervention effects or applicability, then information on these should be collected. Typical settings of healthcare intervention studies include acute care hospitals, emergency facilities, general practice, and extended care facilities such as nursing homes, offices, schools, and communities. Sometimes studies are conducted in different geographical regions with important differences that could affect delivery of an intervention and its outcomes, such as cultural characteristics, economic context, or rural versus city settings. Timing of the study may be associated with important technology differences or trends over time. If such information is important for the interpretation of the review, it should be collected.

Important characteristics of the participants in each included study should be summarized for the reader in the table of 'Characteristics of included studies'.

#### 5.3.4 Interventions

Details of all experimental and comparator interventions of relevance to the review should be collected. Again, details are required for aspects that could affect the presence or magnitude of an effect or that could help review users assess applicability to their own circumstances. Where feasible, information should be sought (and presented in the review) that is sufficient for replication of the interventions under study. This includes any co-interventions administered as part of the study, and applies similarly to comparators such as 'usual care'. Review authors may need to request missing information from study authors.

The Template for Intervention Description and Replication (TIDieR) provides a comprehensive framework for full description of interventions and has been proposed for use in systematic reviews as well as reports of primary studies (Hoffmann et al 2014). The checklist includes descriptions of:

* the rationale for the intervention and how it is expected to work;
* any documentation that instructs the recipient on the intervention;
* what the providers do to deliver the intervention (procedures and processes);
* who provides the intervention (including their skill level), how (e.g. face to face, web-based) and in what setting (e.g. home, school, or hospital);
* the timing and intensity;
* whether any variation is permitted or expected, and whether modifications were actually made; and
* any strategies used to ensure or assess fidelity or adherence to the intervention, and the extent to which the intervention was delivered as planned.

For clinical trials of pharmacological interventions, key information to collect will often include routes of delivery (e.g. oral or intravenous delivery), doses (e.g. amount or intensity of each treatment, frequency of delivery), timing (e.g. within 24 hours of diagnosis), and length of treatment. For other interventions, such as those that evaluate psychotherapy, behavioural and educational approaches, or healthcare delivery strategies, the amount of information required to characterize the intervention will typically be greater, including information about multiple elements of the intervention, who delivered it, and the format and timing of delivery. Chapter 17 provides further information on how to manage intervention complexity, and how the intervention Complexity Assessment Tool (iCAT) can facilitate data collection (Lewin et al 2017).

Important characteristics of the interventions in each included study should be summarized for the reader in the table of 'Characteristics of included studies'. Additional tables or diagrams such as logic models (Chapter 2, Section 2.5.1) can assist descriptions of multi-component interventions so that review users can better assess review applicability to their context.

##### 5.3.4.1 Integrity of interventions

The degree to which specified procedures or components of the intervention are implemented as planned can have important consequences for the findings from a study. We describe this as **intervention integrity**; related terms include adherence, compliance and fidelity (Carroll et al 2007). The verification of intervention integrity may be particularly important in reviews of non-pharmacological trials such as behavioural interventions and complex interventions, which are often implemented in conditions that present numerous obstacles to idealized delivery.

It is generally expected that reports of randomized trials provide detailed accounts of intervention implementation (Zwarenstein et al 2008, Moher et al 2010). In assessing whether interventions were implemented as planned, review authors should bear in mind that some interventions are standardized (with no deviations permitted in the intervention protocol), whereas others explicitly allow a degree of tailoring (Zwarenstein et al 2008).

In addition, the growing field of implementation science has led to an increased awareness of the impact of setting and context on delivery of interventions (Damschroder et al 2009). (See Chapter 17, Section 17.1.2.1 for further information and discussion about how an intervention may be tailored to local conditions in order to preserve its integrity.)

Information about integrity can help determine whether unpromising results are due to a poorly conceptualized intervention or to an incomplete delivery of the prescribed components. It can also reveal important information about the feasibility of implementing a given intervention in real life settings. If it is difficult to achieve full implementation in practice, the intervention will have low feasibility (Dusenbury et al 2003).

Whether a lack of intervention integrity leads to a risk of bias in the estimate of its effect depends on whether review authors and users are interested in the effect of assignment to intervention or the effect of adhering to intervention, as discussed in more detail in Chapter 8, Section 8.2.2. Assessment of deviations from intended interventions is important for assessing risk of bias in the latter, but not the former (see Chapter 8, Section 8.4), but both may be of interest to decision makers in different ways.

An example of a Cochrane Review evaluating intervention integrity is provided by a review of smoking cessation in pregnancy (Chamberlain et al 2017). The authors found that process evaluation of the intervention occurred in only some trials and that the implementation was less than ideal in others, including some of the largest trials. The review highlighted how the transfer of an intervention from one setting to another may reduce its effectiveness when elements are changed, or aspects of the materials are culturally inappropriate.

##### 5.3.4.2 Process evaluations

Process evaluations seek to evaluate the process (and mechanisms) between the intervention's intended implementation and the actual effect on the outcome (Moore et al 2015). Process evaluation studies are characterized by a flexible approach to data collection and the use of numerous methods to generate a range of different types of data, encompassing both quantitative and qualitative methods. Guidance for including process evaluations in systematic reviews is provided in Chapter 21. When it is considered important, review authors should aim to collect information on whether the trial accounted for, or measured, key process factors and whether the trials that thoroughly addressed integrity showed a greater impact. Process evaluations can be a useful source of factors that potentially influence the effectiveness of an intervention.

#### 5.3.5 Outcomes

An outcome is an event or a measurement value observed or recorded for a particular person or intervention unit in a study during or following an intervention, and that is used to assess the efficacy and safety of the studied intervention (Meinert 2012). Review authors should indicate in advance whether they plan to collect information about all outcomes measured in a study or only those outcomes of (pre-specified) interest in the review. Research has shown that trials addressing the same condition and intervention seldom agree on which outcomes are the most important, and consequently report on numerous different outcomes (Dwan et al 2014, Ismail et al 2014,Denniston et al 2015, Saldanha et al 2017a). The selection of outcomes across systematic reviews of the same condition is also inconsistent (Page et al 2014, Saldanha et al 2014, Saldanha et al 2016, Liu et al 2017). Outcomes used in trials and in systematic reviews of the same condition have limited overlap (Saldanha et al 2017a, Saldanha et al 2017b).

We recommend that only the outcomes defined in the protocol be described in detail. However, a complete list of the names of all outcomes measured may allow a more detailed assessment of the risk of bias due to missing outcome data (see Chapter 13).

Review authors should collect all five elements of an outcome (Zarin et al 2011, Saldanha et al 2014):

1. outcome domain or title (e.g. anxiety);
2. measurement tool or instrument (including definition of clinical outcomes or endpoints); for a scale, name of the scale (e.g. the Hamilton Anxiety Rating Scale), upper and lower limits, and whether a high or low score is favourable, definitions of any thresholds if appropriate;
3. specific metric used to characterize each participant's results (e.g. post-intervention anxiety, or change in anxiety from baseline to a post-intervention time point, or post-intervention presence of anxiety (yes/no));
4. method of aggregation (e.g. mean and standard deviation of anxiety scores in each group, or proportion of people with anxiety);
5. timing of outcome measurements (e.g. assessments at end of eight-week intervention period, events occurring during eight-week intervention period).

Further considerations for economics outcomes are discussed in Chapter 20, and for patient-reported outcomes in Chapter 18.

##### Adverse effects

Collection of information about the harmful effects of an intervention can pose particular difficulties, discussed in detail in Chapter 19. These outcomes may be described using multiple terms, including 'adverse event', 'adverse effect', 'adverse drug reaction','side effect' and 'complication'. Many of these terminologies are used interchangeably in the literature, although some are technically different. Harms might additionally be interpreted to include undesirable changes in other outcomes measured during a study, such as a decrease in quality of life where an improvement may have been anticipated.

In clinical trials, adverse events can be collected either systematically or non-systematically. Systematic collection refers to collecting adverse events in the same manner for each participant using defined methods such as a questionnaire or a laboratory test. For systematically collected outcomes representing harm, data can be collected by review authors in the same way as efficacy outcomes (see Section 5.3.5).

Non-systematic collection refers to collection of information on adverse events using methods such as open-ended questions (e.g. 'Have you noticed any symptoms since your last visit?'), or reported by participants spontaneously. In either case, adverse events may be selectively reported based on their severity, and whether the participant suspected that the effect may have been caused by the intervention,which could lead to bias in the available data. Unfortunately, most adverse events are collected non-systematically rather than systematically, creating a challenge for review authors. The following pieces of information are useful and worth collecting (Nicole Fusco, personal communication):

* any coding system or standard medical terminology used (e.g. COSTART, MedDRA), including version number;
* name of the adverse events (e.g. dizziness);
* reported intensity of the adverse event (e.g. mild, moderate, severe);
* whether the trial investigators categorized the adverse event as'serious';
* whether the trial investigators identified the adverse event as being related to the intervention;
* time point (most commonly measured as a count over the duration of the study);
* any reported methods for how adverse events were selected for inclusion in the publication (e.g. 'We reported all adverse events that occurred in at least 5% of participants'); and
* associated results.

Different collection methods lead to very different accounting of adverse events (Safer 2002, Bent et al 2006, Ioannidis et al 2006, Carvajal et al 2011, Allen et al 2013). Non-systematic collection methods tend to underestimate how frequently an adverse event occurs. It is particularly problematic when the adverse event of interest to the review is collected systematically in some studies but non-systematically in other studies. Different collection methods introduce an important source of heterogeneity. In addition, when non-systematic adverse events are reported based on quantitative selection criteria (e.g. only adverse events that occurred in at least 5% of participants were included in the publication), use of reported data alone may bias the results of meta-analyses. Review authors should be cautious of (or refrain from) synthesizing adverse events that are collected differently.

Regardless of the collection methods, precise definitions of adverse effect outcomes and their intensity should be recorded, since they may vary between studies. For example, in a review of aspirin and gastrointestinal haemorrhage, some trials simply reported gastrointestinal bleeds, while others reported specific categories of bleeding, such as haematesis, melaena, and proctornhagia (Derry and Loke 2000). The definition and reporting of severity of the haemorrhages (e.g. major, severe, requiring hospital admission) also varied considerably among the trials (Zanchetti and Hansson 1999). Moreover, a particular adverse effect may be described or measured in different ways among the studies. For example, the terms 'tiredness', 'fatigue' or 'lethargy' may all be used in reporting of adverse effects. Study authors also may use different thresholds for 'abnormal' results (e.g. hypokalaemia diagnosed at a serum potassium concentration of 3.0 mmol/L or 3.5 mmol/L).

No mention of adverse events in trial reports does not necessarily mean that no adverse events occurred. It is usually safest to assume that they were not reported. Quality of life measures are sometimes used as a measure of the participants' experience during the study, but these are usually general measures that do not look specifically at particular adverse effects of the intervention. While quality of life measures are important and can be used to gauge overall participant well-being, they should not be regarded as substitutes for a detailed evaluation of safety and tolerability.

#### 5.3.6 Results

Results data arise from the measurement or ascertainment of outcomes for individual participants in an intervention study. Results data may be available for each individual in a study (i.e. individual participant data; see Chapter 26), or summarized at arm level, or summarized at study level into an intervention effect by comparing two intervention arms. Results data should be collected only for the intervention groups and outcomes specified to be of interest in the protocol (see MECIR Box 5.3.b). Results for other outcomes should not be collected unless the protocol is modified to add them. Any modification should be reported in the review. However, review authors should be alert to the possibility of important, unexpected findings, particularly serious adverse effects.

Reports of studies often include several results for the same outcome. For example, different measurement scales might be used, results may be presented separately for different subgroups, and outcomes may have been measured at different follow-up time points. Variation in the results can be very large, depending on which data are selected (Gatzsche et al 2007, Mayo-Wilson et al 2017a). Review protocols should be as specific as possible about which outcome domains, measurement tools, time points, and summary statistics (e.g. final values versus change from baseline) are to be collected (Mayo-Wilson et al 2017b). A framework should be pre-specified in the protocol to facilitate making choices between multiple eligible measures or results. For example, a hierarchy of preferred measures might be created, or plans articulated to select the result with the median effect size, or to average across all eligible results for a particular outcome domain (see also Chapter 9, Section 9.3.3). Any additional decisions or changes to this framework made once the data are collected should be reported in the review as changes to the protocol.

Section 5.6 describes the numbers that will be required to perform meta-analysis, if appropriate. The unit of analysis (e.g. participant, cluster, body part, treatment period) should be recorded for each result when it is not obvious (see Chapter 6, Section 6.2). The type of outcome data determines the nature of the numbers that will be sought for each outcome. For example, for a dichotomous ('yes' or 'no') outcome, the number of participants and the number who experienced the outcome will be sought for each group. It is important to collect the sample size relevant to each result, although this is not always obvious. A flow diagram as recommended in the CONSORT Statement [13] can help to determine the flow of participants through a study. If one is not available in a published report, review authors can consider drawing one (available from www.consort-statement.org).

The numbers required for meta-analysis are not always available. Often, other statistics can be collected and converted into the required format. For example, for a continuous outcome, it is usually most convenient to seek the number of participants, the mean and the standard deviation for each intervention group. These are often not available directly, especially the standard deviation. Alternative statistics enable calculation or estimation of the missing standard deviation (such as a standard error, a confidence interval, a test statistic (e.g. from a t-test or F-test) or a P value). These should be extracted if they provide potentially useful information (see MECIR Box 5.3.c). Details of recalculation are provided in Section 5.6. Further considerations for dealing with missing data are discussed in Chapter 10 (Section 10.12).

#### 5.3.7 Other information to collect

We recommend that review authors collect the key conclusions of the included study as reported by its authors. It is not necessary to report these conclusions in the review, but they should be used to verify the results of analyses undertaken by the review authors,

[backgroundcolor=gray!20, linecolor=gray!particularly in relation to the direction of effect. Further comments by the study authors, for example any explanations they provide for unexpected findings, may be noted. References to other studies that are cited in the study report may be useful, although review authors should be aware of the possibility of citation bias (see Chapter 7, Section 7.2.3.2). Documentation of any correspondence with the study authors is important for review transparency.

### 5.4 Data collection tools

#### 5.4.1 Rationale for data collection forms

Data collection for systematic reviews should be performed using structured data collection forms (see MECIR Box 5.4.a). These can be paper forms, electronic forms (e.g. Google Form), or commercially or custom-built data systems (e.g. Covidence, EPPI-Reviewer, Systematic Review Data Repository (SRDR)) that allow online form building, data entry by several users, data sharing, and efficient data management (Li et al 2015). All different means of data collection require data collection forms.

The data collection form is a bridge between what is reported by the original investigators (e.g. in journal articles, abstracts, personal correspondence) and what is ultimately reported by the review authors. The data collection form serves several important functions (Meade and Richardson 1997). First, the form is linked directly to the review question and criteria for assessing eligibility of studies, and provides a clear summary of these that can be used to identify and structure the data to be extracted from study reports. Second, the data collection form is the historical record of the provenance of the data used in the review, as well as the multitude of decisions (and changes to decisions) that occur throughout the review process. Third, the form is the source of data for inclusion in an analysis.

Given the important functions of data collection forms, ample time and thought should be invested in their design. Because each review is different, data collection forms will vary across reviews. However, there are many similarities in the types of information that are important. Thus, forms can be adapted from one review to the next. Although we use the term 'data collection form' in the singular, in practice it may be a series of forms used for different purposes: for example, a separate form could be used to assess the eligibility of studies for inclusion in the review to assist in the quick identification of studies to be excluded from or included in the review.

#### Considerations in selecting data collection tools

The choice of data collection tool is largely dependent on review authors' preferences, the size of the review, and resources available to the author team. Potential advantages and considerations of selecting one data collection tool over another are outlined in Table 5.4.a (Li et al 2015). A significant advantage that data systems have is in data management (Chapter 1, Section 1.6) and re-use. They make review updates more efficient, and also facilitate methodological research across reviews. Numerous'meta-epidemiological' studies have been carried out using Cochrane Review data, resulting in methodological advances which would not have been possible if thousands of studies had not all been described using the same data structures in the same system.

### 5.4 Data collection tools

\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt} p{142.3pt}} \hline \hline
**Paper forms** & **Electronic forms** & **Data systems** \\ \hline Can record notes and explanations easily Require minimal software skills & editing and analysis Allow electronic data storage, sharing and collation & Allow online data storage, sharing and easy to expand or edit forms as required \\ \multirow{5}{*}{Can automate data comparison with additional programming Can copy data to analysis software without manual re-entry, reducing errors Allow easy monitoring of progress and performance of the author team Facilitate coordination among data collectors such as allocation of studies for collection and monitoring team progress Allow simultaneous data entry by multiple authors Can export data directly to analysis software In some cases, improve public accessibility through open data sharing Upfront investment of resources to set up the form and train data extractors Structured templates may not be as flexible as electronic forms Cost of commercial data systems Require familiarity with data systems \\ \cline{1-1}  & & Require familiarity with data systems \\ \cline{1-1}  & & Require familiarity with data systems \\ \cline{1-1}  & & \\ \cline{1-1

#### 5.4.3 Design of a data collection form

Regardless of whether data are collected using a paper or electronic form, or a data system, the key to successful data collection is to construct easy-to-use forms and collect sufficient and unambiguous data that faithfully represent the source in a structured and organized manner (Li et al 2015). In most cases, a document format should be developed for the form before building an electronic form or a data system. This can be distributed to others, including programmers and data analysts, and as a guide for creating an electronic form and any guidance or codebook to be used by data extractors. Review authors also should consider compatibility of any electronic form or data system with analytical software, as well as mechanisms for recording, assessing and correcting data entry errors.

Data described in multiple reports (or even within a single report) of a study may not be consistent. Review authors will need to describe how they work with multiple reports in the protocol, for example, by pre-specifying which report will be used when sources contain conflicting data that cannot be resolved by contacting the investigators. Likewise, when there is only one report identified for a study, review authors should specify the section within the report (e.g. abstract, methods, results, tables, and figures) for use in case of inconsistent information.

A good data collection form should minimize the need to go back to the source documents. When designing a data collection form, review authors should involve all members of the team, that is, content area experts, authors with experience in systematic review methods and data collection form design, statisticians, and persons who will perform data extraction. Here are suggested steps and some tips for designing a data collection form, based on the informal collation of experiences from numerous review authors (Li et al 2015).

Step 1. Develop outlines of tables and figures expected to appear in the systematic review, considering the comparisons to be made between different interventions within the review, and the various outcomes to be measured. This step will help review authors decide the right amount of data to collect (not too much or too little). Collecting too much information can lead to forms that are longer than original study reports, and can be very wasteful of time. Collection of too little information, or omission of key data, can lead to the need to return to study reports later in the review process.

Step 2. Assemble and group data elements to facilitate form development. Review authors should consult Table 5.3.a, in which the data elements are grouped to facilitate form development and data collection. Note that it may be more efficient to group data elements in the order in which they are usually found in study reports (e.g. starting with reference information, followed by eligibility criteria, intervention description, statistical methods, baseline characteristics and results).

Step 3. Identify the optimal way of framing the data items. Much has been written about how to frame data items for developing robust data collection forms in primary research studies. We summarize a few key points and highlight issues that are pertinent to systematic reviews.

* Ask closed-ended questions (i.e. questions that define a list of permissible responses) as much as possible. Closed-ended questions do not require post hoc coding and provide better control over data quality than open-ended questions. When setting up a closed-ended question, one must anticipate and structure possible responses