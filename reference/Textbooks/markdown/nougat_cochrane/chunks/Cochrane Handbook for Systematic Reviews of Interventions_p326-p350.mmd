### 11.5 Evaluating confidence in the results

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline  & & \multicolumn{2}{c}{Step 1} & \multicolumn{4}{c}{Step 2} & \multicolumn{4}{c}{Step 3} \\ \cline{3-10}  & & \multicolumn{4}{c}{Overall rating across} & \multicolumn{4}{c}{Overall rating across} & \multicolumn{4}{c}{Overall rating across} \\  & & \multicolumn{4}{c}{Domain-specific ratings} & \multicolumn{4}{c}{domains for direct} & \multicolumn{4}{c}{Domain-specific ratings} & \multicolumn{4}{c}{domains for combined comparisons} \\  & & \multicolumn{4}{c}{for direct comparisons} & \multicolumn{4}{c}{comparations} & \multicolumn{4}{c}{for combined comparisons} & \multicolumn{4}{c}{comparations} \\ \cline{3-10} Direct comparisons & \multicolumn{4}{c}{Salamti} & \multicolumn{4}{c}{Pahan} & \multicolumn{4}{c}{Salamti} & \multicolumn{4}{c}{Pahan} & \multicolumn{4}{c}{Salamti} & \multicolumn{4}{c}{Pahan} & \multicolumn{4}{c}{Salamti} & \multicolumn{4}{c}{Pahan} \\  & GRADE domains & et al & et al & et al & et al & et al & et al & et al & et al & et al & et al & et al \\ \hline
**Diettiian versus** & Study & ✓ & ✓ & x & ✓ & ✓ & x & ✓ & ✓ \\
**unrise** & limitations & & & & & & & & & \\  & Indirectness & ✓ & ✓ & & & & ✓ & x & & \\  & Inconsistency & ✓ & ✓ & & & & ✓ & x & & \\  & Inconsistency & ✓ & ✓ & & & & ✓ & x & & \\  & Imprecision & - & - & & & & ✓ & x & & \\  & Publication bias & ✓ & ✓ & & & & ✓ & x & & \\
**Diettiian versus** & Study & ✓ & ✓ & x & ✓ & ✓ & x & ✓ & ✓ \\
**doctor** & Limitations & & & & & & & & & \\  & Indirectness & ✓ & ✓ & & & & ✓ & x & & \\  & Inconsistency & ✓ & ✓ & & & ✓ & x & & \\  & Imprecision & - & - & & & & ✓ & x & & \\  & Publication bias & ✓ & ✓ & & & ✓ & x & & \\
**Nurse versus doctor** & Study & ✓ & ✓ & x & ✓ & ✓ & x & ✓ & ✓ \\  & limitations & & & & & & & & & \\  & Indirectness & ✓ & ✓ & & & & ✓ & x & & \\  & Inconsistency & ✓ & ✓ & & & ✓ & x & & \\  & Imprecision & - & - & & & ✓ & x & & \\  & Publication bias & ✓ & ✓ & & & ✓ & x & & \\ \hline \hline \end{tabular}
\end{table}
Table 11.5.a: Steps to obtain the overall confidence ratings (across all GRADE domains) for every combined comparison of the dietary advice example. A, ✓ or x indicates whether a particular step is needed in order to proceed to the next stepcomparison estimate and can be interpreted as the **contributions** of the direct comparison estimates. Then, the confidence in an indirect or combined comparison is estimated by combining the confidence assessment for the available direct comparison estimates with their contribution to the combined (or network) comparison. This approach is similar to the process of evaluating the likely impact of a high risk-of-bias study by looking at its weight in a pair-wise meta-analysis to decide whether to downgrade or not in a standard GRADE assessment.

As an example, in the dietary advice network (Figure 11.2.a) suppose that most of the evidence involved in the indirect comparison (i.e. the trials including dietitians) is at low risk of bias, and that there are studies of 'doctor versus nurse' that are mostly at high risk of bias. If the direct evidence on 'doctor versus nurse' has a very large contribution to the network meta-analysis estimate of the same comparison, then we would judge this result to be at high risk of bias. If the direct evidence has a very low contribution, we might judge the result to be at moderate, or possibly low, risk of bias. This approach might be preferable when there are indirect or mixed comparisons informed by many loops within a network, and for a specific comparison these loops lead to different risk-of-bias assessments. The contributions of the direct comparisons and the risk-of-bias assessments may be presented jointly in a bar graph, with bars proportional to the contributions of direct comparisons and different colours representing the different judgements. The bar graph for the heavy menstrual bleeding example is available in Figure 11.5.a,

Figure 11.5.a: Bar graph illustrating the percentage of information for every comparison that comes from low (dark grey), moderate (light grey) or high (blue) risk-of-bias (RoB) studies with respect to both randomization and compliance to treatment for the heavy menstrual bleeding network (Middleton et al 2010). The risk of bias of the direct comparisons was defined based on Appendix 3 of the original paper. The intervention labels are: A, first generation hysteroscopic techniques; B, hysterectomy; C, second generation non-hysteroscopic techniques; D, Mirena. Reproduced with permission of BMJ Publishing Group

which suggests that there are two comparisons ('First generation hyteroscopic techniques versus Mirena' and 'Second generation non-hyteroscopic techniques versus Mirena') for which a substantial amount of information comes from studies at high risk of bias.

Regardless of whether a review contains a network meta-analysis or a simple indirect comparison, Puhan and colleagues propose to focus on so-called'most influential' loops only. These are the connections between a pair of interventions of interest that involve exactly one common comparator. This implies that the assessment for the indirect comparison is dependent only on confidence in the two other direct comparisons in this loop. To illustrate, consider the dietary advice network described in Section 11.2 (Figure 11.2.a), where we are interested in confidence in the evidence for the indirect comparison 'doctor versus nurse'. According to Puhan and colleagues, the lower confidence rating between the two direct comparisons 'dietitian versus doctor' and 'dietitian versus nurse' would be chosen to inform the confidence rating for the indirect comparison. If there are also studies directly comparing doctor versus nurse, the confidence in the combined comparison would be the higher rated source between the direct evidence and the indirect evidence. The main rationale for this is that, in general, the higher rated comparison is expected to be the more precise (and thus the dominating) body of evidence. Also, in the absence of important incoherence, the lower rated evidence is only supportive of the higher rated evidence; thus it is not very likely to reduce the confidence in the estimated intervention effects. One disadvantage of this approach is that investigators need to identify the most influential loop; this loop might be relatively uninfluential when there are many loops in a network, which is often the case when there are many interventions. In large networks, many loops with comparable influence may exist and it is not clear how many of those equally influential loops should be considered under this approach.

At the time of writing, no formal comparison has been performed to evaluate the degree of agreement between these two methods. Thus, at this point we do not prescribe using one approach or the other. However, when indirect comparisons are built on existing pair-wise meta-analyses, which have already been rated with respect to their confidence, it may be reasonable to follow the approach of Puhan and colleagues. On the other hand, when the body of evidence is built from scratch, or when a large number of interventions are involved, it may be preferable to consider the approach of Salanti and colleagues whose application is facilitated via the online tool CINeMA.

Since network meta-analysis produces estimates for several intervention effects, the confidence in the evidence should be assessed for each intervention effect that is reported in the results. In addition, network meta-analysis may also provide information on the relative ranking of interventions, and review authors should consider also assessing confidence in results for relative ranking when these are reported. Salanti and colleagues address confidence in the ranking based on the contributions of the direct comparisons to the _entire_ network as well as on the use of measures and graphs that aim to assess the different GRADE domains in the network as a whole (e.g. measures of global incoherence) (see Section 11.4.4).

The two approaches modify the standard GRADE domains to fit network meta-analysis to varying degrees. These modifications are briefly described in Box 11.5.a; more details and examples are available in the original articles (Puhan et al 2014, Salanti et al 2014).

**Box 11.5.a**: **Modifications to the five domains of the standard GRADE system to fit network meta-analysis.**

_Study limitations (i.e. classical risk-of-bias items)_ Salanti and colleagues suggest a bar graph with bars proportional to the contributions of direct comparisons and different colours representing the different confidence ratings (e.g. green, yellow, red for low, moderate or high risk of bias) with respect to study limitations (Figure 11.5.a). The decision about downgrading or not is then formed by interpreting this graph. Such a graph can be used to rate the confidence of evidence for each combined _comparison_ and for the relative ranking.

_Indirectness_ The assessment of indirectness in the context of network meta-analysis should consider two components: the similarity of the studies in the analysis to the target question (PICO); and the similarity of the studies in the analysis to each other. The first addresses the extent to which the evidence at hand relates to the population, intervention(s), comparators and outcomes of interest, and the second relates to the evaluation of the transitivity assumption. A common view of the two approaches is that they do not support the idea of downgrading indirect evidence by default. They suggest that indirectness should be considered in conjunction with the risk of intransitivity.

_Inconsistency_ Salanti and colleagues propose to create a common domain to consider jointly both types of inconsistency that may occur: heterogeneity within direct comparisons and incoherence. More specifically, they evaluate separately the presence of the two types of variation and then consider them jointly to infer whether downgrading for inconsistency is appropriate or not. It is usual in network meta-analysis to assume a common heterogeneity variance. They propose the use of prediction intervals to facilitate the assessment of heterogeneity for each combined comparison. Prediction intervals are the intervals expected to include the true intervention effects in future studies (Higgins et al 2009, Riley et al 2011) and they incorporate the extent of between-study variation; in the presence of important heterogeneity they are wide enough to include intervention effects with different implications for practice. The potential for incoherence for a particular comparison can be assessed using existing approaches for evaluating local and global incoherence (see Section 11.5). We may downgrade for one or two levels due to the presence of heterogeneity or incoherence, or both. The judgement for the relative ranking is based on the magnitude of the common heterogeneity as well as the use of global incoherence tests (see Section 11.4).

_Imprecision_ Both approaches suggest that imprecision of the combined comparisons can be judged based on their 95% confidence intervals. Imprecision for relative treatment ranking is the variability in the relative order of the interventions. This is reflected by the overlap in the distributions of the ranking probabilities; i.e. when all or some of the interventions have similar probabilities of being at a particular rank.

_Publication bias_ The potential for publication bias in a network meta-analysis can be difficult to judge. If a natural common comparator exists, a 'comparison-adjusted funnel plot' can be employed to identify possible small-study effects in a network meta-analysis (Chainani and Salanti 2012, Chainani et al 2013). This is a modified funnel plot that allows putting together all the studies of the network irrespective of the interventions they compare. However, the primary considerations for both the combined comparisons and relative ranking should be non-statistical. Review authors should consider whether there might be unpublished studies for every possible pair-wise comparison in the network.

### Presenting network meta-analyses

The PRISMA Extension Statement for Reporting of Systematic Reviews Incorporating Network Meta-analyses of Health Care Interventions should be considered when reporting the results from network meta-analysis (Hutton et al 2015). Key graphical and numerical summaries include the network plot (e.g. Figure 11.4.a), a league table of the relative effects between all treatments with associated uncertainty (e.g. Table 11.4.a) and measures of heterogeneity and incoherence.

#### Presenting the evidence base of a network meta-analysis

Network diagrams provide a convenient way to describe the structure of the network (see Section 11.1.1). They may be modified to incorporate information on study-level or comparison-level characteristics. For instance, the thickness of the lines might reflect the number of studies or patients included in each direct comparison (e.g. Figure 11.4.a), or the comparison-specific average of a potential effect modifier. Using the latter device, network diagrams can be considered as a first step for the evaluation of transitivity in a network. In the example of Figure 11.6.a the age of the participants has been considered as a potential effect modifier. The thickness of the line implies that the average age within comparisons _A versus D_ and _C versus D_ seems quite different to the other three direct comparisons.

The inclusion of studies with design limitations in a network (e.g. lack of blinding, inadequate allocation sequence concealment) often threatens the validity of findings. The use of coloured lines in a network of interventions can reveal the presence of such studies in specific direct comparisons. Further discussion on issues related to confidence in the evidence is available in Section 11.5.

#### Tabular presentation of the network structure

For networks including many competing interventions and multiple different study designs, network diagrams might not be the most appropriate tool for presenting the data. An alternative way to present the structure of the network is to use a table, in which the columns represent the competing interventions and the rows represent the different study designs in terms of interventions being compared (Table 11.6.a)

Figure 11.6.a: Example of network diagram with lines weighted according to the average age within each pair-wise comparison.

Thicker lines correspond to greater average age within the respective comparison
\begin{table}
\begin{tabular}{p{28.5pt} p{28.5pt} p{28.

(Lu and Ades 2006). Additional information, such as the number of participants in each arm, may be presented in the non-empty cells.

#### Presenting the flow of evidence in a network

Another way to map the evidence in a network of interventions is to consider how much each of the included direct comparisons contributes to the final combined effect estimates. The percentage information that direct evidence contributes to each relative effect estimated in a network meta-analysis can be presented in the contribution matrix (see Section 11.4), and could help investigators understand the flow of information in the network (Chaimani et al 2013, Chaimani and Salanti 2015).

Figure 11.6.b presents the contribution matrix for the example of the network of interventions for heavy menstrual bleeding (obtained from the _netweight_ macro in Stata). The indirect treatment effect for second generation non-hysteroscopic techniques versus hysterectomy (_B versus C_) can be estimated using information from the four direct relative treatment effects; these contribute information in different proportions depending on the precision of the direct treatment effects and the structure of the network. Evidence from the direct comparison of first generation hysteroscopic

Figure 11.6.b: Contribution matrix for the network on interventions for heavy menstrual bleeding presented in Figure 11.4.a. Four direct comparisons in the network are presented in the columns, and their contributions to the combined treatment effect are presented in the rows. The entries of the matrix are the percentage weights attributed to each direct comparison. The intervention labels are: A, first generation hysteroscopic techniques; B, hysterectomy; C, second generation non-hysteroscopic techniques; D, Mirena

techniques versus hysterectomy (_A versus B_) has the largest contribution to the indirect comparisons hysterectomy versus second generation non-hystereoscopic techniques (_B versus C_) (49.6%) and hysterectomy versus Mirena (_B versus D_) (38.5%), for both of which no direct evidence exists.

#### Presentation of results

Unlike pair-wise meta-analysis, the results from network meta-analysis cannot be easily summarized in a single figure such as a standard forest plot. Especially for networks with many competing interventions that involve many comparisons, presentation of findings in a concise and comprehensible way is challenging.

Summary statistics of the intervention effects for all pairs of interventions are the most important output from network meta-analysis. Results from a subset of comparisons are sometimes presented due to space limitations and the choice of the findings to be reported is based on the research question and the target audience (Tan et al 2013). In such cases, the use of additional figures and tables to present all results in detail is necessary. Additionally, review authors might wish to report the relative ranking of interventions (see Section 11.4.3.3) as a supplementary output, which provides a concise summary of the findings and might facilitate decision making. For this purpose, joint presentation of both relative effects and relative ranking is recommended (see Figure 11.6.c or Table 11.4.a of Section 11.4.3.1).

Figure 11.6.c: Forest plot for effectiveness in heavy menstrual bleeding between four interventions. FGHT, first generation hysteroscopic techniques; SGNHT, second generation non-hysteroscopic techniques

In the presence of many competing interventions, the results across different outcomes (e.g. efficacy and acceptability) might conflict with respect to which interventions work best. To avoid drawing misleading conclusions, review authors may consider the simultaneous presentation of results for outcomes in these two categories.

Interpretation of the findings from network meta-analysis should always be considered with the evidence characteristics: risk of bias in included studies, heterogeneity, incoherence and selection bias. Reporting results with respect to the evaluation of incoherence and heterogeneity (such as l\({}^{2}\) statistic for incoherence) is important for drawing meaningful conclusions.

##### 11.6.4.1 Presentation of intervention effects and ranking

A table presenting direct, indirect and network summary relative effects along with their confidence ratings is a helpful format (Puhan et al 2014). In addition, various graphical tools have been suggested for the presentation of results from network meta-analyses (Salanti et al 2011, Chaimani et al 2013, Tan et al 2014). Summary relative effects for pair-wise comparisons with their confidence intervals can be presented in a forest plot. For example, Figure 11.6.c shows the summary relative effects for each intervention versus a common reference intervention for the 'heavy menstrual bleeding' network.

Ranking probabilities for all possible ranks may be presented by drawing probability lines, which are known as **rankograms**, and show the distribution of ranking probabilities for each intervention (Salanti et al 2011). The rankograms for the heavy menstrual bleeding network example are shown in Figure 11.6.d. The graph suggests that 'Hysterectomy' has the highest probability of being the best intervention, 'First generation hysteroscopic techniques' have the highest probability of being worst followed by 'Mirena' and 'Second generation non-hysteroscopic techniques' have equal chances of being second or third.

The relative ranking for two (competing) outcomes can be presented jointly in a two-dimensional scatterplot (Chaimani et al 2013). An extended discussion on different ways to present jointly relative effects and relative ranking from network meta-analysis is available in Tan and colleagues (Tan et al 2013).

##### 11.6.4.2 Presentation of heterogeneity and incoherence

The level of heterogeneity in a network of interventions can be expressed via the magnitude of the between-study variance Tau\({}^{2}\), typically assumed to be common in all comparisons in the network. A judgement on whether the estimated Tau\({}^{2}\) suggests the presence of important heterogeneity depends on the clinical outcome and the type of interventions being compared. More extended discussion on the expected values of Tau\({}^{2}\) specific to a certain clinical setting is available (Turner et al 2012, Nikolakopoulou et al 2014).

Forest plots that present all the estimated incoherence factors in the network and their uncertainty may be employed for the presentation of local incoherence (Salanti et al 2009, Chaimani et al 2013). The results from evaluating global incoherence can be summarized in the P value of the Chi\({}^{2}\) statistic incoherence test and the l\({}^{2}\) statistic for incoherence (see Chapter 10, Section 10.10.2).

Figure 11.6.4: Ranking probabilities (rankograms) for the effectiveness of interventions in heavy menstrual bleeding. The horizontal axis shows the possible ranks and the vertical axis the ranking probabilities. Each line connects the estimated probabilities of being at a particular rank for every intervention

#### 11.6.4.3 'Summary of findings' tables

The purpose of 'Summary of findings' tables in Cochrane Reviews is to provide concisely the key information in terms of available data, confidence in the evidence and intervention effects (see Chapter 14). Providing such a table is more challenging in reviews that compare multiple interventions simultaneously, which very often involve a large number of comparisons between pairs of interventions. A general principle is that the comparison of multiple interventions is the main feature of a network meta-analysis, so is likely to drive the structure of the 'Summary of findings' table. This is in contrast to the 'Summary of findings' table for a pair-wise comparison, whose main strength is to facilitate comparison of effects on different outcomes. Nevertheless, it remains important to be able to compare network meta-analysis results across different outcomes. This provides presentational challenges that are almost impossible to resolve in two dimensions. One potential solution is an interactive electronic display such that the user can choose whether to emphasize the comparisons across interventions or the comparisons across outcomes.

For small networks of interventions (perhaps including up to five competing interventions) a separate 'Summary of findings' table might be produced for each main outcome. However, in the presence of many (more than five) competing interventions, researchers would typically need to select and report a reduced number of pair-wise comparisons. Review authors should provide a clear rationale for the choice of the comparisons they report in the 'Summary of findings' tables. For example, they may consider including only pair-wise comparisons that correspond to the decision set of interventions; that is, the group of interventions of direct interest for drawing conclusions (see Section 11.3.2.1). The distinction between the decision set and the wider synthesis comparator set (all interventions included in the analysis) should be made in the protocol of the review. If the decision set is still too large, researchers may be able to select the comparisons for the 'Summary of findings' table based on the most important information for clinical practice. For example, reporting the comparisons between the three or four most effective interventions with the most commonly used intervention as a comparator.

### 11.7 Concluding remarks

Network meta-analysis is a method that can inform comparative effectiveness of multiple interventions, but care needs to be taken using this method because it is more statistically complex than a standard meta-analysis. In addition, as network meta-analyses generally ask broader research questions, they usually involve more studies at each step of systematic review, from screening to analysis, than standard meta-analysis. It is therefore important to anticipate the expertise, time and resource required before embarking on one.

A valid indirect comparison and network meta-analysis requires a coherent evidence base. When formulating the research question and deciding the eligibility criteria, populations and interventions in relation to the assumption of transitivity need to be considered. Network meta-analysis is only valid when studies comparing different sets of interventions are similar enough to be combined. When conducted properly, itprovides more precise estimates of relative effect than a single direct or indirect estimate. Network meta-analysis can yield estimates between any pairs of interventions, including those that have never been compared directly against each other. Network meta-analysis also allows the estimation of the ranking and hierarchy of interventions. Much care should be taken when interpreting the results and drawing conclusions from network meta-analysis, especially in the presence of incoherence or other potential biases.

### Chapter information

**Authors:** Anna Chaimani, Deborah M Caldwell, Tianjing Li, Julian PT Higgins, Georgia Salanti

**Acknowledgements:** Lorne Becker contributed important insights in the discussion of separating Overview from Intervention Reviews with network meta-analysis. Gordon Guyatt provided helpful comments on earlier version of the chapter and Jeroen Jansen provided helpful contributions on Section 11.2.

**Funding:** This work was supported by the Methods Innovation Fund Program of the Cochrane Collaboration (MIF1) under the project 'Methods for comparing multiple interventions in Intervention reviews and Overviews of reviews'.

### References

* [1] Ades AE, Caldwell DM, Reken S, Welton NJ, Sutton AJ, Dias S. Evidence synthesis for decision making 7: a reviewer's checklist. _Medical Decision Making_ 2013; **33**: 679-691.
* [2] Baker WL, Baker EL, Coleman CI. Pharmacologic treatments for chronic obstructive pulmonary disease: a mixed-treatment comparison meta-analysis. _Pharmacotherapy_ 2009; **29**: 891-905.
* [3] Bucher HC, Guyatt GH, Griffith LE, Walter SD. The results of direct and indirect treatment comparisons in meta-analysis of randomized controlled trials. _Journal of Clinical Epidemiology_ 1997; **50**: 683-691.
* [4] Caldwell DM, Ades AE, Higgins JPT. Simultaneous comparison of multiple treatments: combining direct and indirect evidence. _BMJ_ 2005; **331**: 897-900.
* [5] Caldwell DM, Welton NJ, Ades AE. Mixed treatment comparison analysis provides internally coherent treatment effect estimates based on overviews of reviews and can reveal inconsistency. _Journal of Clinical Epidemiology_ 2010; **63**: 875-882.
* [6] Caldwell DM, Dias S, Welton NJ. Extending treatment networks in health technology assessment: how far should we go? _Value in Health_ 2015; **18**: 673-681.
* [7] Chaimani A, Salanti G. Using network meta-analysis to evaluate the existence of small-study effects in a network of interventions. _Research Synthesis Methods_ 2012; **3**: 161-176.
* [8] Chaimani A, Higgins JPT, Mavridis D, Spyridonos P, Salanti G. Graphical tools for network meta-analysis in STATA. _PloS One_ 2013; **8**: e76654.
* [9]* [11] Chaimani A, Salanti G. Visualizing assumptions and results in network meta-analysis: the network graphs package. _Stata Journal_ 2015; **15**: 905-950.
* [12] Chaimani A, Caldwell DM, Li T, Higgins JPT, Salanti G. Additional considerations are required when preparing a protocol for a systematic review with multiple interventions. _Journal of Clinical Epidemiology_ 2017; **83**: 65-74.
* [13] Cipriani A, Higgins JPT, Geddes JR, Salanti G. Conceptual and technical challenges in network meta-analysis. _Annals of Internal Medicine_ 2013; **159**: 130-137.
* [14] Cooper NJ, Peters J, Lai MC, Juni P, Wandel S, Palmer S, Paulden M, Conti S, Welton NJ, Abrams KR, Bujkiewicz S, Spiegelhalter D, Sutton AJ. How valuable are multiple treatment comparison methods in evidence-based health-care evaluation? _Value in Health_ 2011; **14**: 371-380.
* [15] Dias S, Welton NJ, Caldwell DM, Ades AE. Checking consistency in mixed treatment comparison meta-analysis. _Statistics in Medicine_ 2010; **29**: 932-944.
* [16] Dias S, Ades AE, Welton NJ, Jansen JP, Sutton AJ. _Network meta-analysis for decision-making_. Chichester (UK): Wiley; 2018.
* [17] Donegan S, Williamson P, Gamble C, Tudur-Smith C. Indirect comparisons: a review of reporting and methodological quality. _PloS One_ 2010; **5**: e11054.
* [18] Donegan S, Williamson P, D'Alessandro U, Tudur Smith C. Assessing key assumptions of network meta-analysis: a review of methods. _Research Synthesis Methods_ 2013; **4**: 291-323.
* [19] Dumville JC, Soares MO, O'Meara S, Cullum N. Systematic review and mixed treatment comparison: dressings to heal diabetic foot ulcers. _Diabetologia_ 2012; **55**: 1902-1910.
* [20] Efthimiou O, Debray TPA, Valkenhoef G, Trele S, Panayidou K, Moons KGM, Reitsma JB, Shang A, Salanti G, GetReal Methods Review Group. GetReal in network meta-analysis: a review of the methodology. _Research Synthesis Methods_ 2016; **7**: 236-263.
* [21] Giovane CD, Vacchi L, Mavridis D, Filippini G, Salanti G. Network meta-analysis models to account for variability in treatment definitions: application to dose effects. _Statistics in Medicine_ 2013; **32**: 25-39.
* [22] Glenny AM, Altman DG, Song F, Sakarovitch C, Deeks JJ, D'Amico R, Bradburn M, Eastwood AJ. Indirect comparisons of competing interventions. _Health Technology Assessment_ 2005; **9**: 1-iv.
* [23] Higgins JPT, Whitehead A. Borrowing strength from external trials in a meta-analysis. _Statistics in Medicine_ 1996; **15**: 2733-2749.
* [24] Higgins JPT, Thompson SG, Spiegelhalter DJ. Are-evaluation of random-effects meta-analysis. _Journal of the Royal Statistical Society Series A (Statistics in Society)_ 2009; **172**: 137-159.
* [25] Higgins JPT, Jackson D, Barrett JK, Lu G, Ades AE, White IR. Consistency and inconsistency in network meta-analysis: concepts and models for multi-arm studies. _Research Synthesis Methods_ 2012; **3**: 98-110.
* [26] Hutton B, Salanti G, Caldwell DM, Chaimani A, Schmid CH, Cameron C, Ioannidis JPA, Straus S, Thorlund K, Jansen JP, Mulrow C, Catala-Lopez F, Gotzsche PC, Dickersin K, Boutron I, Altman DG, Moher D. The PRISMA extension statement for reporting of systematic reviews incorporating network meta-analyses of health care interventions: checklist and explanations. _Annals of Internal Medicine_ 2015; **162**: 777-784.
* [27] Jackson D, Riley R, White IR. Multivariate meta-analysis: potential and promise. _Statistics in Medicine_ 2011; **30**: 2481-2498.
* [28] Jackson D, Barrett JK, Rice S, White IR, Higgins JPT. A design-by-treatment interaction model for network meta-analysis with random inconsistency effects. _Statistics in Medicine_ 2014; **33**: 3639-3654.
* [29]James A, Yavchitz A, Ravaud P, Boutron I. Node-making process in network meta-analysis of nonpharmacological treatment are poorly reported. _Journal of Clinical Epidemiology_ 2018; **97**: 95-102.
* Jansen and Naci (2013) Jansen JP, Naci H. Is network meta-analysis as valid as standard pairwise meta-analysis? It all depends on the distribution of effect modifiers. _BMC Medicine_ 2013; **11**: 159.
* Jansen et al. (2014) Jansen JP, Trikalinos T, Cappelleri JC, Daw J, Andes S, Eldessouki R, Salanti G. Indirect treatment comparison/network meta-analysis study questionnaire to assess relevance and credibility to inform health care decision making: an ISPOR-AMCP-NPC good practice task force report. _Value in Health_ 2014; **17**: 157-173.
* Konig et al. (2013) Konig J, Krahn U, Binder H. Visualizing the flow of evidence in network meta-analysis and characterizing mixed treatment comparisons. _Statistics in Medicine_ 2013; **32**: 5414-5429.
* Krahn et al. (2013) Krahn U, Binder H, Konig J. A graphical tool for locating inconsistency in network meta-analyses. _BMC Medical Research Methodology_ 2013; **13**: 35.
* Li and Dickersin (2013) Li T, Dickersin K. Citation of previous meta-analyses on the same topic: a clue to perpetuation of incorrect methods? _Ophthalmology_ 2013; **120**: 1113-1119.
* Lu and Ades (2004) Lu G, Ades AE. Combination of direct and indirect evidence in mixed treatment comparisons. _Statistics in Medicine_ 2004; **23**: 3105-3124.
* Lu and Ades (2006) Lu G, Ades AE. Assessing evidence inconsistency in mixed treatment comparisons. _Journal of the American Statistical Association_ 2006; **101**: 447-459.
* Lu and Ades (2009) Lu G, Ades A. Modeling between-trial variance structure in mixed treatment comparisons. _Biostatistics_ 2009; **10**: 792-805.
* Lu et al. (2011) Lu G, Welton NJ, Higgins JPT, White IR, Ades AE. Linear inference for mixed treatment comparison meta-analysis: a two-stage approach. _Research Synthesis Methods_ 2011; **2**: 43-60.
* Madan et al. (2011) Madan J, Stevenson MD, Cooper KL, Ades AE, Whyte S, Akehurst R. Consistency between direct and indirect trial evidence: is direct evidence always more reliable? _Value in Health_ 2011; **14**: 953-960.
* Madan et al. (2014) Madan J, Chen Y-F, Aveyard P, Wang D, Yahaya I, Munafo M, Bauld L, Welton N. Synthesis of evidence on heterogeneous interventions with multiple outcomes recorded over multiple follow-up times reported inconsistently: a smoking cessation case-study. _Journal of the Royal Statistical Society: Series A (Statistics in Society)_ 2014; **177**: 295-314.
* Mavridis and Salanti (2013) Mavridis D, Salanti G. A practical introduction to multivariate meta-analysis. _Statistical Methods in Medical Research_ 2013; **22**: 133-158.
* Mawdsley et al. (2016) Mawdsley D, Bennetts M, Dias S, Boucher M, Welton N. Model-based network meta-analysis: a framework for evidence synthesis of clinical trial data. _CPT: Pharmacometrics & Systems Pharmacology_ 2016; **5**: 393-401.
* Middleton et al. (2010) Middleton LJ, Champaneria R, Daniels JP, Bhattacharya S, Cooper KG, Hilken NH, O'Donovan P, Gannon M, Gray R, Khan KS, International Heavy Menstrual Bleeding Individual Patient Data Meta-analysis Collaborative G, Abbott J, Barrington J, Bhattacharya S, Bongers MY, Brun JL, Busfield R, Sowter M, Clark TJ, Cooper J, Cooper KG, Corson SL, Dickersin K, Dwyer N, Gannon M, Hawe J, Hurskainen R, Meyer WR, O'Connor H, Pinion S, Sambrook AM, Tam WH, Zon-Rabelink IAA, Zupi E. Hysterectomy, endometrial destruction, and levonorgestrel releasing intrauterine system (Mirena) for heavy menstrual bleeding: systematic review and meta-analysis of data from individual patients. _BMJ_ 2010; **341**: c3929.
* Nikolakopoulou et al. (2014) Nikolakopoulou A, Chaimani A, Veroniki AA, Vasiliadis HS, Schmid CH, Salanti G. Characteristics of networks of interventions: a description of a database of 186 published networks. _PloS One_ 2014; **9**: e86754.
* Nasi and Sow (2013)* Owen et al. (2015) Owen RK, Tincello DG, Keith RA. Network meta-analysis: development of a three-level hierarchical modeling approach incorporating dose-related constraints. _Value in Health_ 2015; **18**: 116-126.
* Petropoulou et al. (2015) Petropoulou M, Nikolakopoulou A, Veroniki AA, Rios P, Vafaei A, Zarin W, Giannatsi M, Sullivan S, Tricco AC, Chaimani A, Egger M, Salanti G. Bibliographic study showed improving statistical methodology of network meta-analyses published between 1999 and 2015. _Journal of Clinical Epidemiology_ 2016; **82**: 20-28.
* Puhan et al. (2014) Puhan MA, Schunemann HJ, Murad MH, Li T, Brignardello-Petersen R, Singh JA, Kessels AG, Guyatt GH; GRADE Working Group. A GRADE Working Group approach for rating the quality of treatment effect estimates from network meta-analysis. _BMJ_ 2014; **349**: g5630.
* Riley et al. (2011) Riley RD, Higgins JPT, Deeks JJ. Interpretation of random effects meta-analyses. _BMJ_ 2011; **342**: d549.
* Rucker (2012) Rucker G. Network meta-analysis, electrical networks and graph theory. _Research Synthesis Methods_ 2012; **3**: 312-324.
* Rucker and Schwarzer (2013) Rucker G, Schwarzer G. netmeta: an R package for network meta-analysis 2013. http://www. r-project.org.
* Rucker and Schwarzer (2015) Rucker G, Schwarzer G. Ranking treatments in frequentist network meta-analysis works without resampling methods. _BMC Medical Research Methodology_ 2015; **15**: 58.
* Salanti and Higgins (2008) Salanti G, Higgins JPT, Ades AE, Ioannidis JPA. Evaluation of networks of randomized trials. _Statistical Methods in Medical Research_ 2008; **17**: 279-301.
* Salanti and Marinho (2009) Salanti G, Marinho V, Higgins JPT. A case study of multiple-treatments meta-analysis demonstrates that covariates should be considered. _Journal of Clinical Epidemiology_ 2009; **62**: 857-864.
* Salanti and Ades (2011) Salanti G, Ades AE, Ioannidis JPA. Graphical methods and numerical summaries for presenting results from multiple-treatment meta-analysis: an overview and tutorial. _Journal of Clinical Epidemiology_ 2011; **64**: 163-171.
* Salanti (2012) Salanti G. Indirect and mixed-treatment comparison, network, or multiple-treatments meta-analysis: many names, many benefits, many concerns for the next generation evidence synthesis tool. _Research Synthesis Methods_ 2012; **3**: 80-97.
* Salanti et al. (2014) Salanti G, Del Giovane C, Chaimani A, Caldwell DM, Higgins JPT. Evaluating the quality of evidence from a network meta-analysis. _PloS One_ 2014; **9**: e99682.
* Schmitz et al. (2013) Schmitz S, Adams R, Walsh C. Incorporating data from various trial designs into a mixed treatment comparison model. _Statistics in Medicine_ 2013; **32**: 2935-2949.
* Schwarzer et al. (2015) Schwarzer G, Carpenter JR, Rucker G. _Meta-analysis with R_. Cham: Springer; 2015.
* Soares et al. (2014) Soares MO, Dumville JC, Ades AE, Welton NJ. Treatment comparisons for decision making: facing the problems of sparse and few data. _Journal of the Royal Statistical Society Series A (Statistics in Society)_ 2014; **177**: 259-279.
* Sobieraj et al. (2013) Sobieraj DM, Cappelleri JC, Baker WL, Phung OJ, White CM, Coleman Cl. Methods used to conduct and report Bayesian mixed treatment comparisons published in the medical literature: a systematic review. _BMJ Open_ 2013; **3**: pil.
* Song et al. (2003) Song F, Altman DG, Glenny AM, Deeks JJ. Validity of indirect comparison for estimating efficacy of competing interventions: empirical evidence from published meta-analyses. _BMJ_ 2003; **326**: 472.
* Song et al. (2011) Song F, Xiong T, Parekh-Bhurke S, Loke YK, Sutton AJ, Eastwood AJ, Holland R, Chen YF, Glenny AM, Deeks JJ, Altman DG. Inconsistency between direct and indirect comparisons of competing interventions: meta-epidemiological study. _BMJ_ 2011; **343**: d4909.
* Song et al. (2013)Song F, Clark A, Bachmann MO, Maas J. Simulation evaluation of statistical properties of methods for indirect and mixed treatment comparisons. _BMC Medical Research Methodology_ 2012; **12**: 138.
* Tan et al. (2013) Tan SH, Bujkiewicz S, Sutton A, Dequen P, Cooper N. Presentational approaches used in the UK for reporting evidence synthesis using indirect and mixed treatment comparisons. _Journal of Health Services Research and Policy_ 2013; **18**: 224-232.
* Tan et al. (2014) Tan SH, Cooper NJ, Bujkiewicz S, Welton NJ, Caldwell DM, Sutton AJ. Novel presentational approaches were developed for reporting network meta-analysis. _Journal of Clinical Epidemiology_ 2014; **67**: 672-680.
* Thijs et al. (2008) Thijs V, Lemmens R, Fieuws S. Network meta-analysis: simultaneous meta-analysis of common antiplatelet regimens after transient ischaemic attack or stroke. _European Heart Journal_ 2008; **29**: 1086-1092.
* Turner et al. (2012) Turner RM, Davey J, Clarke MJ, Thompson SG, Higgins JPT. Predicting the extent of heterogeneity in meta-analysis, using empirical data from the Cochrane Database of Systematic Reviews. _International Journal of Epidemiology_ 2012; **41**: 818-827.
* Veroniki et al. (2013) Veroniki AA, Vasiliadis HS, Higgins JPT, Salanti G. Evaluation of inconsistency in networks of interventions. _International Journal of Epidemiology_ 2013; **42**: 332-345.
* Veroniki et al. (2014) Veroniki AA, Mavridis D, Higgins JPT, Salanti G. Characteristics of a loop of evidence that affect detection and estimation of inconsistency: a simulation study. _BMC Medical Research Methodology_ 2014; **14**: 106.
* White et al. (2012) White IR, Barrett JK, Jackson D, Higgins JPT. Consistency and inconsistency in network meta-analysis: model estimation using multivariate meta-regression. _Research Synthesis Methods_ 2012; **3**: 111-125.
* White (2015) White IR. Network meta-analysis. _Stata Journal_ 2015; **15**: 951-985.

## 12 Synthesizing and presenting findings using other methods

Joanne E McKenzie, Sue E Brennan

KEY POINTS

* Meta-analysis of effect estimates has many advantages, but other synthesis methods may need to be considered in the circumstance where there is incompletely reported data in the primary studies.
* Alternative synthesis methods differ in the completeness of the data they require, the hypotheses they address, and the conclusions and recommendations that can be drawn from their findings.
* These methods provide more limited information for healthcare decision making than meta-analysis, but may be superior to a narrative description where some results are privileged above others without appropriate justification.
* Tabulation and visual display of the results should always be presented alongside any synthesis, and are especially important for transparent reporting in reviews without meta-analysis.
* Alternative synthesis and visual display methods should be planned and specified in the protocol. When writing the review, details of the synthesis methods should be described.
* Synthesis methods that involve vote counting based on statistical significance have serious limitations and are unacceptable.

### Why a meta-analysis of effect estimates may not be possible

Meta-analysis of effect estimates has many potential advantages (see Chapters 10 and 11). However, there are circumstances where it may not be possible to undertake a meta-analysis and other statistical synthesis methods may be considered (McKenzie and Brennan 2014).

Some common reasons why it may not be possible to undertake a meta-analysis are outlined in Table 1.a. Legitimate reasons include limited evidence; incompletely
\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt} p{142.3pt}} \hline \hline
**Scenario** & **Description** & **Examples of possible solutions*** \\ \hline Limited evidence for a pre-specified comparison & Meta-analysis is not possible with no studies, or only one study. This circumstance may reflect the infancy of research in a particular area, or that the specified PICO for the synthesis aims to address a narrow question. & Build contingencies into the analysis plan to group one or more of the PICO elements at a broader level (Chapter 2, Section 2.5.3). \\ \hline Incompletely reported outcome or effect estimate & Within a study, the intervention effects may be incompletely reported (e.g. effect estimate with no measure of precision; direction of effect with P value or statement of statistical significance; only the direction of effect). & Calculate the effect estimate and measure of precision from the available statistics if possible (Chapter 6). \\ \hline Different effect measures & Across studies, the same outcome could be treated differently (e.g. a time-to-event outcome has been dichotomized in some studies or analysed using different methods. Both scenarios could lead to different effect measures (e.g. hazard ratios and odds ratios). & Calculate the effect estimate and measure of precision for the same effect measure from the available statistics if possible (Chapter 6). \\ \hline Bias in the evidence & Concerns about missing studies, missing outcomes within the studies (Chapter 13), or bias in the studies (Chapters 8 and 25), are legitimate reasons for not undertaking a meta-analysis. These concerns similarly apply to other synthesis methods (Section 12.2). & Calculate the effect estimate and measure of precision from the available statistics if possible (Chapter 6). \\ \hline \hline \end{tabular}
\end{table}
Table 12.1.a Scenarios that may preclude meta-analysis, with possible solutions
[MISSING_PAGE_POST]

#### 12.

reported outcome/effect estimates, or different effect measures used across studies; and bias in the evidence. Other commonly cited reasons for not using meta-analysis are because of too much clinical or methodological diversity, or statistical heterogeneity (Achana et al 2014). However, meta-analysis methods should be considered in these circumstances, as they may provide important insights if undertaken and interpreted appropriately.

### Statistical synthesis when meta-analysis of effect estimates is not possible

A range of statistical synthesis methods are available, and these may be divided into three categories based on their preferability (Table 2.2.a). Preferable methods are the meta-analysis methods outlined in Chapters 10 and 11, and are not discussed in detail here. This chapter focuses on methods that might be considered when a meta-analysis of effect estimates is not possible due to incompletely reported data in the primary studies. These methods divide into those that are 'acceptable' and 'unacceptable'. The 'acceptable' methods differ in the data they require, the hypotheses they address, limitations around their use, and the conclusions and recommendations that can be drawn (see Section 12.2.1). The 'unacceptable' methods in common use are described (see Section 12.2.2), along with the reasons for why they are problematic.

Compared with meta-analysis methods, the 'acceptable' synthesis methods provide more limited information for healthcare decision making. However, these 'acceptable' methods may be superior to a narrative that describes results study by study, which comes with the risk that some studies or findings are privileged above others without appropriate justification. Further, in reviews with little or no synthesis, readers are left to make sense of the research themselves, which may result in the use of seemingly simple yet problematic synthesis methods such as vote counting based on statistical significance (see Section 12.2.2.1).

All methods first involve calculation of a'standardized metric', followed by application of a synthesis method. In applying any of the following synthesis methods, it is important that only one outcome per study (or other independent unit, for example one comparison from a trial with multiple intervention groups) contributes to the synthesis. Chapter 9 outlines approaches for selecting an outcome when multiple have been measured. Similar to meta-analysis, sensitivity analyses can be undertaken to examine if the findings of the synthesis are robust to potentially influential decisions (see Chapter 10, Section 10.14 and Section 12.4 for examples).

Authors should report the specific methods used in lieu of meta-analysis (including approaches used for presentation and visual display), rather than stating that they have conducted a 'narrative synthesis' or 'narrative summary' without elaboration. The limitations of the chosen methods must be described, and conclusions worded with appropriate caution. The aim of reporting this detail is to make the synthesis process more transparent and reproducible, and help ensure use of appropriate methods and interpretation.

### 12 Statistical synthesis

\begin{table}
\begin{tabular}{p{42.7pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline  & & & & & & \\ Synthesis method & Question answered & & & & & & \\ \hline \hline \multicolumn{1}{p{42.7pt}}{Preferable} & & & & & & & \\ Meta-analysis of effect estimates & What is the common intervention (Chapters 10 and 11) & What is the average intervention & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & & \\  & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 12.2.a Summary of preferable and acceptable synthesis methods

#### 12 Synthesizing and presenting findings using other methods

#### 12.2.1 Acceptable synthesis methods

##### 12.2.1.1 Summarizing effect estimates

##### 12.2.1.1 Description of method

Summarizing effect estimates might be considered in the circumstance where estimates of intervention effect are available (or can be calculated), but the variances of the effects are not reported or are incorrect (and cannot be calculated from other statistics, or reasonably imputed) (Grimshaw et al 2003). Incorrect calculation of variances arises more commonly in non-standard study designs that involve clustering or matching (Chapter 23). While missing variances may limit the possibility of meta-analysis, the (standardized) effects can be summarized using descriptive statistics such as the median, interquartile range, and the range. Calculating these statistics addresses the question 'What is the range and distribution of observed effects?'

Reporting of methods and resultsThe statistics that will be used to summarize the effects (e.g. median, interquartile range) should be reported. Box-and-whisker or bubble plots will complement reporting of the summary statistics by providing a visual display of the distribution of observed effects (Section 12.3.3). Tabulation of the available effect estimates will provide transparency for readers by linking the effects to the studies (Section 12.3.1). Limitations of the method should be acknowledged (Table 12.2.2.1).

##### 12.2.1.2 Combining P values

Description of methodCombining P values can be considered in the circumstance where there is no, or minimal, information reported beyond P values and the direction of effect; the types of outcomes and statistical tests differ across the studies; or results from non-parametric tests are reported (Borenstein et al 2009). Combining P values addresses the question 'Is there evidence that there is an effect in at least one study?' There are several methods available (Loughin 2004), with the method proposed by Fisher outlined here (Becker 1994).

Fisher's method combines the P values from statistical tests across \(k\) studies using the formula:

\[\text{Chi}^{2}=-2\sum_{i=1}^{k}\ln(P_{i}).\]

One-sided P values are used, since these contain information about the direction of effect. However, these P values must reflect the same directional hypothesis (e.g. all testing if intervention A is more effective than intervention B). This is analogous to standardizing the direction of effects before undertaking a meta-analysis. Two-sided P values, which do not contain information about the direction, must first be converted to one-sided P values. If the effect is consistent with the directional hypothesis (e.g. intervention A is beneficial compared with B), then the one-sided P value is calculated as

\[P_{1\text{-}sided}=\frac{P_{2\text{-}sided}}{2};\]

otherwise,

\[P_{1\text{-}sided}=1-\left(\frac{P_{2\text{-}sided}}{2}\right).\]In studies that do not report an exact P value but report a conventional level of significance (e.g. P \(<\) 0.05), a conservative option is to use the threshold (e.g. 0.05). The P values must have been computed from statistical tests that appropriately account for the features of the design, such as clustering or matching, otherwise they will likely be incorrect.

The Chi\({}^{2}\) statistic will follow a chi-squared distribution with \(2k\) degrees of freedom if there is no effect in every study. A large Chi\({}^{2}\) statistic compared to the degrees of freedom (with a corresponding low P value) provides evidence of an effect in at least one study (see Section 12.4.2 for guidance on implementing Fisher's method for combining P values).

Reporting of methods and resultsThere are several methods for combining P values (Loughin 2004), so the chosen method should be reported, along with details of sensitivity analyses that examine if the results are sensitive to the choice of method. The results from the test should be reported alongside any available effect estimates (either individual results or meta-analysis results of a subset of studies) using text, tabulation and appropriate visual displays (Section 12.3). The albatross plot is likely to complement the analysis (Section 12.3.4). Limitations of the method should be acknowledged (Table 12.2.a).

##### 12.2.1.3 Vote counting based on the direction of effect

Description of methodVote counting based on the direction of effect might be considered in the circumstance where the direction of effect is reported (with no further information), or there is no consistent effect measure or data reported across studies. The essence of vote counting is to compare the number of effects showing benefit to the number of effects showing harm for a particular outcome. However, there is wide variation in the implementation of the method due to differences in how 'benefit' and 'harm' are defined. Rules based on subjective decisions or statistical significance are problematic and should be avoided (see Section 12.2.2).

To undertake vote counting properly, each effect estimate is first categorized as showing benefit or harm based on the observed direction of effect alone, thereby creating a standardized binary metric. A count of the number of effects showing benefit is then compared with the number showing harm. Neither statistical significance nor the size of the effect are considered in the categorization. A sign test can be used to answer the question 'is there any evidence of an effect?' If there is no effect, the study effects will be distributed evenly around the null hypothesis of no difference. This is equivalent to testing if the true proportion of effects favouring the intervention (or comparator) is equal to 0.5 (Bushman and Wang 2009) (see Section 12.4.2.3 for guidance on implementing the sign test). An estimate of the proportion of effects favouring the intervention can be calculated (\(p=u/n\), where \(u=\) number of effects favouring the intervention, and \(n=\) number of studies) along with a confidence interval (e.g. using the Wilson or Jeffreys interval methods (Brown et al 2001)). Unless there are many studies contributing effects to the analysis, there will be large uncertainty in this estimated proportion.

Reporting of methods and resultsThe vote counting method should be reported in the 'Data synthesis' section of the review. Failure to recognize vote counting as a synthesis method has led to it being applied informally (and perhaps unintentionally) to summarize results (e.g. through the use of wording such as '3 of 10 studies showed improvement in the outcome with intervention compared to control';'most studies found'; 'the majority of studies'; 'few studies' etc). In such instances, the method is rarely reported, and it may not be possible to determine whether an unacceptable (invalid) rule has been used to define benefit and harm (Section 12.2.2). The results from vote counting should be reported alongside any available effect estimates (either individual results or meta-analysis results of a subset of studies) using text, tabulation and appropriate visual displays (Section 12.3). The number of studies contributing to a synthesis based on vote counting may be larger than a meta-analysis, because only minimal statistical information (i.e. direction of effect) is required from each study to vote count. Vote counting results are used to derive the harvest and effect direction plots, although often using unacceptable methods of vote counting (see Section 12.3.5). Limitations of the method should be acknowledged (Table 12.2.a).

#### 12.2.2 Unacceptable synthesis methods

##### 12.2.2.1 Vote counting based on statistical significance

Conventional forms of vote counting use rules based on statistical significance and direction to categorize effects. For example, effects may be categorized into three groups: those that favour the intervention and are statistically significant (based on some predefined P value), those that favour the comparator and are statistically significant, and those that are statistically non-significant (Hedges and Vevea 1998). In a simpler formulation, effects may be categorized into two groups: those that favour the intervention and are statistically significant, and all others (Friedman 2001). Regardless of the specific formulation, when based on statistical significance, all have serious limitations and can lead to the wrong conclusion.

The conventional vote counting method fails because underpowered studies that do not rule out clinically important effects are counted as not showing benefit. Suppose, for example, the effect sizes estimated in two studies were identical. However, only one of the studies was adequately powered, and the effect in this study was statistically significant. Only this one effect (of the two identical effects) would be counted as showing 'benefit'. Paradoxically, Hedges and Vevea showed that as the number of studies increases, the power of conventional vote counting tends to zero, except with large studies and at least moderate intervention effects (Hedges and Vevea 1998). Further, conventional vote counting suffers the same disadvantages as vote counting based on direction of effect, namely, that it does not provide information on the magnitude of effects and does not account for differences in the relative sizes of the studies.

##### 12.2.2.2 Vote counting based on subjective rules

Subjective rules, involving a combination of direction, statistical significance and magnitude of effect, are sometimes used to categorize effects. For example, in a review examining the effectiveness of interventions for teaching quality improvement to clinicians, the authors categorized results as 'beneficial effects', 'no effects' or 'detrimental effects' (Boonyasai et al 2007). Categorization was based on direction of effect and statistical significance (using a predefined P value of 0.05) when available. If statistical significance was not reported, effects greater than 10% were categorized as 'beneficial' or 'detrimental', depending on their direction. These subjective rules often vary in the