elements, cut-offs and algorithms used to categorize effects, and while detailed descriptions of the rules may provide a veneer of legitimacy, such rules have poor performance validity (Ioannidis et al 2008).

A further problem occurs when the rules are not described in sufficient detail for the results to be reproduced (e.g. ter Wee et al 2012, Thomicroft et al 2016). This lack of transparency does not allow determination of whether an acceptable or unacceptable vote counting method has been used (Valentine et al 2010).

### Visual display and presentation of the data

Visual display and presentation of data is especially important for transparent reporting in reviews without meta-analysis, and should be considered irrespective of whether synthesis is undertaken (see Table 2.2.a for a summary of plots associated with each synthesis method). Tables and plots structure information to show patterns in the data and convey detailed information more efficiently than text. This aids interpretation and helps readers assess the veracity of the review findings.

#### Structured tabulation of results across studies

Ordering studies alphabetically by study ID is the simplest approach to tabulation; however, more information can be conveyed when studies are grouped in subpanels or ordered by a characteristic important for interpreting findings. The grouping of studies in tables should generally follow the structure of the synthesis presented in the text, which should closely reflect the review questions. This grouping should help readers identify the data on which findings are based and verify the review authors' interpretation.

If the purpose of the table is comparative, grouping studies by any of following characteristics might be informative:

* comparisons considered in the review, or outcome domains (according to the structure of the synthesis);
* study characteristics that may reveal patterns in the data, for example potential effect modifiers including population subgroups, settings or intervention components.

If the purpose of the table is complete and transparent reporting of data, then ordering the studies to increase the prominence of the most relevant and trustworthy evidence should be considered. Possibilities include:

* certainty of the evidence (synthesized result or individual studies if no synthesis);
* risk of bias, study size or study design characteristics; and
* characteristics that determine how directly a study addresses the review question, for example relevance and validity of the outcome measures.

One disadvantage of grouping by study characteristics is that it can be harder to locate specific studies than when tables are ordered by study ID alone, for example when cross-referencing between the text and tables. Ordering by study ID within categories may partly address this.

The value of standardizing intervention and outcome labels is discussed in Chapter 3 (Sections 3.2.2 and 3.4), while the importance and methods for standardizing effect estimates is described in Chapter 6. These practices can aid readers' interpretation of tabulated data, especially when the purpose of a table is comparative.

#### 12.3.2 Forest plots

Forest plots and methods for preparing them are described elsewhere (Chapter 10, Section 10.2). Some mention is warranted here of their importance for displaying study results when meta-analysis is not undertaken (i.e. without the summary diamond). Forest plots can aid interpretation of individual study results and convey overall patterns in the data, especially when studies are ordered by a characteristic important for interpreting results (e.g. dose and effect size, sample size). Similarly, grouping studies in subpanels based on characteristics thought to modify effects, such as population subgroups, variants of an intervention, or risk of bias, may help explore and explain differences across studies (Schriger et al 2010). These approaches to ordering provide important techniques for informally exploring heterogeneity in reviews without meta-analysis, and should be considered in preference to alphabetical ordering by study ID alone (Schriger et al 2010).

#### 12.3.3 Box-and-whisker plots and bubble plots

Box-and-whisker plots (see Figure 12.4.a, Panel A) provide a visual display of the distribution of effect estimates (Section 12.2.1.1). The plot conventionally depicts five values. The upper and lower limits (or 'hinges') of the box, represent the 75th and 25th percentiles, respectively. The line within the box represents the 50th percentile (median), and the whiskers represent the extreme values (McGill et al 1978). Multiple box plots can be juxtaposed, providing a visual comparison of the distributions of effect estimates (Schriger et al 2006). For example, in a review examining the effects of audit and feedback on professional practice, the format of the feedback (verbal, written, both verbal and written) was hypothesized to be an effect modifier (Ivers et al 2012). Box-and-whisker plots of the risk differences were presented separately by the format of feedback, to allow visual comparison of the impact of format on the distribution of effects. When presenting multiple box-and-whisker plots, the width of the box can be varied to indicate the number of studies contributing to each. The plot's common usage facilitates rapid and correct interpretation by readers (Schriger et al 2010). The individual studies contributing to the plot are not identified (as in a forest plot), however, and the plot is not appropriate when there are few studies (Schriger et al 2006).

A bubble plot (see Figure 12.4.a, Panel B) can also be used to provide a visual display of the distribution of effects, and is more suited than the box-and-whisker plot when there are few studies (Schriger et al 2006). The plot is a scatter plot that can display multiple dimensions through the location, size and colour of the bubbles. In a review examining the effects of educational outreach visits on professional practice, a bubble plot was used to examine visually whether the distribution of effects was modified by the targeted behaviour (O'Brien et al 2007). Each bubble represented the effect size (y-axis) and whether the study targeted a prescribing or other behaviour (x-axis).

The size of the bubbles reflected the number of study participants. However, different formulations of the bubble plot can display other characteristics of the data (e.g. precision, risk-of-bias assessments).

#### 12.3.4 Albatross plot

The albatross plot (see Figure 12.4.a, Panel C) allows approximate examination of the underlying intervention effect sizes where there is minimal reporting of results within studies (Harrison et al 2017). The plot only requires a two-sided P value, sample size and direction of effect (or equivalently, a one-sided P value and a sample size) for each result. The plot is a scatter plot of the study sample sizes against two-sided P values,

Figure 12.4.a: Possible graphical displays of different types of data. (A) Box-and-whisker plots of odds ratios for all outcomes and separately by overall risk of bias. (B) Bubble plot of odds ratios for all outcomes and separately by the model of care. The colours of the bubbles represent the overall risk of bias judgement (dark grey = low risk of bias; light grey = some concerns; blue = high risk of bias). (C) Albatross plot of the study sample size against P values (for the five continuous outcomes in Table 12.4.c, column 6). The effect contours represent standardized mean differences. (D) Harvest plot (height depicts overall risk of bias judgement (all = low risk of bias; medium = some concerns; short = high risk of bias), shading depicts model of care (light grey = caseload; dark grey = team), alphabet characters represent the studies)

where the results are separated by the direction of effect. Superimposed on the plot are 'effect size contours' (inspiring the plot's name). These contours are specific to the type of data (e.g. continuous, binary) and statistical methods used to calculate the P values. The contours allow interpretation of the approximate effect sizes of the studies, which would otherwise not be possible due to the limited reporting of the results. Characteristics of studies (e.g. type of study design) can be identified using different colours or symbols, allowing informal comparison of subgroups.

The plot is likely to be more inclusive of the available studies than meta-analysis, because of its minimal data requirements. However, the plot should complement the results from a statistical synthesis, ideally a meta-analysis of available effects.

##### Harvest and effect direction plots

Harvest plots (see Figure 12.4.a, Panel D) provide a visual extension of vote counting results (Ogilvie et al 2008). In the plot, studies based on the categorization of their effects (e.g. 'beneficial effects', 'no effects' or 'detrimental effects') are grouped together. Each study is represented by a bar positioned according to its categorization. The bars can be 'visually weighted' (by height or width) and annotated to highlight study and outcome characteristics (e.g. risk-of-bias domains, proximal or distal outcomes, study design, sample size) (Ogilvie et al 2008, Crowther et al 2011). Annotation can also be used to identify the studies. A series of plots may be combined in a matrix that displays, for example, the vote counting results from different interventions or outcome domains.

The methods papers describing harvest plots have employed vote counting based on statistical significance (Ogilvie et al 2008, Crowther et al 2011). For the reasons outlined in Section 12.2.2.1, this can be misleading. However, an acceptable approach would be to display the results based on direction of effect.

The effect direction plot is similar in concept to the harvest plot in the sense that both display information on the direction of effects (Thomson and Thomas 2013). In the first version of the effect direction plot, the direction of effects for _each outcome within a single study_ are displayed, while the second version displays the direction of the effects for _outcome domains across studies_. In this second version, an algorithm is first applied to'synthesize' the directions of effect for all outcomes within a domain (e.g, outcomes'sleep disturbed by wheeze', wheeze limits speech', 'wheeze during exercise' in the outcome domain'respiratory'). This algorithm is based on the proportion of effects that are in a consistent direction and statistical significance. Arrows are used to indicate the reported direction of effect (for either outcomes or outcome domains). Features such as statistical significance, study design and sample size are denoted using size and colour. While this version of the plot conveys a large amount of information, it requires further development before its use can be recommended since the algorithm underlying the plot is likely to have poor performance validity.

### Worked example

The example that follows uses four scenarios to illustrate methods for presentation and synthesis when meta-analysis is not possible. The first scenario contrasts a common approach to tabulation with alternative presentations that may enhance the transparency of reporting and interpretation of findings. Subsequent scenarios show the application of the synthesis approaches outlined in preceding sections of the chapter. Box 12.4.a summarizes the review comparisons and outcomes, and decisions taken by the review authors

**Box 12.4.a** The review

The review used in this example examines the effects of midwfe-led continuity models versus other models of care for childbearing women. One of the outcomes considered in the review, and of interest to many women choosing a care option, is maternal satisfaction with care. The review included 15 randomized trials, all of which reported a measure of satisfaction. Overall, 32 satisfaction outcomes were reported, with between one and 11 outcomes reported per study. There were differences in the concepts measured (e.g. global satisfaction; specific domains such as of satisfaction with information), the measurement period (i.e. antenatal, intrapartum, postpartum care), and the measurement tools (different scales; variable evidence of validity and reliability). Before conducting their synthesis, the review authors did the following.

1. **Specified outcome groups in their protocol** (see Chapter 3). Five types of satisfaction outcomes were defined (global measures, satisfaction with information, satisfaction with decisions, satisfaction with care, sense of control), any of which would be grouped for synthesis since they all broadly reflect satisfaction with care. The review authors hypothesized that the period of care (antenatal, intrapartum, postpartum) might influence satisfaction with a model of care, so planned to analyse outcomes for each period separately. The review authors specified that outcomes would be synthesized across periods if data were sparse.
2. **Specified decision rules in their protocol for dealing with multiplicity of outcomes** (Chapter 3). For studies that reported multiple satisfaction outcomes per period, one outcome would be chosen by (i) selecting the most relevant outcome (a global measure > satisfaction with care > sense of control > satisfaction with decisions > satisfaction with information), and if there were two or more equally relevant outcomes, then (ii) selecting the measurement tool with best evidence of validity and reliability.
3. **Examined study characteristics to determine which studies were similar enough for synthesis** (Chapter 9). All studies had similar models of care as a comparator. Satisfaction outcomes from each study were categorized into one of the five pre-specified categories, and then the decision rules were applied to select the most relevant outcome for synthesis.
4. **Determined what data were available for synthesis** (Chapter 9). All measures of satisfaction were ordinal; however, outcomes were treated differently across studies (see Tables 12.4.a, 12.4.b and 12.4.c). In some studies, the outcome was dichotomized, while in others it was treated as ordinal or continuous. Based on their pre-specified synthesis methods, the review authors selected the preferred method for the available data. In this example, four scenarios, with progressively fewer data, are used to illustrate the application of alternative synthesis methods.
5. **Determined if modification to the planned comparison or outcomes was needed**. No changes were required to comparisons or outcome groupings.

in planning their synthesis. While the example is loosely based on an actual review, the review description, scenarios and data are fabricated for illustration.

##### Scenario 1: structured reporting of effects

We first address a scenario in which review authors have decided that the tools used to measure satisfaction measured concepts that were too dissimilar across studies for synthesis to be appropriate. Setting aside three of the 15 studies that reported on the birth partner's satisfaction with care, a structured summary of effects is sought of the remaining 12 studies. To keep the example table short, only one outcome is shown per study for each of the measurement periods (antenatal, intrapartum or postpartum).

Table 12.4.a depicts a common yet suboptimal approach to presenting results. Note two features.

* Studies are ordered by study ID, rather than grouped by characteristics that might enhance interpretation (e.g. risk of bias, study size, validity of the measures, certainty of the evidence (GRADE)).
* Data reported are as extracted from each study; effect estimates were not calculated by the review authors and, where reported, were not standardized across studies (although data were available to do both).

Table 12.4.b shows an improved presentation of the same results. In line with best practice, here effect estimates have been calculated by the review authors for all outcomes, and a common metric computed to aid interpretation (in this case an odds ratio; see Chapter 6 for guidance on conversion of statistics to the desired format). Redundant information has been removed ('statistical test' and 'P value' columns). The studies have been re-ordered, first to group outcomes by period of care (intrapartum outcomes are shown here), and then by risk of bias. This re-ordering serves two purposes. Grouping by period of care aligns with the plan to consider outcomes for each period separately and ensures the table structure matches the order in which results are described in the text. Re-ordering by risk of bias increases the prominence of studies at lowest risk of bias, focusing attention on the results that should most influence conclusions. Had the review authors determined that a synthesis would be informative, then ordering to facilitate comparison across studies would be appropriate; for example, ordering by the type of satisfaction outcome (as pre-defined in the protocol, starting with global measures of satisfaction), or the comparisons made in the studies.

The results may also be presented in a forest plot, as shown in Figure 12.4.b. In both the table and figure, studies are grouped by risk of bias to focus attention on the most trustworthy evidence. The pattern of effects across studies is immediately apparent in Figure 12.4.b and can be described efficiently without having to interpret each estimate (e.g. difference between studies at low and high risk of bias emerge), although these results should be interpreted with caution in the absence of a formal test for subgroup differences (see Chapter 10, Section 10.11). Only outcomes measured during the intrapartum period are displayed, although outcomes from other periods could be added, maximizing the information conveyed.

An example description of the results from Scenario 1 is provided in Box 12.4.b. It shows that describing results study by study becomes unwieldy with more than a 

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline \multicolumn{1}{c}{**Outcome (scale details*)**} & \multicolumn{1}{c}{**Intervention**} & \multicolumn{1}{c}{**Control**} & \multicolumn{1}{c}{**(metric)**} & \multicolumn{1}{c}{**95\% CI**} & \multicolumn{1}{c}{**Statistical test**} & \multicolumn{1}{c}{**P value**} \\ \hline Barry 2005 & \% (N) & \% (N) & & & & \\ Experience of labour & 37\% (246) & 32\% (223) & 5\% (RD) & & & \(P>0.05\) \\ Biro 2000 & n/N & n/N & & & & \\ Perception of care: labour/birth & 260/344 & 192/287 & 1.13 (RR) & 1.02 to 1.25 & 2 = 2.36 & 0.018 \\ Crowe 2010 & Mean (SD) N & Mean (SD) N & & & \\ Experience of antenatal care & 21.0 (5.6) 182 & 19.7 (7.3) 186 & 1.3 (MD) & -0.1 to 2.7 & t = 1.88 & 0.061 \\ (0 to 24 points) & 9.8 (3.1) 182 & 9.3 (3.3) 186 & 0.5 (MD) & -0.2 to 1.2 & t = 1.50 & 0.135 \\ Experience of postpartum care & 11.7 (2.9) 182 & 10.9 (4.2) 186 & 0.8 (MD) & 0.1 to 1.5 & t = 2.12 & 0.035 \\ (0 to 18 points) & & & & & \\ Flint 1989 & n/N & n/N & & & & \\ Care from staff during labour & 240/275 & 208/256 & 1.07 (RR) & 1.00 to 1.16 & 2 = 1.89 & 0.059 \\ Frances 2000 & & & & & \\ Communication: labour/birth & & & & & & \\ Harvey 1996 & Mean (SD) N & Mean (SD) N & & & \\ Labour \& Delivery Satisfaction & 182 (14.2) 101 & 185 (30) 93 & t = -0.90 for MD & 0.369 for MD \\ Index (37 to 22 points) & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 12.4.a Scenario 1: table ordered by study ID, data as reported by study authors

[MISSING_PAGE_EMPTY:8]

#### 12.2 Synthesizing and presenting findings using other methods

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Outcome\({}^{*}\) (scale details)** & **Intervention** & **Control** & **Mean difference (95\% CI)\({}^{**}\)** & **Odsds ratio (95\% CI)\({}^{\dagger}\)** \\ \hline
**Low risk of bias** & & & \\ \hline Barry 2005 & n/N & n/N & \\ Experience of labour & 90/246 & 72/223 & 1.21 (0.82 to 1.79) \\ Frances 2000 & n/N & n/N & \\ Communication: labour/birth & & & 0.90 (0.61 to 1.34) \\ Rowley 1995 & n/N & n/N & \\ Encouraged to ask questions [during labour/birth] & & & 1.02 (0.66 to 1.58) \\ \hline
**Some concerns** & & & \\ Biro 2000 & n/N & n/N & \\ Perception of care: labour/birth & 260/344 & 192/287 & 1.54 (1.08 to 2.19) \\ Crowe 2010 & Mean (SD) N & Mean (SD) N & \\ Experience of labour/birth (0 to 18 points) & 9.8 (3.1) 182 & 9.3 (3.3) 186 & 0.5 (-0.15 to 1.15) & 1.32 (0.91 to 1.92) \\ Harvey 1996 & Mean (SD) N & Mean (SD) N & \\ Labour \& Delivery Satisfaction Index (37 to 222 points) & 182 (14.2) 101 & 185 (300) 93 & â€“3 (-10 to 4) & 0.79 (0.48 to 1.32) \\ Johns 2004 & n/N & n/N & \\ Satisfaction with intrapartum care & 605/1163 & 363/826 & 1.38 (1.15 to 1.64) \\ Parr 2002 & n/N & n/N & \\ Experience of childbirth & & & 0.85 (0.39 to 1.87) \\ \hline \hline \end{tabular}
\end{table}
Table 12.4.b Scenario 1: intrapartum outcome table ordered by risk of bias, standardized effect estimates calculated for all studies

[MISSING_PAGE_EMPTY:10]

few studies, highlighting the importance of tables and plots. It also brings into focus the risk of presenting results without any synthesis, since it seems likely that the reader will try to make sense of the results by drawing inferences across studies. Since a synthesis was considered inappropriate, GRADE was applied to individual studies and then used to prioritize the reporting of results, focusing attention on the most relevant and trustworthy evidence. An alternative might be to report results at low risk of bias, an approach analogous to limiting a meta-analysis to studies at low risk of bias. Where possible, these and other approaches to prioritizing (or ordering) results from individual studies in text and tables should be pre-specified at the protocol stage.

#### Overview of scenarios 2-4: synthesis approaches

We now address three scenarios in which review authors have decided that the outcomes reported in the 15 studies all broadly reflect satisfaction with care. While the measures were quite diverse, a synthesis is sought to help decision makers understand whether women and their birth partners were generally more satisfied with the care received in midw-led continuity models compared with other models. The three scenarios differ according to the data available (see Table 4.c), with each reflecting progressively less complete reporting of the effect estimates. The data available determine the synthesis method that can be applied.

Figure 4.b: Forest plot depicting standardized effect estimates (odds ratios) for satisfaction

* Scenario 2: effect estimates available without measures of precision (illustrating synthesis of summary statistics).
* Scenario 3: P values available (illustrating synthesis of P values).
* Scenario 4: directions of effect available (illustrating synthesis using vote-counting based on direction of effect).

For studies that reported multiple satisfaction outcomes, one result is selected for synthesis using the decision rules in Box 12.4.a (point 2).

##### Scenario 2: summarizing effect estimates

In Scenario 2, effect estimates are available for all outcomes. However, for most studies, a measure of variance is not reported, or cannot be calculated from the available data. We illustrate how the effect estimates may be summarized using descriptive statistics. In this scenario, it is possible to calculate odds ratios for all studies. For the continuous outcomes, this involves first calculating a standardized mean difference, and then converting this to an odds ratio (Chapter 10, Section 10.6). The median odds ratio is 1.32 with an interquartile range of 1.02 to 1.53 (15 studies). Box-and-whisker plots may be used to display these results and examine informally whether the distribution of effects differs by the overall risk-of-bias assessment (Figure 12.4.a, Panel A). However, because there are relatively few effects, a reasonable alternative would be to present bubble plots (Figure 12.4.a, Panel B).

An example description of the results from the synthesis is provided in Box 12.4.c.

12.4 Worked example

**Box 12.4.c** How to describe the results from this synthesis

**Scenario 2. Synthesis of summary statistics**

'The median odds ratio of satisfaction was 1.32 for midwfe-led models of care compared with other models (interquartile range 1.02 to 1.53; 15 studies). Only five of the 15 effects were judged to be at a low risk of bias, and informal visual examination suggested the size of the odds ratios may be smaller in this group.'

##### 12.4.2 Scenario 3: combining P values

In Scenario 3, there is minimal reporting of the data, and the type of data and statistical methods and tests vary. However, 11 of the 15 studies provide a precise P value and direction of effect, and a further two report a P value less than a threshold (\(<\) 0.001) and direction. We use this scenario to illustrate a synthesis of P values. Since the reported P values are two-sided (Table 12.4.c, column 6), they must first be converted to one-sided P values, which incorporate the direction of effect (Table 12.4.c, column 7).

Fisher's method for combining P values involved calculating the following statistic:

\[X^{2}=-2\sum_{i=1}^{k}\ln(P_{i})=-2\times(\ln(0.068)+\ldots+\ln(0.170))=-2 \times-41.2=82.3\]

where \(P_{i}\) is the one-sided P value from study \(i\), and \(k\) is the total number of P values. This formula can be implemented using a standard spreadsheet package. The statistic is then compared against the chi-squared distribution with 26 (\(=\) 2 \(\times\) 13) degrees of freedom to obtain the P value. Using a Microsoft Excel spreadsheet, this can be obtained by typing =CHIDIST(82.3, 26) into any cell. In Stata or R, the packages (both named) **metap** could be used. These packages include a range of methods for combining P values.

The combination of P values suggests there is strong evidence of benefit of midwfe-led models of care in at least one study (P \(<\) 0.001 from a Chi\({}^{2}\) test, 13 studies). Restricting this analysis to those studies judged to be at an overall low risk of bias (sensitivity analysis), there is no longer evidence to reject the null hypothesis of no benefit of midwfe-led model of care in any studies (P = 0.314, 3 studies). For the five studies reporting continuous satisfaction outcomes, sufficient data (precise P value, direction, total sample size) are reported to construct an albatross plot (Figure 12.4.a, Panel C). The location of the points relative to the standardized mean difference contours indicate that the likely effects of the intervention in these studies are small.

An example description of the results from the synthesis is provided in Box 12.4.d.

**Box 12.4.d** How to describe the results from this synthesis

**Scenario 3. Synthesis of P values**

'There was strong evidence of benefit of midwfe-led models of care in at least one study (P \(<\) 0.001, 13 studies). However, a sensitivity analysis restricted to studies with an overall low risk of bias suggested there was no effect of midwfe-led models of care in any of the trials (P = 0.314, 3 studies). Estimated standardized mean differences for five of the outcomes were small (ranging from -0.13 to 0.45) (Figure 12.4.a, Panel C).'

#### 12.4.2.3 Scenario 4: vote counting based on direction of effect

In Scenario 4, there is minimal reporting of the data, and the type of effect measure (when used) varies across the studies (e.g. mean difference, proportional odds ratio). Of the 15 results, only five report data suitable for meta-analysis (effect estimate and measure of precision; Table 12.4.3, column 8), and no studies reported precise P values. We use this scenario to illustrate vote counting based on direction of effect. For each study, the effect is categorized as beneficial or harmful based on the direction of effect (indicated as a binary metric; Table 12.4.3, column 9).

Of the 15 studies, we exclude three because they do not provide information on the direction of effect, leaving 12 studies to contribute to the synthesis. Of these 12, 10 effects favour midwife-led models of care (83%). The probability of observing this result if midwife-led models of care are truly ineffective is 0.039 (from a binomial probability test, or equivalently, the sign test). The 95% confidence interval for the percentage of effects favouring midwife-led care is wide (55% to 95%).

The binomial test can be implemented using standard computer spreadsheet or statistical packages. For example, the two-sided P value from the binomial probability test presented can be obtained from Microsoft Excel by typing =2*BINOM.DIST(2, 12, 0.5, TRUE) into any cell in the spreadsheet. The syntax requires the smaller of the 'number of effects favouring the intervention' or 'the number of effects favouring the control' (here, the smaller of these counts is 2), the number of effects (here 12), and the null value (true proportion of effects favouring the intervention = 0.5). In Stata, the **bitest** command could be used (e.g. **bitesti 12 10 0.5**).

A harvest plot can be used to display the results (Figure 12.4.1, Panel D), with characteristics of the studies represented using different heights and shading. A sensitivity analysis might be considered, restricting the analysis to those studies judged to be at an overall low risk of bias. However, only four studies were judged to be at a low risk of bias (of which, three favoured midwife-led models of care), precluding reasonable interpretation of the count.

An example description of the results from the synthesis is provided in Box 12.4.3.

### Chapter information

**Authors:** Joanne E McKenzie, Sue E Brennan

**Acknowledgements:** Sections of this chapter build on chapter 9 of version 5.1 of the _Handbook_, with editors Jonathan J Deeks, Julian PT Higgins and Douglas G Altman.

**Box 12.4.4 How to describe the results from this synthesis**

**Scenario 4. Synthesis using vote counting based on direction of effects**

'There was evidence that midwife-led models of care had an effect on satisfaction, with 10 of 12 studies favouring the intervention (83% (95% CI 55% to 95%), P = 0.039) (Figure 12.4.1, Panel D). Four of the 12 studies were judged to be at a low risk of bias, and three of these favoured the intervention. The available effect estimates are presented in [review] Table X.'We are grateful to the following for commenting helpfully on earlier drafts: Miranda Cumpston, Jamie Hartmann-Boyce, Tianjing Li, Rebecca Ryan and Hilary Thomson.

**Funding:**: JEM is supported by an Australian National Health and Medical Research Council (NHMRC) Career Development Fellowship (1143429). SEB's position is supported by the NHMRC Cochrane Collaboration Funding Program.

### References

* Achana et al. (2014) Achana F, Hubbard S, Sutton A, Kendrick D, Cooper N. An exploration of synthesis methods in public health evaluations of interventions concludes that the use of modern statistical methods would be beneficial. _Journal of Clinical Epidemiology_ 2014; **67**: 376-390.
* Becker (1994) Becker BJ. Combining significance levels. In: Cooper H, Hedges LV, editors. _A handbook of research synthesis_. New York (NY): Russell Sage; 1994. pp. 215-235.
* Boonyasai et al. (2007) Boonyasai RT, Windish DM, Chakraborti C, Feldman LS, Rubin HR, Bass EB. Effectiveness of teaching quality improvement to clinicians: a systematic review. _JAMA_ 2007; **298**: 1023-1037.
* Borenstein et al. (2009) Borenstein M, Hedges LV, Higgins JPT, Rothstein HR. Meta-Analysis methods based on direction and p-values. _Introduction to Meta-Analysis_. Chichester (UK): John Wiley & Sons, Ltd; 2009. pp. 325-330.
* Brown and Cai (2001) Brown LD, Cai TT, DasGupta A. Interval estimation for a binomial proportion. _Statistical Science_ 2001; **16**: 101-117.
* Bushman and Wang (2009) Bushman BJ, Wang MC. Vote-counting procedures in meta-analysis. In: Cooper H, Hedges LV, Valentine JC, editors. _Handbook of Research Synthesis and Meta-Analysis_. 2nd ed. New York (NY): Russell Sage Foundation; 2009. pp. 207-220.
* Crowther et al. (2001) Crowther M, Avenell A, MacLennan G, Mowatt G. A further use for the Harvest plot: a novel method for the presentation of data synthesis. _Research Synthesis Methods_ 2011; **2**: 79-83.
* Friedman (2001) Friedman L. Why vote-count reviews don't count. _Biological Psychiatry_ 2001; **49**: 161-162.
* Grimshaw et al. (2003) Grimshaw J, McAuley LM, Bero LA, Grilli R, Oxman AD, Ramsay C, Vale L, Zwarestein M. Systematic reviews of the effectiveness of quality improvement strategies and programmes. _Quality and Safety in Health Care_ 2003; **12**: 298-303.
* Harrison et al. (2017) Harrison S, Jones HE, Martin RM, Lewis SJ, Higgins JPT. The albatross plot: a novel graphical tool for presenting results of diversely reported studies in a systematic review. _Research Synthesis Methods_ 2017; **8**: 281-289.
* Hedges et al. (1998) Hedges L, Veeva J. Fixed- and random-effects models in meta-analysis. _Psychological Methods_ 1998; **3**: 486-504.
* Ioannidis and Patsopoulos (2008) Ioannidis JP, Patsopoulos NA, Rothstein HR. Reasons or excuses for avoiding meta-analysis in forest plots. _BMJ_ 2008; **336**: 1413-1415.
* Ivers et al. (2012) Ivers N, Jamtvedt G, Flottorp S, Young JM, Odgaard-Jensen J, French SD, O'Brien MA, Johansen M, Grimshaw J, Oxman AD. Audit and feedback: effects on professional practice and healthcare outcomes. _Cochrane Database of Systematic Reviews_ 2012; **6**: CD000259.
* Jones (1995) Jones DR. Meta-analysis: weighing the evidence. _Statistics in Medicine_ 1995; **14**: 137-149.
* Loughin (2004) Loughin TM. A systematic comparison of methods for combining p-values from independent tests. _Computational Statistics & Data Analysis_ 2004; **47**: 467-485.
* Loughin (2007)McGill R, Tukey JW, Larsen WA. Variations of box plots. _The American Statistician_ 1978; **32**: 12-16.
* McKenzie and Brennan (2014) McKenzie JE, Brennan SE. Complex reviews: methods and considerations for summarising and synthesising results in systematic reviews with complexity. Report to the Australian National Health and Medical Research Council. 2014.
* O'Brien et al. (2007) O'Brien MA, Rogers S, Jamtvedt G, Oxman AD, Odgaard-Jensen J, Kristoffersen DT, Forsetlund L, Bainbridge D, Freemantle N, Davis DA, Haynes RB, Harvey EL. Educational outreach visits: effects on professional practice and health care outcomes. _Cochrane Database of Systematic Reviews_ 2007; **4**: CD000409.
* Ogilvie et al. (2008) Ogilvie D, Fayter D, Petricrew M, Sowden A, Thomas S, Whitehead M, Worthy G. The harvest plot: a method for synthesising evidence about the differential effects of interventions. _BMC Medical Research Methodology_ 2008; **8**: 8.
* Riley et al. (2011) Riley RD, Higgins JP, Deeks JJ. Interpretation of random effects meta-analyses. _BMJ_ 2011; **342**: d549.
* Schriger et al. (2006) Schriger DL, Sinha R, Schroter S, Liu PY, Altman DG. From submission to publication: a retrospective review of the tables and figures in a cohort of randomized controlled trials submitted to the British Medical Journal. _Annals of Emergency Medicine_ 2006; **48**: 750-756, 756 e751-721.
* Schriger et al. (2010) Schriger DL, Altman DG, Vetter JA, Heafner T, Moher D. Forest plots in reports of systematic reviews: a cross-sectional study reviewing current practice. _International Journal of Epidemiology_ 2010; **39**: 421-429.
* ter Wee et al. (2012) ter Wee MM, Lems WF, Usan H, Gulpen A, Boonen A. The effect of biological agents on work participation in rheumatoid arthritis patients: a systematic review. _Annals of the Rheumatic Diseases_ 2012; **71**: 161-171.
* Thomson and Thomas (2013) Thomson HJ, Thomas S. The effect direction plot: visual display of non-standardised effects across multiple outcome domains. _Research Synthesis Methods_ 2013; **4**: 95-101.
* Thornicroft et al. (2016) Thornicroft G, Mehta N, Clement S, Evans-Lacko S, Doherty M, Rose D, Koschorke M, Shidhaye R, O'Reilly C, Henderson C. Evidence for effective interventions to reduce mental-health-related stigma and discrimination. _Lancet_ 2016; **387**: 1123-1132.
* Valentine et al. (2010) Valentine JC, Pigott TD, Rothstein HR. How many studies do you need?: a primer on statistical power for meta-analysis. _Journal of Educational and Behavioral Statistics_ 2010; **35**: 215-247.

## 13 Assessing risk of bias due to missing results in a synthesis

_Matthew J Page, Julian PT Higgins, Jonathan AC Sterne_

This chapter should be cited as: Page MJ, Higgins JPT, Sterne JAC. Chapter 13: Assessing risk of bias due to missing results in a synthesis. In: Higgins JPT, Thomas J, Chandler J, Cumpston M, Li T, Page MJ, Welch VA (editors). _Cochrane Handbook for Systematic Reviews of Interventions._ 2nd Edition. Chichester (UK): John Wiley & Sons, 2019: 349-374.

### Introduction

Systematic reviews seek to identify all research that meets pre-specified eligibility criteria. This goal can be compromised if decisions about how, when or where to report results of eligible studies are influenced by the P value, magnitude or direction of the study's results. For example,'statistically significant' results that suggest an intervention works are more likely than'statistically non-significant' results to be available, available rapidly, available in high impact journals and cited by others, and hence more easily identifiable for systematic reviews. The term'reporting bias' has often been used to describe this problem, but we prefer the term **non-reporting bias**.

Non-reporting biases lead to **bias due to missing results** in a systematic review. Syntheses such as meta-analyses are at risk of bias due to missing results when results of some eligible studies are unavailable because of the P value, magnitude or direction of the results. Bias due to missing results differs from a related source of bias - **bias in selection of the reported result** - where study authors select a result for reporting from among multiple measurements or analyses, on the basis of the P value, magnitude or direction of the results. In such cases, the study result that is available for inclusion in the synthesis is at risk of bias. Bias in selection of the reported result is described in more detail in Chapter 7, and addressed in the RoB 2 tool (Chapter 8) and ROBINS-I tool (Chapter 25).

Failure to consider the potential impact of non-reporting biases on the results of the review can lead to the uptake of ineffective and harmful interventions in clinical practice. For example, when unreported results were included in a systematic review of oseltamivir (Tamiflu) for influenza, the drug was not shown to reduce hospital admissions, had unclear effects on pneumonia and other complications of influenza, and increased the risk of harms such as nausea, vomiting and psychiatric adverse events. These findings were different from synthesized results based only on published study results (Jefferson et al 2014).

We structure the chapter as follows. We start by discussing approaches for avoiding or minimizing bias due to missing results in systematic reviews in Section 13.2, and provide guidance for assessing the risk of bias due to missing results in Section 13.3. For the purpose of discussing these biases,'statistically significant' (P < 0.05) results are sometimes denoted as 'positive' results and'statistically non-significant' or null results as 'negative' results. As explained in Chapter 15, Cochrane Review authors should not use any of these labels when reporting their review findings, since they are based on arbitrary thresholds and may not reflect the clinical or policy significance of the findings.

In this chapter, we use the term **result** to describe the combination of a point estimate (such as a mean difference or risk ratio) and a measure of its precision (such as a confidence interval) for a particular study outcome. We use the term **outcome** to refer to an event (such as mortality or a reduction in pain). When fully defined, an outcome for an individual participant includes the following elements: an outcome domain; a specific measure; a specific metric; and a time point (Zarin et al 2011). An example of a fully defined outcome is 'a 50% change from baseline to eight weeks on the Montgomery-Asberg Depression Rating Scale total score'. A corresponding result for this outcome additionally requires a method of aggregation across individuals: here it might be a risk ratio with 95% confidence interval, which estimates the between-group difference in the proportion of people with the outcome.

### 13.2 Minimizing risk of bias due to missing results

The convincing evidence for the presence of non-reporting biases, summarized in Chapter 7 (Section 7.2.3), should be of great concern to review authors. Regardless of whether an entire study report or a particular study result is unavailable selectively (e.g. because the P value, magnitude or direction of the results were considered unfavourable by the investigators), the same consequence can arise: risk of bias in a synthesis because available results differ systematically from missing results. We discuss two means of reducing, or potentially avoiding, bias due to missing results.

#### Inclusion of results from sources other than published reports

Eyding and colleagues provide a striking example of the value of searching beyond the published literature (Eyding et al 2010). They sought data from published trials of reboxetine versus placebo for major depression, as well as unpublished data from the manufacturer (Pfizer, Berlin). Of 13 trials identified, data for only 26% were published. Meta-analysis painted a far roiser picture of the effects of reboxetine when restricted to the published results (Figure 13.2.a). For example, the between-group difference in the number of patients with an important reduction in depression was much larger in the published trial compared with a meta-analysis of the published and unpublished trials. Similarly, a meta-analysis of two published trials suggested a negligible difference between reboxetine and placebo in the number of patients who withdrew because of adverse events. However, when six unpublished trials were

Figure 13.2.a: Results of meta-analyses of reboxetine versus placebo for acute treatment of major depression, with or without unpublished data (data from Eyding et al 2010). Reproduced with permission of BMJ Publishing Group.

added, the summary estimate suggested that patients on reboxetine were more than twice as likely to withdraw (Eyding et al 2010).

Cases such as this illustrate how bias in a meta-analysis can be reduced by the inclusion of missing results. In other situations, the bias reduction may not be so dramatic. Schmucker and colleagues reviewed five methodological studies examining the difference in summary effect estimates of 173 meta-analyses that included or omitted results from sources other than journal articles (e.g. conference abstracts, theses, government reports, regulatory websites) (Schmucker et al 2017). They found that the direction and magnitude of the differences in summary estimates varied. While inclusion of unreported results may not change summary estimates markedly in all cases, doing so often leads to an increase in precision of the summary estimates (Schmucker et al 2017). Guidance on searching for unpublished sources is included in Chapter 4 (Section 4.3).

##### Inclusion of results from trials results registers

As outlined in Chapter 4 (Section 4.3.3), trials registers can be used to identify any initiated, ongoing or completed (but not necessarily published) studies that meet the eligibility criteria of a review. In 2008, ClinicalTrials.gov created data fields to accept summary results for any registered trial (see Chapter 5, Section 5.3.1) (Zarin et al 2011). A search of ClinicalTrials.gov in June 2019 retrieved over 305,000 studies, of which summary results were reported for around 36,000 (12%). Empirical evidence suggests that including results from ClinicalTrials.gov can lead to important changes in the results of some meta-analyses. When Baudard and colleagues searched trials registers for 95 systematic reviews of pharmaceutical interventions that had not already done so, they identified 122 trials that were eligible for inclusion in 41 (47%) of the reviews (Baudard et al 2017). Results for 45 of the 122 trials were available and could be included in a meta-analysis in 14 of the reviews. The percentage change in meta-analytic effects after including results from trials registers was greater than 10% for five of the 14 reviews and greater than 20% for two reviews; in almost all cases the revised meta-analysis showed decreased efficacy of the drug (Baudard et al 2017). Several initiatives are underway to increase results posting in ClinicalTrials.gov and the European Union Clinical Trials Register (DeVito et al 2018, Goldacre et al 2018), so searching these registers should continue to be an important way of minimizing bias in future systematic reviews.

##### Inclusion of results from clinical study reports and other regulatory documents

Another way to minimize risk of bias due to missing results in reviews of regulated interventions (e.g. drugs, biologics) is to seek clinical study reports (CSRs) and other regulatory documents, such as FDA Drug Approval Packages (see Chapter 4, Section 4.3.4). CSRs are comprehensive documents submitted by pharmaceutical companies in an application for regulatory approval of a product (Jefferson et al 2018), while FDA Drug Approval Packages (at the Drugs@FDA website) include summaries of CSRs and related documents, written by FDA staff (Ladanie et al 2018) (see Chapter 5, Sections 5.5.6 and 5.5.7). For some trials, regulatory data are the only source of information about the trial. Comparisons of the results available in regulatory documents with results available in corresponding journal articles have revealed that unfavourable results for benefit outcomes and adverse events are largely under-reported in journal articles (Wieseler et al 2013, Maund et al 2014, Schroll et al 2016). A few systematic reviews have found that conclusions about the benefits and harms of interventions changed after regulatory data were included in the review (Turner et al 2008, Rodgers et al 2013, Jefferson et al 2014).

CSRs and other regulatory documents have great potential for improving the credibility of systematic reviews of regulated interventions, but substantial resources are needed to access them and disentangle the data within them (Schroll et al 2015, Doshi and Jefferson 2016). Only limited guidance is currently available for review authors considering embarking on a review including regulatory data. Jefferson and colleagues provide criteria for assessing whether to include regulatory data for a drug or biologic in a systematic review (Jefferson et al 2018). The R1AT (Restoring Invisible and Abandoned Trials) Support Center website provides useful information, including a taxonomy of regulatory documents, a glossary of relevant terms, guidance on how to request CSRs from regulators and contact information for making requests (Doshi et al 2018). Also, Ladanie and colleagues provide guidance on how to access and use FDA Drug Approval Packages for evidence syntheses (Ladanie et al 2018).

#### Restriction of syntheses to inception cohorts

Review authors can sometimes reduce the risk of bias due to missing results by limiting the type of studies that are eligible for inclusion. Because systematic reviews traditionally search comprehensively for completed studies, non-reporting biases, poor indexing and other factors make it impossible to know whether all studies were in fact identified. An alternative approach is to review an **inception cohort** of studies. An inception cohort refers to a set of studies known to have been initiated, irrespective of their results (e.g. selecting studies only from trials registers) (Dwan et al 2013). This means there is a full accounting of which studies do and do not have results available.

There are various ways to assemble an inception cohort. Review authors could pre-specify that studies will be included only if they were registered prospectively (e.g. registered before patient enrolment in public, industry or regulatory registers (Roberts et al 2015, Jorgensen et al 2018), or in grants databases such as NIH RePORTER (Driessen et al 2015). Or, review authors may obtain unabridged access to reports of all studies of a product conducted by a particular manufacturer (Simmonds et al 2013). Alternatively, a clinical trial collaborative group may prospectively plan to undertake multiple trials using similar designs, participants, interventions and outcomes, and synthesize the findings of all trials once completed ('prospective meta-analysis'; see Chapter 22) (Askie et al 2018). The benefit of these strategies is that review authors can identify all eligible studies regardless of the P value, magnitude or direction of any result.

Limiting inclusion to prospectively registered studies avoids the possibility of missing any eligible _studies_. However, there is still the potential for missing results in these studies. Therefore, review authors would need to assess the availability of results for each study identified (guidance on how to do so is provided in Section 13.3.3). If none of the prospectively registered studies suffer from selective non-reporting or under-reporting of results, then none of the syntheses will be at risk of bias due to missing results. Conversely, if some results are missing selectively, then there may be a risk of bias in the synthesis, particularly if the total amount of data missing is large (for more details see Section 13.3.4).

#### 13.2.2 The \(\alpha\)-function

The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.1}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.2}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.3}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.4}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.5}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.6}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.7}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.8}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.9}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.10}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.11}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.12}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.13}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.14}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.15}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.16}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.17}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.18}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.19}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.20}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.21}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.22}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.23}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.24}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.25}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.26}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.27}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.28}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.29}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.30}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.31}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.32}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.33}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.34}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.35}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.36}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.37}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.38}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.39}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.40}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha_{i}) \tag{13.41}\]

where \(\alpha_{i}\) is the \(\alpha\)-function. The \(\alpha\)-function is defined as

\[\alpha(\alpha)=\sum_{i=1}^{n}\alpha_{i}(\alpha3. Record whether any of the studies identified are missing from each synthesis because results known (or presumed) to have been generated by study investigators are unavailable: the 'known unknowns' (Section 13.3.3).
4. Consider whether each synthesis is likely to be biased because of the missing results in the studies identified (Section 13.3.4).
5. Consider whether results from additional studies are likely to be missing from each synthesis: the 'unknown unknowns' (Section 13.3.5).
6. Reach an overall judgement about risk of bias due to missing results in each synthesis (Section 13.3.6).

The framework is designed to assess risk of bias in syntheses of _quantitative_ data about the effects of interventions, regardless of the type of synthesis (e.g. meta-analysis, or calculation of the median effect estimate across studies). The issue of non-reporting bias has received little attention in the context of qualitative research, so more work is needed to develop methods relevant to qualitative evidence syntheses (Toews et al 2017).

If review authors are unable to, or choose not to, generate a synthesized result (e.g. a meta-analytic effect estimate, or median effect across studies), then the complete framework cannot be applied. Nevertheless, review authors should not ignore any missing results when drawing conclusions in this situation (see Chapter 12). For example, the primary outcome in the Cochrane Review of latrepidine for Alzheimer's disease (Chau et al 2015) was clinical global impression of change, measured by CIBIC-Plus (Clinician's Interview-Based Impression of Change Plus Caregiver Input). This was assessed in four trials, but results were available for only one, and review authors suspected selective non-reporting of results in the other three. After describing the mean difference in CIBIC-Plus from the trial with results available, the review authors concluded that they were uncertain about the efficacy of latrepidine on clinical global impression of change, owing to the missing results from three trials.

#### Selecting syntheses to assess for risk of bias

It may not be feasible to assess risk of bias due to missing results in all syntheses in a review, particularly if many syntheses are conducted and many studies are eligible for inclusion in each. Review authors should therefore strive to assess risk of bias due to missing results in syntheses of outcomes that are most important to patients and health professionals. Such outcomes will typically be included in 'Summary of findings' tables (see Chapter 14). Ideally, review authors should pre-specify the syntheses for which they plan to assess the risk of bias due to missing results.

#### Defining eligible results for the synthesis

Review authors should consider what type of results are eligible for inclusion in each selected synthesis. Eligibility will depend on the specificity of the planned synthesis. For example, a highly specific approach may be to synthesize mean differences from trials measuring depression using a particular instrument (the Beck Depression Inventory (BDI)) at a particular time point (six weeks). A broader approach would be to synthesize mean differences from trials measuring depression using any instrument, at any time