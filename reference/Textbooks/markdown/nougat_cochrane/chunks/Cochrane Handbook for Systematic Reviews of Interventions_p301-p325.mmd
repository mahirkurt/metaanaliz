Joseph Lau, Keith O'Rourke, Gerta Rucker, Rob Scholten, Jonathan Sterne, Simon Thompson, Anne Whitehead

**Acknowledgements:** We are grateful to the following for commenting helpfully on earlier drafts: Bodil Als-Nielsen, Deborah Ashby, Jesse Berlin, Joseph Beyene, Jacqueline Birks, Michael Bracken, Marion Campbell, Chris Cates, Wendong Chen, Mike Clarke, Albert Cobos, Esther Coren, Francois Curtin, Roberto D'Amico, Keith Dear, Heather Dickinson, Diana Elbourne, Simon Gates, Paul Glasziou, Christian Gluud, Peter Herbison, Sally Hollis, David Jones, Steff Lewis, Tianjing Li, Joanne McKenzie, Philipp Middleton, Nathan Pace, Craig Ramsey, Keith O'Rourke, Rob Scholten, Guido Schwarzer, Jack Sinclair, Jonathan Sterne, Simon Thompson, Andy Vail, Clarine van Oel, Paula Williamson and Fred Wolf.

**Funding:** JJD received support from the National Institute for Health Research (NIHR) Birmingham Biomedical Research Centre at the University Hospitals Birmingham NHS Foundation Trust and the University of Birmingham. JPTH is a member of the NIHR Biomedical Research Centre at University Hospitals Bristol NHS Foundation Trust and the University of Bristol. JPTH received funding from National Institute for Health Research Senior Investigator award NF-SI-0617-10145. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health.

### References

Agresti A. An Introduction to Categorical Data Analysis. New York (NY): John Wiley & Sons; 1996.

Akl EA, Kahale LA, Agoritsas T, Brignardello-Petersen R, Busse JW, Carrasco-Labra A, Ebrahim S, Johnston BC, Neumann I, Sola I, Sun X, Vandvik P, Zhang Y, Alonso-Coello P, Guyatt G. Handling trial participants with missing outcome data when conducting a meta-analysis: a systematic survey of proposed approaches. Systematic Reviews2015; **4**: 98.

Akl EA, Kahale LA, Ebrahim S, Alonso-Coello P, Schunemann HJ, Guyatt GH. Three challenges described for identifying participants with missing data in trials reports, and potential solutions suggested to systematic reviewers. Journal of Clinical Epidemiology2016; **76**: 147-154.

Altman DG, Bland JM. Detecting skewness from summary information. BMJ1996; **313**: 1200.

Anzures-Cabrera J, Sarpatwari A, Higgins JPT. Expressing findings from meta-analyses of continuous outcomes in terms of risks. Statistics in Medicine2011; **30**: 2967-2985.

Berlin JA, Longnecker MP, Greenland S. Meta-analysis of epidemiologic dose-response data. Epidemiology1993; **4**: 218-228.

Berlin JA, Antman EM. Advantages and limitations of metaanalytic regressions of clinical trials data. Online Journal of Current Clinical Trials1994; **Doc No 134**.

Berlin JA, Santanna J, Schmid CH, Szczech LA, Feldman KA, Group A-LAITS. Individual patient-versus group-level data meta-regressions for the investigation of treatment effect modifiers: ecological bias rears its ugly head. Statistics in Medicine2002; **21**: 371-387.

Borenstein M, Hedges LV, Higgins JPT, Rothstein HR. A basic introduction to fixed-effect and random-effects models for meta-analysis. _Research Synthesis Methods_ 2010; **1**: 97-111.
* [33] Borenstein M, Higgins JPT. Meta-analysis and subgroups. _Prevention Science_ 2013; **14**: 134-143.
* [34] Bradburn MJ, Deeks JJ, Berlin JA, Russell Localio A. Much ado about nothing: a comparison of the performance of meta-analytical methods with rare events. _Statistics in Medicine_ 2007; **26**: 53-77.
* [35] Chinn S. A simple method for converting an odds ratio to effect size for use in meta-analysis. _Statistics in Medicine_ 2000; **19**: 3127-3131.
* [36] da Costa BR, Nuesch E, Rutjes AW, Johnston BC, Reichenbach S, Trelle S, Guyatt GH, Juni P. Combining follow-up and change data is valid in meta-analyses of continuous outcomes: a meta-epidemiological study. _Journal of Clinical Epidemiology_ 2013; **66**: 847-855.
* [37] Deeks JJ. Systematic reviews of published evidence: miracles or minefields? _Annals of Oncology_ 1998; **9**: 703-709.
* [38] Deeks JJ, Altman DG, Bradburn MJ. Statistical methods for examining heterogeneity and combining results from several studies in meta-analysis. In: Egger M, Davey Smith G, Altman DG, editors. _Systematic Reviews in Health Care: Meta-analysis in Context_. 2nd ed. London (UK): BMJ Publication Group; 2001. p. 285-312.
* [39] Deeks JJ. Issues in the selection of a summary statistic for meta-analysis of clinical trials with binary outcomes. _Statistics in Medicine_ 2002; **21**: 1575-1600.
* [40] DerSimonian R, Laird N. Meta-analysis in clinical trials. _Controlled Clinical Trials_ 1986; **7**: 177-188.
* [41] DiGuiseppi C, Higgins JPT. Interventions for promoting smoke alarm ownership and function. _Cochrane Database of Systematic Reviews_ 2001; **2**: CD002246.
* [42] Ebrahim S, Akl EA, Mustafa RA, Sun X, Walter SD, Heels-Ansdell D, Alonso-Coello P, Johnston BC, Guyatt GH. Addressing continuous data for participants excluded from trial analysis: a guide for systematic reviewers. _Journal of Clinical Epidemiology_ 2013; **66**: 1014-1021 e1011.
* [43] Ebrahim S, Johnston BC, Akl EA, Mustafa RA, Sun X, Walter SD, Heels-Ansdell D, Alonso-Coello P, Guyatt GH. Addressing continuous data measured with different instruments for participants excluded from trial analysis: a guide for systematic reviewers. _Journal of Clinical Epidemiology_ 2014; **67**: 560-570.
* [44] Efthimiou O. Practical guide to the meta-analysis of rare events. _Evidence-Based Mental Health_ 2018; **21**: 72-76.
* [45] Egger M, Davey Smith G, Schneider M, Minder C. Bias in meta-analysis detected by a simple, graphical test. _BMJ_ 1997; **315**: 629-634.
* [46] Engels EA, Schmid CH, Terrin N, Olkin I, Lau J. Heterogeneity and statistical significance in meta-analysis: an empirical study of 125 meta-analyses. _Statistics in Medicine_ 2000; **19**: 1707-1728.
* [47] Greenland S, Robins JM. Estimation of a common effect parameter from sparse follow-up data. _Biometrics_ 1985; **41**: 55-68.
* [48] Greenland S. Quantitative methods in the review of epidemiologic literature. _Epidemiologic Reviews_ 1987; **9**: 1-30.
* [49] Greenland S, Longnecker MP. Methods for trend estimation from summarized dose-response data, with applications to meta-analysis. _American Journal of Epidemiology_ 1992; **135**: 1301-1309.
* [50]Guevara JP, Berlin JA, Wolf FM. Meta-analytic methods for pooling rates when follow-up duration varies: a case study. _BMC Medical Research Methodology_ 2004; **4**: 17.
* Hartung J, Knapp G. A refined method for the meta-analysis of controlled clinical trials with binary outcome. _Statistics in Medicine_ 2001; **20**: 3875-3889.
* Hasselblad V, McCrory DC. Meta-analytic tools for medical decision making: a practical guide. _Medical Decision Making_ 1995; **15**: 81-96.
* Higgins JPT, Thompson SG. Quantifying heterogeneity in a meta-analysis. _Statistics in Medicine_ 2002; **21**: 1539-1558.
* Higgins JPT, Thompson SG, Deeks JJ, Altman DG. Measuring inconsistency in meta-analyses. _BMJ_ 2003; **327**: 557-560.
* Higgins JPT, Thompson SG. Controlling the risk of spurious findings from meta-regression. _Statistics in Medicine_ 2004;**23**: 1663-1682.
* Higgins JPT, White IR, Wood AM. Imputation methods for missing outcome data in meta-analysis of clinical trials. _Clinical Trials_ 2008a; **5**: 225-239.
* Higgins JPT, White IR, Anzures-Cabrera J. Meta-analysis of skewed data: combining results reported on log-transformed or raw scales. _Statistics in Medicine_ 2008b; **27**: 6072-6092.
* Higgins JPT, Thompson SG, Spiegelhalter DJ. A re-evaluation of random-effects meta-analysis. _Journal of the Royal Statistical Society: Series A (Statistics in Society)_ 2009; **172**: 137-159.
* Kjaergard LL, Villumsen J, Gluud C. Reported methodologic quality and discrepancies between large and small randomized trials in meta-analyses. _Annals of Internal Medicine_ 2001; **135**: 982-989.
* Langan D, Higgins JPT, Simmonds M. An empirical comparison of heterogeneity variance estimators in 12 894 meta-analyses. _Research Synthesis Methods_ 2015; **6**: 195-205.
* Langan D, Higgins JPT, Simmonds M. Comparative performance of heterogeneity variance estimators in meta-analysis: a review of simulation studies. _Research Synthesis Methods_ 2017; **8**: 181-198.
* Langan D, Higgins JPT, Jackson D, Bowden J, Veroniki AA, Kontopantelis E, Viechtbauer W, Simmonds M. A comparison of heterogeneity variance estimators in simulated random-effects meta-analyses. _Research Synthesis Methods_ 2019; **10**: 83-98.
* Lewis S, Clarke M. Forest plots: trying to see the wood and the trees. _BMJ_ 2001; **322**: 1479-1480.
* A Bayesian modelling framework: concepts, structure, and extensibility. _Statistics and Computing_ 2000; **10**: 325-337.
* Mantel N, Haenszel W. Statistical aspects of the analysis of data from retrospective studies of disease. _Journal of the National Cancer Institute_ 1959; **22**: 719-748.
* McIntosh MW. The population risk as an explanatory variable in research synthesis of clinical trials. _Statistics in Medicine_ 1996; **15**: 1713-1728.
* Morgenstern H. Uses of ecologic analysis in epidemiologic research. _American Journal of Public Health_ 1982; **72**: 1336-1344.
* Oxman AD, Guyatt GH. A consumers guide to subgroup analyses. _Annals of Internal Medicine_ 1992; **116**: 78-84.
* Peto R, Collins R, Gray R. Large-scale randomized evidence: large, simple trials and overviews of trials. _Journal of Clinical Epidemiology_ 1995; **48**: 23-40.
* Poole C, Greenland S. Random-effects meta-analyses are not always conservative. _American Journal of Epidemiology_ 1999; **150**: 469-475.
* P * Rhodes et al. (2016) Rhodes KM, Turner RM, White IR, Jackson D, Spiegelhalter DJ, Higgins JPT. Implementing informative priors for heterogeneity in meta-analysis using meta-regression and pseudo data. _Statistics in Medicine_ 2016; **35**: 5495-5511.
* Rice et al. (2018) Rice K, Higgins JPT, Lumley T. A re-evaluation of fixed effect(s) meta-analysis. _Journal of the Royal Statistical Society Series A (Statistics in Society)_ 2018; **181**: 205-227.
* Riley and Higgins (2011) Riley RD, Higgins JPT, Deeks JJ. Interpretation of random effects meta-analyses. _BMJ_ 2011; **342**: d549.
* Rover (2017) Rover C. Bayesian random-effects meta-analysis using the bayesmeta R package 2017. [https://arxiv.org/abs/1711.08683](https://arxiv.org/abs/1711.08683).
* Rucker et al. (2009) Rucker G, Schwarzer G, Carpenter J, Olkin I. Why add anything to nothing? The arcsine difference as a measure of treatment effect in meta-analysis with zero cells. _Statistics in Medicine_ 2009; **28**: 721-738.
* Sharp (2001) Sharp SJ. Analysing the relationship between treatment benefit and underlying risk: precautions and practical recommendations. In: Egger M, Davey Smith G, Altman DG, editors. _Systematic Reviews in Health Care: Meta-analysis in Context._ 2nd ed. London (UK): BMJ Publication Group; 2001: 176-188.
* Sidik and Jonkman (2002) Sidik K, Jonkman JN. A simple confidence interval for meta-analysis. _Statistics in Medicine_ 2002; **21**: 3153-3159.
* Simmonds et al. (2011) Simmonds MC, Tierney J, Bowden J, Higgins JPT. Meta-analysis of time-to-event data: a comparison of two-stage methods. _Research Synthesis Methods_ 2011; **2**: 139-149.
* Sinclair and Bracken (1994) Sinclair JC, Bracken MB. Clinically useful measures of effect in binary analyses of randomized trials. _Journal of Clinical Epidemiology_ 1994; **47**: 881-889.
* Smith et al. (1995) Smith TC, Spiegelhalter DJ, Thomas A. Bayesian approaches to random-effects meta-analysis: a comparative study. _Statistics in Medicine_ 1995; **14**: 2685-2699.
* Spiegelhalter et al. (2004) Spiegelhalter DJ, Abrams KR, Myles JP. _Bayesian Approaches to Clinical Trials and Health-Care Evaluation._ Chichester (UK): John Wiley & Sons; 2004.
* Spittal et al. (2015) Spittal MJ, Pirkis J, Gurrin LC. Meta-analysis of incidence rate data in the presence of zero events. _BMC Medical Research Methodology_ 2015; **15**: 42.
* Sutton et al. (2000) Sutton AJ, Abrams KR, Jones DR, Sheldon TA, Song F. _Methods for Meta-analysis in Medical Research._ Chichester (UK): John Wiley & Sons; 2000.
* Sutton and Abrams (2001) Sutton AJ, Abrams KR. Bayesian methods in meta-analysis and evidence synthesis. _Statistical Methods in Medical Research_ 2001; **10**: 277-303.
* Sweeting et al. (2004) Sweeting MJ, Sutton AJ, Lambert PC. What to add to nothing? Use and avoidance of continuity corrections in meta-analysis of sparse data. _Statistics in Medicine_ 2004; **23**: 1351-1375.
* Thompson et al. (1997) Thompson SG, Smith TC, Sharp SJ. Investigating underlying risk as a source of heterogeneity in meta-analysis. _Statistics in Medicine_ 1997; **16**: 2741-2758.
* Thompson and Sharp (1999) Thompson SG, Sharp SJ. Explaining heterogeneity in meta-analysis: a comparison of methods. _Statistics in Medicine_ 1999; **18**: 2693-2708.
* Thompson and Higgins (2002) Thompson SG, Higgins JPT. How should meta-regression analyses be undertaken and interpreted? _Statistics in Medicine_ 2002; **21**: 1559-1574.
* Turner et al. (2012) Turner RM, Davey J, Clarke MJ, Thompson SG, Higgins JPT. Predicting the extent of heterogeneity in meta-analysis, using empirical data from the Cochrane Database of Systematic Reviews. _International Journal of Epidemiology_ 2012; **41**: 818-827.
* Veroniki et al. (2016) Veroniki AA, Jackson D, Viechtbauer W, Bender R, Bowden J, Knapp G, Kuss O, Higgins JPT, Langan D, Salanti G. Methods to estimate the between-study variance and its uncertainty in meta-analysis. _Research Synthesis Methods_ 2016; **7**: 55-79.
* Zucker et al. (2016)Whitehead A, Jones NMB. A meta-analysis of clinical trials involving different classifications of response into ordered categories. _Statistics in Medicine_ 1994; **13**: 2503-2515.
* [33] Yusuf S, Peto R, Lewis J, Collins R, Sleight P. Beta blockade during and after myocardial infarction: an overview of the randomized trials. _Progress in Cardiovascular Diseases_ 1985; **27**: 335-371.
* [34] Yusuf S, Wittes J, Probstfield J, Tyroler HA. Analysis and interpretation of treatment effects in subgroups of patients in randomized clinical trials. _JAMA_ 1991; **266**: 93-98.

## Chapter 11 Understanding network meta-analyses

### 11.1 What is network meta-analysis?

Most Cochrane Reviews present comparisons between pairs of interventions (an experimental intervention and a comparator intervention) for a specific condition and in a specific population or setting. However, it is usually the case that several, perhaps even numerous, competing interventions are available for any given condition. People who need to decide between alternative interventions would benefitfrom a single review that includes all relevant interventions, and presents their comparative effectiveness and potential for harm. Network meta-analysis provides an analysis option for such a review.

Any set of studies that links three or more interventions via direct comparisons forms a **network of interventions**. In a network of interventions there can be multiple ways to make **indirect comparisons** between the interventions. These are comparisons that have not been made directly within studies, and they can be estimated using mathematical combinations of the direct intervention effect estimates available. **Network meta-analysis** combines direct and indirect estimates across a network of interventions in a single analysis. Synonymous terms, less often used, are mixed treatment comparisons and multiple treatments meta-analysis.

##### Network diagrams

A **network diagram** is a graphical depiction of the structure of a network of interventions (Chairani et al 2013). It consists of nodes representing the interventions in the network and lines showing the available direct comparisons between pairs of interventions. An example of a network diagram with four interventions is given in Figure 11.1.a. In this example, distinct lines forming a closed triangular loop have been added to illustrate the presence of a three-arm study. Note that for large and complex networks, such presentation of multi-arm studies may give complicated and unhelpful network diagrams; in this case it might be preferable to show multi-arm studies in a tabular format. Further discussion of displaying networks is available in Section 11.6.1.

Figure 11.1.a: Example of network diagram with four competing interventions and information on the presence of multi-arm randomized trials

#### Advantages of network meta-analysis

A network meta-analysis exploits all available direct and indirect evidence. Empirical studies have suggested it yields more precise estimates of the intervention effects in comparison with a single direct or indirect estimate (Cooper et al 2011, Caldwell et al 2015). In addition, network meta-analysis can provide information for comparisons between pairs of interventions that have never been evaluated within individual randomized trials. The simultaneous comparison of all interventions of interest in the same analysis enables the estimation of their relative ranking for a given outcome (see Section 11.4.3.3 for more discussion of ranking).

##### Outline of this chapter

This chapter provides an overview of the concepts, assumptions and methods that relate to network meta-analyses and to the indirect intervention comparisons on which they are built. Section 11.2 first describes what an indirect comparison is and how it can be made in a simple trio of interventions. It then introduces the notion of transitivity (and its statistical analogue, coherence) as the core assumption underlying the validity of an indirect comparison. Examples are provided where this assumption is likely to hold or be violated.

Section 11.3 provides guidance on the design of a Cochrane Review with multiple interventions and the appropriate definition of the research question with respect to selecting studies, outcomes and interventions. Section 11.4 briefly describes the available statistical methods for synthesizing the data, estimating the relative ranking and assessing coherence in a network of interventions. Finally, Sections 11.5 and 11.6 provide approaches for evaluating confidence in the evidence and presenting the evidence base and the results from a network meta-analysis. Note that the chapter only introduces the statistical aspects of network meta-analysis; authors will need a knowledge-able statistician to plan and execute these methods.

### Important concepts

At the heart of network meta-analysis methodology is the concept of an **indirect comparison**. Indirect comparisons are necessary to estimate the relative effect of two interventions when no studies have compared them directly.

##### Indirect comparisons

Indirect comparisons allow us to estimate the relative effects of two interventions that have not been compared directly within a trial. For example, suppose there are randomized trials directly comparing provision of dietary advice by a dietitian (which we refer to as intervention A) with advice given by a doctor (intervention B). Suppose there are also randomized trials comparing dietary advice given by a dietitian (intervention A) with advice given by a nurse (intervention C). Suppose further that these randomized trials have been combined in standard, pair-wise meta-analyses separately to derive **direct estimates** of intervention effects for _A versus B_ (sometimes depicted 'AB')and _A versus C_ ('AC'), measured as mean difference (MD) in weight reduction (see Chapter 6, Section 6.5.1.1). The situation is illustrated in Figure 11.2.a, where the solid straight lines depict available evidence. We wish to learn about the relative effect of advice by a doctor versus a nurse (_B versus C_); the dashed line depicts this comparison, for which there is no direct evidence.

One way to understand an indirect comparison is to think of the BC comparison (of _B versus C_) as representing the benefit of B over C. All else being equal, the benefit of B over C is equivalent to the benefit of B over A plus the benefit of A over C. Thus, for example, the indirect comparison describing benefit of 'doctor' over 'nurse' may be thought of as the benefit of 'doctor' over 'dietitian' plus the benefit of 'dietitian' over 'nurse' (these 'benefits' may be positive or negative; we do not intend to imply any particular superiority among these three types of people offering dietary advice). This is represented graphically in Figure 11.2.b.

Mathematically, the sum can be written:

\[\text{indirect}\,\text{MD}(\text{BvsC})=\text{direct}\,\text{MD}(\text{BvsA}) +\text{direct}\,\text{MD}(\text{AvsC}).\]

We usually write this in the form of subtraction:

\[\text{indirect}\,\text{MD}(\text{BvsC})=\text{direct}\,\text{MD}(\text{AvsC} )-\text{direct}\,\text{MD}(\text{AvsB}),\]

Figure 11.2.a: Illustration of an indirect estimate that compares the effectiveness of ‘doctor’ (B) and ‘nurse’ (C) in providing dietary advice through a common comparator ‘dietitian’ (A)

such that the difference between the summary statistics of the intervention effect in the direct _A versus C_ and _A versus B_ meta-analyses provides an indirect estimate of the _B versus C_ intervention effect.

For this simple case where we have two direct comparisons (three interventions) the analysis can be conducted by performing subgroup analyses using standard meta-analysis routines (including RevMan): studies addressing the two direct comparisons (i.e. _A versus B_ and _A versus C_) can be treated as two subgroups in the meta-analysis. The difference between the summary effects from the two subgroups gives an estimate for the indirect comparison.

Most software will provide a P value for the statistical significance of the difference between the subgroups based on the estimated variance of the indirect effect estimate (Bucher et al 1997):

\[\text{Variance}[\text{indirect MD}(\text{BvsC})]=\text{Variance}[\text{direct MD}(\text{AvsC})]+\text{Variance}[\text{direct MD}(\text{AvsB})],\]

where variance[direct MD(AvsC)] and variance[direct MD(AvsB)] are the variances of the respective direct estimates (from the two subgroup analyses).

A 95% confidence interval for the indirect summary effect is constructed by the formula:

\[\Big{[}\text{indirect MD}(\text{BvsC})\pm 1.96\times\sqrt{\text{ Variance}[\text{indirect MD}(\text{BvsC})]}\Big{]}.\]

This method uses the intervention effects from each group of randomized trials and therefore preserves within-trial randomization. If we had instead pooled single arms across the studies (e.g. all B arms and all C arms, ignoring the A arms) and then performed a direct comparison between the pooled B and C arms (i.e. treating the data as if they came from a single large randomized trial), then our analysis would discard the benefits of within-trial randomization (Li and Dickersin 2013). This approach should not be used.

When four or more competing interventions are available, indirect estimates can be derived via multiple routes. The only requirement is that two interventions are 'connected' and not necessarily via a single common comparator. An example of this situation is provided in Figure 11.2.c. Here 'doctor' (B) and 'pharmacist' (D) do not have a common comparator, but we can compare them indirectly via the route 'doctor' (B) - 'dietitian' (A) - 'nurse' (C) - 'pharmacist (D) by an extension of the arguments set out earlier.

#### 11.2.2 Transitivity

##### Validity of an indirect comparison

The underlying assumption of indirect comparisons is that we can learn about the true relative effect of B versus C via treatment A by combining the true relative effects _A versus B_ and _A versus C_. This relationship can be written mathematically as

\[\text{effect of B versus C}=(\text{effect of Aversus C})-(\text{effect of Aversus B}).\]

In words, this means that we can compare interventions B and C via intervention A (Figure 11.2.a).

Indirect comparisons provide observational evidence across randomized trials and may suffer the biases of observational studies, such as confounding (see Chapter 10, Sections 10.11.5 and 10.11.6). The validity of an indirect comparison requires that the different sets of randomized trials are similar, on average, in all important factors other than the intervention comparison being made (Song et al 2003, Glenny et al 2005, Donegan et al 2010, Salanti 2012). We use the term **transitivity** to refer to this requirement. It is closely related to the statistical notion of coherence (see Section 11.2.3.2); the distinction is a little like that between diversity and (statistical) heterogeneity in pair-wise meta-analysis (see Chapter 10, Section 10.10.1).

Studies that compare different interventions may differ in a wide range of characteristics. Sometimes these characteristics are associated with the effect of an intervention. We refer to such characteristics as effect modifiers; they are the aspects of diversity that induce heterogeneity in pairwise meta-analyses. If the _A versus B_ and _A versus C_ randomized trials differ with respect to their effect modifiers, then it would not be appropriate to make an indirect comparison.

Transitivity requires that intervention A is similar when it appears in _A versus B_ studies and _A versus C_ studies with respect to characteristics (effect modifiers) that may affect the two relative effects (Salanti et al 2009). For example, in the dietary advice network the common comparator 'dietitian' might differ with respect to the frequency of advice sessions between trials that compare dietitian with doctor (_A versus B_) and trials that compare dietitian with nurse (_A versus C_). If the participants visit the dietitian once a week in AB studies and once a month in AC studies, transitivity may be violated. Similarly, any other effect modifiers should not differ between AB and AC studies.

Transitivity requires all competing interventions of a systematic review to be **jointly randomizable**. That is, we can imagine all interventions being compared simultaneously in a single multi-arm randomized trial. Another way of viewing this is that, in any

Figure 11.2.c: Example of deriving indirect estimate that compares the effectiveness of ‘doctor’ (B) and ‘pharmacist’ (D) in providing dietary advice through a connected loop

particular trial, the'missing' interventions (those not included in trial) may be considered to be missing for reasons unrelated to their effects (Caldwell et al 2005, Salanti 2012).

##### Assessing transitivity

Clinical and methodological differences are inevitable between studies in a systematic review. Researchers undertaking indirect comparisons should assess whether such differences are sufficiently large to induce **intransitivity**. In principle, transitivity can be evaluated by comparing the distribution of effect modifiers across the different comparisons (Salanti 2012, Cipriani et al 2013, Jansen and Naci 2013). Imbalanced distributions would threaten the plausibility of the transitivity assumption and thus the validity of indirect comparison. In practice, however, this requires that the effect modifiers are known and have been measured. There are also some statistical options for assessing whether the transitive relationship holds in some circumstances, which we discuss in Section 11.4.4.

Extended guidance on considerations of potential effect modifiers is provided in discussions of heterogeneity in Chapter 10 (Section 10.11). For example, we may believe that age is a potential effect modifier so that the effect of an intervention differs between younger and older populations. If the average age in _A versus B_ randomized trials is substantially older or younger than in _A versus C_ randomized trials, transitivity may be implausible, and an indirect comparison _B versus C_ may be invalid.

Figure 11.2.d shows hypothetical examples of valid and invalid indirect comparisons for the dietary advice example. Suppose a single effect modifier is severity of disease (e.g. obesity measured by the BMI score). The top row depicts a situation in which all patients in all trials have moderate severity. There are AB studies and AC studies in this population. Estimation of BC is valid here because there is no difference in the effect modifier. The second row depicts a similar situation in a second population of patients who all have severe disease. A valid indirect estimate of _B versus C_ for this population can also be made. In the third row we depict a situation in which all AB trials are conducted only in moderately obese populations and all AC trials are conducted only in severely obese populations. In this situation, the distribution of effect modifiers is different in the two direct comparisons, so the indirect effect based on this row is invalid (due to intransitivity).

In practice, differences in effect modifiers are usually less extreme than this hypothetical scenario; for example, AB randomized trials may have 80% moderately obese population and 20% severely obese, and AC randomized trials may have 20% moderately obese and 80% severely obese population. Intransitivity would probably still invalidate the indirect estimate _B versus C_ if severity is an important effect modifier.

##### Indirect comparisons and the validity of network meta-analysis

##### Combining direct and indirect evidence

Often there is direct evidence for a specific comparison of interventions as well as a possibility of making an indirect comparison of the interventions via one or more common comparators. If the key assumption of transitivity is considered reasonable, direct and indirect estimates should be considered jointly. When both direct and indirect intervention effects are available for a particular comparison, these can be synthesized into a single effect estimate. This summary effect is sometimes called a **combined** or Figure 11.2.d: Example of valid and invalid indirect comparisons when the severity of disease acts as effect modifier and its distribution differs between the two direct comparisons. The shaded boxes represent the treatment effect estimates from each source of evidence (striped box for _A versus B_ and checked box for _A versus C_). In the first row, randomized trials of _A versus B_ and of _A versus C_ are all conducted in moderately obese populations; in the second row randomized trials are all conducted in severely obese populations. In both of these the indirect comparisons of the treatment effect estimates would be valid. In the last row, the _A versus B_ and _A versus C_ randomized trials are conducted in different populations. As severity is an effect modifier, the indirect comparison based on these would not be valid (Jansen et al 2014). Reproduced with permission of Elsevier

mixed** estimate of the intervention effect. We will use the former term in this chapter. A combined estimate can be computed as an inverse variance weighted average (see Chapter 10, Section 10.3) of the direct and indirect summary estimates.

Since combined estimates incorporate indirect comparisons, they rely on the transitivity assumption. Violation of transitivity threatens the validity of both indirect and combined estimates. Of course, biased direct intervention effects for any of the comparisons also challenge the validity of a combined effect (Madan et al 2011).

##### Coherence (or consistency)

The key assumption of transitivity relates to potential clinical and methodological variation across the different comparisons. These differences may be reflected in the data in the form of disagreement in estimates between different sources of evidence. The statistical manifestation of transitivity and is typically called either **coherence** or **consistency**. We will use the former to distinguish the notion from inconsistency (or heterogeneity) within standard meta-analyses (e.g. as is measured using the I\({}^{2}\) statistic; see Chapter 10, Section 10.10.2). Coherence implies that the different sources of evidence (direct and indirect) agree with each other.

The coherence assumption is expressed mathematically by the **coherence equations**, which state that the true direct and indirect intervention effects for a specific comparison are identical:

\[\text{`true'MD}(\text{BvsC})=\text{`true'MD}(\text{AvsC})-\text{`true'MD}( \text{AvsB}).\]

Some methods for testing this assumption are presented in Section 11.4.4.

##### Validity of network meta-analysis

The validity of network meta-analysis relies on the fulfilment of underlying assumptions. Transitivity should hold for every possible indirect comparison, and coherence should hold in every loop of evidence within the network (see Section 11.4.4). Considerations about heterogeneity within each direct comparison in the network should follow the existing recommendations for standard pair-wise meta-analysis (see Chapter 10, Section 10.10).

### Planning a Cochrane Review to compare multiple interventions

##### Expertise required in the review team

Because of the complexity of network meta-analysis, it is important to establish a multidisciplinary review team that includes a statistician skilled in network meta-analysis methodology early and throughout. Close collaboration between the statistician and the content area expert is essential to ensure that the studies selected for a network meta-analysis are similar except for the interventions being compared (see Section 11.2.2). Because basic meta-analysis software such as RevMan does not support network meta-analysis, the statistician will have to rely on statistical software packages such as Stata, R, WinBUGS or OpenBUGS for analysis.

#### The importance of a well-defined research question

Defining the research question of a systematic review that intends to compare multiple interventions should follow the general guidelines described in Chapter 2 and should be stated in the objectives of the review. In this section, we summarize and highlight key issues that are pertinent to systematic review with a network meta-analysis.

Because network meta-analysis could be used to estimate the relative ranking of the included interventions (Salanti et al 2011, Chaimani et al 2013), reviews that aim to rank the competing interventions should specify this in their objectives (Chaimani et al 2017). Review authors should consider obtaining an estimate of relative ranking as a secondary objective to supplement the relative effects. An extended discussion on the relative ranking of interventions is provided in Section 11.4.3.3.

##### Defining the population and choosing the interventions

Populations and interventions often need to be considered together given the potential for intransitivity (see Section 11.2.2). A driving principle is that any eligible participant should be eligible for randomization to any included intervention (Salanti 2012, Jansen and Naci 2013). Review authors should select their target population with this consideration in mind. Particular care is needed in the definition of the eligible interventions, as discussed in Chaimani and colleagues (Chaimani et al 2017). For example, suppose a systematic review aims to compare four chemotherapy regimens for a specific cancer. Regimen (D) is appropriate for stage II patients exclusively and regimen (A) is appropriate for both stage I and stage II patients. The remaining two regimens (B) and (C) are appropriate for stage I patients exclusively. Now suppose A and D were compared in stage II patients, and A, B and C were compared in stage I patients (see Figure 11.3.a). The four interventions forming the network are unlikely to satisfy the transitivity assumption because regimen D is not given to the same patient population as regimens B and C. Thus, a four-arm randomized trial comparing all interventions (A, B, C and D) simultaneously is not a reasonable study to conduct.

Figure 11.3.a: Example of a network comparing four chemotherapy regimens, where transitivity is violated due to incomparability between the interventions

#### Decision sets and supplementary sets of interventions

Usually there is a specific set of interventions of direct interest when planning a network meta-analysis, and these are sometimes referred to as the **decision set**. These are the options among which patients and health professionals would be choosing in practice with respect to the outcomes under investigation. In selecting which competing interventions to include in the decision set, review authors should ensure that the transitivity assumption is likely to hold (see also Section 11.2.2) (Salanti 2012).

The ability of network meta-analysis to incorporate indirect evidence means that inclusion of interventions that are not of direct interest to the review authors might provide additional information in the network. For example, placebo is often included in network meta-analysis even though it is not a reasonable treatment option, because many studies have compared active interventions against placebo. In such cases, excluding placebo would result in ignoring a considerable amount of indirect evidence. Similar considerations apply to historical or legacy interventions.

We use the term **supplementary set** to refer to interventions, such as placebo, that are included in the network meta-analysis for the purpose of improving inference among interventions in the decision set. The full set of interventions, the decision set plus the supplementary set, has been called in the literature the **synthesis comparator set**(Ades et al 2013, Caldwell et al 2015).

When review authors decide to include a supplementary set of interventions in a network, they need to be cautious regarding the plausibility of the transitivity assumption. In general, broadening the network challenges the transitivity assumption. Thus, supplementary interventions should be added when their value outweighs the risk of violating the transitivity assumption. The addition of supplementary interventions in the analysis might be considered more valuable for sparse networks that include only a few trials per comparison. In these networks the benefit of improving the precision of estimates by incorporating supplementary indirect evidence may be quite important. There is limited empirical evidence to inform the decision of how far one should go in constructing the network evidence base (Konig et al 2013, Caldwell et al 2015). inevitably it will require some judgement, and the robustness of decisions can be evaluated in sensitivity analyses and discussed in the review.

##### Grouping variants of an intervention (defining nodes in the network diagram)

The definition of nodes needs careful consideration in situations where variants of one or more interventions are expected to appear in the eligible trials (James et al 2018). The appropriateness of merging, for example, different doses of the same drug or different drugs within a class depends to a large extent on the research question. Lumping and splitting the variants of the competing interventions might be interesting to both review authors and evidence users; in such a case this should be stated clearly in the objectives of the review and the potential for intransitivity should be evaluated in every network. A decision on how the nodes of an expanded network could be merged is not always straightforward and researchers should act based on predefined criteria where possible. These criteria should be formed in such a way that maximizes similarity of the interventions within a node and minimizes similarity across nodes.

The following example refers to a network that used two criteria to classify electronic interventions for smoking cessation into five categories: "To be able to draw generalizable conclusions on the different types of electronic interventions, we developed a categorization system that brought similar interventions together in a limited number of categories. We sought advice from experts in smoking cessation on the key dimensions that would influence the effectiveness of smoking cessation programmes. Through this process, two dimensions for evaluating interventions were identified. The first dimension was related to whether the intervention offered generic advice or tailored its feedback to information provided by the user in some way. The second dimension related to whether the intervention used a single channel or multiple channels. From these dimensions, we developed a system with five categories..., ranging from interventions that provide generic information through a single channel, e.g. a static Web site or mass e-mail (category e1) to complex interventions with multiple channels delivering tailored information, e.g. an interactive Web site plus an interactive forum (category e5)" (Madan et al 2014).

Empirical evidence is currently lacking on whether more or less expanded networks are more prone to important intransitivity or incoherence. Extended discussions of how different dosages can be modelled in network meta-analysis are available (Giovane et al 2013, Owen et al 2015, Mawdsley et al 2016).

##### 11.3.2.4 Defining eligible comparisons of interventions (defining lines in the network diagram)

Once the nodes of the network have been specified, every study that meets the eligibility criteria and compares any pair of the eligible interventions should be included in the review. The exclusion of specific direct comparisons without a rationale may introduce bias in the analysis and should be avoided.

##### 11.3.3 Selecting outcomes to examine

In the context of a network meta-analysis, outcomes should be specified a priori regardless of the number of interventions the review intends to compare or the number of studies the review is able to include. Review authors should be aware that some characteristics may be effect modifiers for some outcomes but not for other outcomes. This implies that sometimes the potential for intransitivity should be examined separately for each outcome before undertaking the analyses.

##### 11.3.4 Study designs to include

Randomized designs are generally preferable to non-randomized designs to ensure an increased level of validity of the summary estimates (see Chapter 3, Section 3.3). Sometimes observational data from non-randomized studies may form a useful source of evidence (see Chapter 24). In general, combining randomized with observational studies in a network meta-analysis is not recommended. In the case of sparse networks (i.e. networks with a few studies but many interventions), observational data might be used to supplement the analysis; for example, to form prior knowledge or provide information on baseline characteristics (Schmitz et al 2013, Soares et al 2014).

### 11.4 Synthesis of results

#### What does a network meta-analysis estimate?

In a connected network, the coherence equations provide mathematical links between the intervention effects, so that some effects can be computed from others using transitivity assumptions. This means that not all pair-wise comparisons are independently estimated. In fact, the number of comparisons that need to be estimated in a network meta-analysis equals the number of interventions minus one. In practice, we select a particular set of comparisons of this size, and we often label these the **basic comparisons** for the analysis (Lu and Ades 2006). For example, in the network of four interventions for heavy menstrual bleeding illustrated in Figure 11.4.a we might choose the following three basic comparisons: 'Hysterectomy versus first generation hystereoscopic techniques', 'Mirena versus first generation hystereoscopic techniques' and'second generation non-hystereoscopic techniques versus first generation hystereoscopic techniques'. All other comparisons in the network (e.g. 'Mirena versus hysterectomy', 'Mirena versus second generation non-hystereoscopic techniques', etc.) can be computed from the three basic comparisons.

The main result of a network meta-analysis is a set of **network estimates** of the intervention effects for all basic comparisons. We obtain estimates for the other comparisons after the analysis using the coherence equations (see Section 11.2.3.2). It does not matter which set of comparisons we select as the basic comparisons. Often we would identify one intervention as a reference, and define the basic comparisons as the effect of each of the other interventions against this reference.

Figure 11.4.a Network graph of four interventions for heavy menstrual bleeding (Middleton et al 2010). The size of the nodes is proportional to the number of participants assigned to the intervention and the thickness of the lines is proportional to the number of randomized trials that studied the respective direct comparison. Reproduced with permission of BMJ Publishing Group

#### 11.4.2 Synthesizing direct and indirect evidence using meta-regression

Network meta-analysis can be performed using several approaches (Salanti et al 2008). The main technical requirement for all approaches is that all interventions included in the analysis form a 'connected' network. A straightforward approach that be used for many networks is to use meta-regression (see Chapter 10, Section 10.11.4). This approach works as long as there are no multi-arm trials in the network (otherwise, other methods are more appropriate).

We introduced indirect comparisons in Section 11.2.1 in the context of subgroup analysis, where the subgroups are defined by the comparisons. Differences between subgroups of studies can also be investigated via meta-regression. When standard meta-regression is used to conduct a single indirect comparison, a single dummy variable is used to specify whether the result of each study relates to one direct comparison or the other (a dummy variable is coded as 1 or 0 to indicate which comparison is made in the study). For example, in the dietary advice network containing only three intervention nodes (see Section 11.2.1, Figure 11.2.a) the dummy variable might be used to indicate the comparison 'dietitian versus nurse'. This variable takes the value 1 for a study that involves that corresponding comparison and 0 if it involves the comparison 'dietitian versus doctor', and is included as a single covariate in the meta-regression. In this way, the meta-regression model would have an intercept and a regression coefficient (slope). The estimated intercept gives the meta-analytic direct summary estimate for the comparison 'dietitian versus doctor' while the sum of the estimated regression coefficient and intercept gives the direct summary estimate for 'dietitian versus nurse'. Consequently, the estimated coefficient is the indirect summary estimate for the comparison 'doctor versus nurse'.

An alternative way to perform the same analysis of an indirect comparison is to re-parameterize the meta-regression model by using two dummy variables and no intercept, instead of one dummy variable and an intercept. The first dummy variable would indicate the comparison 'dietitian versus doctor', and the second the comparison 'dietitian versus nurse'. The estimated regression coefficients then give the summary estimates for these two comparisons, and it is convenient to consider these as the two basic comparisons for this analysis. The difference between the two regression coefficients is the summary estimate for the indirect comparison 'doctor versus nurse'.

The coding of each basic comparison using a dummy variable, and the omission of the intercept, proves to be a useful approach for implementing network meta-analysis using meta-regression, and helps explain the role of the coherence equations. Specifically, suppose now that in the dietary advice example, studies that directly compare 'doctor versus nurse' are also available. Because we are already estimating all of the basic comparisons required for three interventions, we do not require a third dummy variable (under coherence, the comparison 'doctor versus nurse' can be expressed as the difference between the other two comparisons: see Section 11.2.3.2). This means that studies comparing 'doctor versus nurse' inform us about the difference between the two comparisons already in the analysis. Consequently, we need to assign values -1 and 1 to the dummies 'dietitian versus doctor' and 'dietitian versus nurse', respectively. The meta-regression is again fitted including both dummy variables without an intercept. The interpretations of the estimated regression coefficients are the same as for the indirect comparison.

#### Performing network meta-analysis

We now consider approaches designed specifically for network meta-analysis that can be used when we have multi-arm trials. An overview of methodological developments can be found in Efthimiou and colleagues (Efthimiou et al 2016).

A popular approach to conducting network meta-analysis is using hierarchical models, commonly implemented within a Bayesian framework (Sobieraj et al 2013, Petropoulou et al 2016). Detailed descriptions of hierarchical models for network meta-analysis can be found elsewhere (Lu and Ades 2004, Salanti et al 2008, Dias et al 2018). Software options for a Bayesian approach include WinBUGS and OpenBUGS.

Multivariate meta-analysis methods, initially developed to synthesize multiple outcomes jointly (Jackson et al 2011, Mavridis and Salanti 2013), offer an alternative approach to conducting network meta-analysis. A multivariate meta-analysis approach focuses the analysis on the set of basic comparisons (e.g. each intervention against a common reference intervention) and treats these as analogous to different outcomes. A study can report on one or more of the basic comparisons; for example, there are two comparisons in a three-arm randomized trial. For studies that do not target any of the basic comparisons (e.g. a study that does not include the common reference intervention), a technique known as data augmentation can be used to allow the appropriate parameterization (White et al 2012). The method is implemented in the **network** macro available for Stata (White 2015). A detailed description of the concepts and the implementation of this approach is available (White et al 2012).

Methodology from electrical networks and graphic theory also can be used to fit network meta-analysis and is outlined in by Rucker (Rucker 2012). This approach has been implemented in the R package **netmeta**(Rucker and Schwarzer 2013).

##### Illustrating example

To illustrate the advantages of network meta-analysis, Figure 11.4.a presents a network of four interventions for heavy menstrual bleeding (Middleton et al 2010). Data are available for four out of six possible direct comparisons. Table 11.4.a presents the results from direct (pair-wise) meta-analyses and a network meta-analysis using the meta-regression approach. Network meta-analysis provides evidence about the comparisons 'Hysterectomy versus second generation non-hystereoscopic techniques' and 'Hysterectomy versus Mirena', which no individual randomized trial has assessed. Also, the network meta-analysis results are more precise (narrower confidence intervals) than the pair-wise meta-analysis results for two comparisons ('Mirena versus first generation hystereoscopic techniques' and 'Second generation non-hystereoscopic techniques versus Mirena'). Note that precision is not gained for all comparisons; this is because for some comparisons (e.g. 'Hysterectomy versus first generation hystereotopic techniques'), the heterogeneity among studies in the network as a whole is larger than the heterogeneity within the direct comparison, and therefore some uncertainty is added in the network estimates (see Section 11.4.3.2).

##### Assumptions about heterogeneity

Heterogeneity reflects the underlying differences between the randomized trials that directly compare the same pair of interventions (see Chapter 10, Section 10.10). In a pair-wise meta-analysis, the presence of important heterogeneity can make theinterpretation of the summary effect challenging. Network meta-analysis estimates are a combination of the available direct estimates via both direct and indirect comparisons, so heterogeneity among studies for one comparison can impact on findings for many other comparisons.

It is important to specify assumptions about heterogeneity in the network meta-analysis model. Heterogeneity can be specific to each comparison, or assumed to the same for every pair-wise comparison. The idea is similar to a subgroup analysis: the different subgroups could have a common heterogeneity or different heterogeneities. The latter can be estimated accurately only if enough studies are available in each subgroup.

It is common to assume that the amount of heterogeneity is the same for every comparison in the network (Higgins and Whitehead, 1996). This has three advantages compared with assuming comparison-specific heterogeneities. First, it shares information across comparisons, so that comparisons with only one or two trials can borrow information about heterogeneity from comparisons with several trials. Second, heterogeneity is estimated more precisely because more data contribute to the estimate, resulting usually in more precise estimates of intervention effects. Third, assuming common heterogeneity makes model estimation computationally easier than assuming comparison-specific heterogeneity (Lu and Ades, 2009).

The choice of heterogeneity assumption should be based on clinical and methodological understanding of the data, and assessment of the plausibility of the assumption, in addition to statistical properties.

##### 11.4.3.3 Ranking interventions

One hallmark feature of network meta-analysis is that it can estimate relative rankings of the competing interventions for a particular outcome. **Ranking probability**, the

\begin{table}
\begin{tabular}{c c c} \hline \hline \multicolumn{3}{c}{**Pair-wise meta-analysis**} \\ \hline
**Hysterectomy** & **–** & **–** & **0.38** \\
**(0.24 to 0.82)** & **–** & **–** & **(0.22 to 0.65)** \\
**(0.43 to 0.96)** & **–** & **–** & **(0.45 to 4.08)** \\
**(0.18 to 1.06)** & **–** & **–** & **(0.51 to 15.87)** \\
**(0.38 to 0.65)** & **–** & **–** & **(0.43 to 1.84)** \\
**Network meta-analysis** & & & \\ \hline \hline \end{tabular}
\end{table}
Table 14: Intervention effects, measured as odds ratios of patient dissatisfaction at 12 months of four interventions for heavy menstrual bleeding with 95% confidence intervals. Odds ratios lower than 1 favour the column-defining intervention for the network meta-analysis results (lower triangle) and the row-defining intervention for the pair-wise meta-analysis results (upper triangle)probability that an intervention is at a specific rank (first, second, etc.) when compared with the other interventions in the network, is frequently used. Ranking probabilities may vary for different outcomes. As for any estimated quantity, ranking probabilities are estimated with some variability. Therefore, inference based solely on the probability of being ranked as the best, without accounting for the variability, is misleading and should be avoided.

Ranking measures such as the **mean ranks**, **median ranks** and the **cumulative ranking probabilities** summarize the estimated probabilities for all possible ranks and account for uncertainty in relative ranking. Further discussion of ranking measures is available elsewhere (Salanti et al 2011, Chaimani et al 2013, Tan et al 2014, Rucker and Schwarzer 2015).

The estimated ranking probabilities for the heavy menstrual bleeding network (see Section 11.4.3.2) are presented in Table 11.4.b. 'Hysterectomy' is the most effective intervention according to mean rank.

#### 11.4.4 Disagreement between evidence sources (incoherence)

##### 11.4.4.1 What is incoherence?

**Incoherence** refers to the violation of the coherence assumption in a network of interventions (see Section 11.2.3.2). Incoherence occurs when different sources of information for a particular relative effect are in disagreement (Song et al 2003, Lu and Ades 2006, Salanti 2012). In much of the literature on network meta-analysis, the term **inconsistency** has been used, rather than incoherence.

The amount of incoherence in a closed loop of evidence in a network graph can be measured as the absolute difference between the direct and indirect summary estimates for any of the pair-wise comparisons in the loop (Bucher et al 1997, Song et al 2011, Veroniki et al 2013). We refer to this method of detecting incoherence as the 'loop-specific approach'. The obtained statistic is usually called an **incoherence factor** or **inconsistency factor** (IF). For example, in the dietary advice network the incoherence factor would be estimated as:

\[\text{IF}=|\text{direct}\,\text{MD}(\text{BvsC})-\text{indirect}\,\text{MD}( \text{BvsC})|\]

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & & & \multicolumn{2}{c}{Second generation non-} & \multicolumn{2}{c}{First generation} \\  & Rank & Hysterectomy & hystereoscopic techniques & Mirena & hystereoscopic techniques \\ \hline \hline \multirow{3}{*}{**mean rank**} & **1** & 96\% & 1\% & 4\% & 0\% \\  & **2** & 4\% & 46\% & 40\% & 9\% \\  & **3** & 0\% & 46\% & 19\% & 35\% \\ \cline{1-1}  & **4** & 0\% & 7\% & 37\% & 56\% \\ \cline{1-1}  & Mean rank & 1 & 3 & 3 & 4 \\ \hline \hline \end{tabular}
\end{table}
Table 11.4.b: Ranking probabilities and mean ranks for intervention effectiveness in heavy menstrual bleeding. Lower mean rank values indicate that the interventions are associated with less mortality If measures the level of disagreement between the direct and indirect effect estimates. The standard error of the incoherence factor is obtained from

\[\text{Variance}[|\text{F}|=\text{Variance}[\text{direct}\,\text{MD}(\text{ BvsC})]+\text{Variance}[\text{indirect}\,\text{MD}(\text{BvsC})]\]

and can be used to construct a 95% confidence interval for the IF:

\[\text{IF}\pm 1.96\times\text{SE}(\text{IF}).\]

Several approaches have been suggested for evaluating incoherence in a network of interventions with many loops (Donegan et al 2013, Veroniki et al 2013), broadly categorized as **local** and **global** approaches. Local approaches evaluate regions of network separately to detect possible 'incoherence spots', whereas global approaches evaluate coherence in the entire network.

##### Approaches to evaluating local incoherence

A recommended local approach for investigating incoherence is SIDE (Separating Indirect from Direct Evidence). This evaluates the IF for every pair-wise comparison in a network by contrasting a direct estimate (when available) with an indirect estimate; the latter being estimated from the entire network once the direct evidence has been removed. The method was first introduced by Dias and colleagues (Dias et al 2010) under the name 'node-splitting'. The SIDE approach has been implemented in the **network** macro for Stata (White 2015) and the **netmeta** command in R (Schwarzer et al 2015). For example, Table 1.4.c presents the incoherence results of a network that compares the effectiveness of four active interventions and placebo in preventing serious vascular events after transient ischaemic attack or stroke (Thijs et al 2008). Data are available for seven out of ten possible direct comparisons and none of them was found to be statistically significant in terms of incoherence.

In the special case where direct and several independent indirect estimates are available, the 'composite Chi\({}^{2}\) statistic' can be used instead (Caldwell et al 2010).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{Direct} & \multicolumn{2}{c}{Indirect} & \multicolumn{2}{c}{Incoherence factor} \\ \cline{2-7}  & \multicolumn{3}{c}{Standard} & \multicolumn{2}{c}{Standard} & \multicolumn{2}{c}{Standard} & \multicolumn{2}{c}{Standard} \\ \cline{2-7} Comparison & Estimate & error & Estimate & error & Estimate & error & _P value_ \\ \hline A versus C & -0.15 & 0.05 & -0.21 & 0.10 & 0.07 & 0.12 & _0.56_ \\ A versus D & -0.45 & 0.07 & -0.32 & 0.11 & -0.14 & 0.13 & _0.28_ \\ A versus E & -0.26 & 0.14 & -0.23 & 0.07 & -0.03 & 0.16 & _0.85_ \\ B versus C & 0.18 & 0.11 & 0.13 & 0.08 & 0.05 & 0.14 & _0.70_ \\ B versus E & 0.07 & 0.07 & 0.12 & 0.12 & -0.05 & 0.14 & _0.70_ \\ C versus D & -0.23 & 0.06 & -0.35 & 0.12 & 0.12 & 0.13 & _0.38_ \\ C versus E & -0.06 & 0.05 & -0.11 & 0.10 & 0.05 & 0.11 & _0.66_ \\ \hline \hline \end{tabular}
\end{table}
Table 1.4.c: Results based on the SIDE approach to evaluating local incoherence. P values less than 0.05 suggest statistically significant incoherence The loop-specific approach described in Section 11.4.4.1 can be extended to networks with many interventions by evaluating incoherence separately in each closed loop of evidence. The approach can be performed using the _ifplot_ macro available for Stata (Chaimani and Salanti 2015). However, unlike the SIDE approach, this method does not incorporate the information from the entire network when estimating the indirect evidence.

Tests for incoherence have low power and therefore may fail to detect incoherence as statistically significant even when it is present (Song et al 2012, Veroniki et al 2014). This means that the absence of statistically significant incoherence is not evidence for the absence of incoherence. Review authors should consider the confidence intervals for incoherence factors and decide whether they include values that are sufficiently large to suggest clinically important discrepancies between direct and indirect evidence.

##### 11.4.4.3 Approaches to evaluating global incoherence

Global incoherence in a network can be evaluated and detected via **incoherence models**. These models differ from the **coherence** models described in Section 11.4.3.1 by relaxing the coherence equations (see Section 11.2.3.2) and allowing intervention effects to vary when estimated directly and indirectly (Lu and Ades 2006). The models add additional terms, equivalent to the incoherence factors (IFs) defined in Section 11.4.4.1, to the coherence equations. For example, in the dietary advice network the coherence equation given in Section 11.2.3.2 would be modified to:

\[\text{``true''indirect\,MD}(\text{BvsC})=\text{``true''direct\,MD}(\text{ AvsC})-\text{``true''direct\,MD}(\text{AvsB})+\text{IF}_{ABC}.\]

The quantity \(\text{IF}_{ABC}\) measures incoherence in the evidence loop 'dietitian-doctor-nurse'. Obviously, complex networks will have several IFs. For a network to be coherent, all IF need to be close to zero. This can be formally tested via a Chi\({}^{2}\) statistic test which is available in Stata in the **network** macro (White 2015). An extension of this model has been suggested where incoherence measures the disagreement when an effect size is measured in studies that involve different sets of interventions (termed 'design incoherence') (Higgins et al 2012).

Measures like the Q-test and the l\({}^{2}\) statistic, which are commonly used for the evaluation of heterogeneity in a pair-wise meta-analysis (see Chapter 10, Section 10.10.2), have been developed for the assessment of heterogeneity and incoherence in network meta-analysis (Krahn et al 2013, Rucker and Schwarzer 2013, Jackson et al 2014). These have been implemented in the package **netmeta** in R (Schwarzer et al 2015).

##### 11.4.4.4 Forming conclusions about incoherence

We suggest review authors use both local and global approaches and consider their results jointly to make inferences about incoherence. The approaches presented in Sections 11.4.4.2 and 11.4.4.3 for evaluating incoherence have limitations. As for tests for statistical heterogeneity in a standard pair-wise meta-analysis, tests for detecting incoherence often lack power to detect incoherence when it is present, as shown in simulations and empirical studies (Song et al 2012, Veroniki et al 2014). Also, different assumptions and different methods in the estimation of heterogeneity may have an impact on the findings about incoherence (Veroniki et al 2013, Veroniki et al 2014). Empirical evidence suggests that review authors sometimes assess the presence of incoherence, if at all, using inappropriate methods (Veroniki et al 2013, Nikolakopoulou et al 2014, Petropoulou et al 2016).

Conclusions should be drawn not just from consideration of statistical significance but by interpreting the range of values included in confidence intervals of the incoherence factors. Researchers should remember that the absence of statistically significant incoherence does not ensure transitivity in the network, which should always be assessed by examining effect modifiers before undertaking the analysis (see Section 11.2.2).

Once incoherence is detected, possible explanations should be sought. Errors in data collection, broad eligibility criteria and imbalanced distributions of effect modifiers may have introduced incoherence. Possible analytical strategies in the presence of incoherence are available (Salanti 2012, Jansen and Naci 2013).

### Evaluating confidence in the results of a network meta-analysis

The GRADE approach is recommended for use in Cochrane Reviews to assess the confidence of the evidence for each pair-wise comparison of interventions (see Chapter 14). The approach starts by assuming high confidence in the evidence for randomized trials of a specific pair-wise comparison and then rates down the evidence for considerations of five issues: study limitations, indirectness, inconsistency, imprecision and publication bias.

Rating the confidence in the evidence from a network of interventions is more challenging than pair-wise meta-analysis (Dumville et al 2012). To date, two frameworks have been suggested in the literature to extend the GRADE system to indirect comparisons and network meta-analyses: Salanti and colleagues (Salanti et al 2014) and Puhan and colleagues (Puhan et al 2014). Section 11.5.1 describes the principles of each approach, noting similarities and differences.

#### 11.5.1 Available approaches for evaluating confidence in the evidence

The two available approaches to evaluating confidence in evidence from a network meta-analysis acknowledge that the confidence in each combined comparison depends on the confidence in the direct and indirect comparisons that contribute to it, and that the confidence in each indirect comparison in turn depends on the confidence in the pieces of direct evidence that contribute to it. Therefore, all GRADE assessments are built to some extent on applying GRADE ideas for direct evidence. The two approaches diverge in the way they combine the considerations when thinking about an indirect or combined comparison, as illustrated in Table 11.5.a using the dietary advice example.

The framework by Salanti and colleagues is driven by the ability to express each estimated intervention effect from a network meta-analysis as a weighted sum of all the available direct comparisons (see Section 11.4) (Lu et al 2011, Konig et al 2013, Krahn et al 2013). The weight is determined, under some assumptions, by the **contribution matrix,** which has been implemented in the _netweight_ macro (Chaimani and Salanti 2015) available for the Stata statistical package and programmed in an online tool - CINeMA - which assesses 'Confidence in Network Meta-Analysis' ([http://cinema.ispm](http://cinema.ispm). ch/). The matrix contains the percentage of information attributable to each direct